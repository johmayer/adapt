{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a115dd",
   "metadata": {},
   "source": [
    "## Jakob Ohmayer (4742300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5eef0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab49b35",
   "metadata": {},
   "source": [
    "### Load model ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb694a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "183a13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet import resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165c2d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_PLACES=cores\n",
      "env: OMP_PROC_BIND=close\n",
      "env: OMP_WAIT_POLICY=active\n"
     ]
    }
   ],
   "source": [
    "threads = 40\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "#maybe better performance\n",
    "%env OMP_PLACES=cores\n",
    "%env OMP_PROC_BIND=close\n",
    "%env OMP_WAIT_POLICY=active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa06300",
   "metadata": {},
   "source": [
    "### Choose approximate multiplier \n",
    "\n",
    "Here the multiplier SPR12_44 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "562689c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "axx_mult = 'SPR12_44'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539e7e1",
   "metadata": {},
   "source": [
    "### Load model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc26796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_SPR12_44/build.ninja...\n",
      "Building extension module PyInit_conv2d_SPR12_44...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): AdaPT_Conv2d(\n",
       "    3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet34(pretrained=True, axx_mult = axx_mult)\n",
    "\n",
    "model.eval() # for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76721ed0",
   "metadata": {},
   "source": [
    "### Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f63b4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def val_dataloader(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CIFAR10(root=\"datasets/cifar10_data\", train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    )\n",
    "dataset = CIFAR10(root=\"datasets/cifar10_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "data = val_dataloader()\n",
    "\n",
    "# data_t is used for calibration purposes and is a subset of train-set\n",
    "data_t = DataLoader(trainset_1, batch_size=128,\n",
    "                                            shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa74c5d",
   "metadata": {},
   "source": [
    "### Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters. Is rerun after each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "946f0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "\n",
    "     for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "         model(image.cpu())\n",
    "         if i >= num_batches:\n",
    "             break\n",
    "\n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "def calibrate_model(model, data_t):\n",
    "    # It is a bit slow since we collect histograms on CPU\n",
    "    with torch.no_grad():\n",
    "        stats = collect_stats(model, data_t, num_batches=2)\n",
    "        amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "        # optional - test different calibration methods\n",
    "        #amax = compute_amax(model, method=\"mse\")\n",
    "        #amax = compute_amax(model, method=\"entropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7733cec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:17<00:00,  8.92s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1117 21:51:50.554443 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.557247 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.557740 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.558352 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.559018 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.559560 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.560225 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.560859 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.561560 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.562194 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.562855 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.563482 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.564141 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.564822 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.565463 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.566158 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.566790 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.567434 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.568051 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.568691 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.569314 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.569979 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.570428 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.570998 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.571528 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.572028 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.572550 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.573066 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.573617 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.574108 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.574615 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.575148 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.575654 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.576163 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.576702 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.577195 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.577664 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.578127 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.578592 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.579063 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.579512 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.579985 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.580488 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.580941 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.581412 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.581919 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.582410 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.582862 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.583370 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.583856 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.584332 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.584842 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.585339 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.585850 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.586330 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.586813 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.587331 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.587805 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.588301 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.588785 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.589282 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.589784 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.590286 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.590770 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.591279 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.591750 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.592209 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.592692 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.593153 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.593709 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.594198 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.594663 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:51:50.595369 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.595853 134165911848768 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W1117 21:51:50.596552 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.597247 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.597895 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.598547 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.599193 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.599858 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.600491 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.601142 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.601798 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.602454 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.603083 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1117 21:51:50.603738 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.604362 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.605020 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.605676 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.606353 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.606976 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.607666 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.608319 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.609015 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.609676 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.610347 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.611038 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.611726 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.612371 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.613065 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.613745 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.614419 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.615097 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.615755 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.616412 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.617065 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.617752 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.618518 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.619188 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.619883 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.620517 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.621248 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.621886 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.622539 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.623175 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.623823 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.624466 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.625111 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.625778 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.626399 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.627060 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.627696 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.628351 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.629042 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.629718 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.630373 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.631033 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.631701 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.632350 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.633028 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.633698 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.634359 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.635015 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.635687 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.636332 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.636975 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.637742 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.638414 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.639228 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.640063 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.640734 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.641489 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.642114 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.642779 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:51:50.643459 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1418 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6598 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0555 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3260 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5872 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0395 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2946 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0345 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5637 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2232 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0253 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5238 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2206 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.5238 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3405 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1705 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3570 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1735 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3965 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2222 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5021 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2130 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.5021 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0216 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2452 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1395 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2415 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1315 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2595 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1192 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3129 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1098 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3827 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1301 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4942 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1771 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4942 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7859 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2351 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.1864 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2054 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator quant)\n"
     ]
    }
   ],
   "source": [
    "calibrate_model(model, data_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446f0bd",
   "metadata": {},
   "source": [
    "### Run model evaluation (before re-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44bfa498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [29:07<00:00, 22.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1747.6586084780001\n",
      "Accuracy of the network on the 10000 test images: 33.7740 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627d03b",
   "metadata": {},
   "source": [
    "**Accuracy before re-training**: 33.77%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204303c",
   "metadata": {},
   "source": [
    "### Run approximate-aware re-training for 15 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e626266",
   "metadata": {},
   "source": [
    "As re-training takes quite long and brakes sometimes the individual models are saved after each epoch to enable loading these models to continue fine-tuning. Therefore the log output of the following cell is not correct. The majority of the output is included in the file ``training.log`` in the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b8bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1117 21:51:51.078365 134165911848768 tensor_quantizer.py:402] conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.079574 134165911848768 tensor_quantizer.py:402] conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.089933 134165911848768 tensor_quantizer.py:402] layer1.0.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.090771 134165911848768 tensor_quantizer.py:402] layer1.0.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.093954 134165911848768 tensor_quantizer.py:402] layer1.0.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.094571 134165911848768 tensor_quantizer.py:402] layer1.0.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.096243 134165911848768 tensor_quantizer.py:402] layer1.1.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.096866 134165911848768 tensor_quantizer.py:402] layer1.1.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.098430 134165911848768 tensor_quantizer.py:402] layer1.1.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.099478 134165911848768 tensor_quantizer.py:402] layer1.1.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.100982 134165911848768 tensor_quantizer.py:402] layer1.2.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.102082 134165911848768 tensor_quantizer.py:402] layer1.2.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.103872 134165911848768 tensor_quantizer.py:402] layer1.2.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.105261 134165911848768 tensor_quantizer.py:402] layer1.2.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.106424 134165911848768 tensor_quantizer.py:402] layer2.0.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.108473 134165911848768 tensor_quantizer.py:402] layer2.0.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.115400 134165911848768 tensor_quantizer.py:402] layer2.0.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.117561 134165911848768 tensor_quantizer.py:402] layer2.0.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.118925 134165911848768 tensor_quantizer.py:402] layer2.0.downsample.0.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.121539 134165911848768 tensor_quantizer.py:402] layer2.0.downsample.0.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.123147 134165911848768 tensor_quantizer.py:402] layer2.1.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.124385 134165911848768 tensor_quantizer.py:402] layer2.1.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.130056 134165911848768 tensor_quantizer.py:402] layer2.1.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.130559 134165911848768 tensor_quantizer.py:402] layer2.1.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.132160 134165911848768 tensor_quantizer.py:402] layer2.2.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.133114 134165911848768 tensor_quantizer.py:402] layer2.2.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.134713 134165911848768 tensor_quantizer.py:402] layer2.2.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.135655 134165911848768 tensor_quantizer.py:402] layer2.2.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.137391 134165911848768 tensor_quantizer.py:402] layer2.3.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.138060 134165911848768 tensor_quantizer.py:402] layer2.3.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.139459 134165911848768 tensor_quantizer.py:402] layer2.3.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.140163 134165911848768 tensor_quantizer.py:402] layer2.3.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.141909 134165911848768 tensor_quantizer.py:402] layer3.0.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.142785 134165911848768 tensor_quantizer.py:402] layer3.0.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.144410 134165911848768 tensor_quantizer.py:402] layer3.0.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.145141 134165911848768 tensor_quantizer.py:402] layer3.0.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.146306 134165911848768 tensor_quantizer.py:402] layer3.0.downsample.0.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.147224 134165911848768 tensor_quantizer.py:402] layer3.0.downsample.0.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.148862 134165911848768 tensor_quantizer.py:402] layer3.1.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.149783 134165911848768 tensor_quantizer.py:402] layer3.1.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.150954 134165911848768 tensor_quantizer.py:402] layer3.1.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.151639 134165911848768 tensor_quantizer.py:402] layer3.1.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.153188 134165911848768 tensor_quantizer.py:402] layer3.2.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.154125 134165911848768 tensor_quantizer.py:402] layer3.2.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.155694 134165911848768 tensor_quantizer.py:402] layer3.2.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.156585 134165911848768 tensor_quantizer.py:402] layer3.2.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.158207 134165911848768 tensor_quantizer.py:402] layer3.3.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.158910 134165911848768 tensor_quantizer.py:402] layer3.3.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.160550 134165911848768 tensor_quantizer.py:402] layer3.3.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.161412 134165911848768 tensor_quantizer.py:402] layer3.3.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.163069 134165911848768 tensor_quantizer.py:402] layer3.4.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.163953 134165911848768 tensor_quantizer.py:402] layer3.4.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.165556 134165911848768 tensor_quantizer.py:402] layer3.4.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.166288 134165911848768 tensor_quantizer.py:402] layer3.4.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.167937 134165911848768 tensor_quantizer.py:402] layer3.5.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.168835 134165911848768 tensor_quantizer.py:402] layer3.5.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.170432 134165911848768 tensor_quantizer.py:402] layer3.5.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.171353 134165911848768 tensor_quantizer.py:402] layer3.5.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.173945 134165911848768 tensor_quantizer.py:402] layer4.0.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.174920 134165911848768 tensor_quantizer.py:402] layer4.0.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.177467 134165911848768 tensor_quantizer.py:402] layer4.0.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.178403 134165911848768 tensor_quantizer.py:402] layer4.0.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.179786 134165911848768 tensor_quantizer.py:402] layer4.0.downsample.0.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.180689 134165911848768 tensor_quantizer.py:402] layer4.0.downsample.0.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.183205 134165911848768 tensor_quantizer.py:402] layer4.1.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.183909 134165911848768 tensor_quantizer.py:402] layer4.1.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.186174 134165911848768 tensor_quantizer.py:402] layer4.1.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.186878 134165911848768 tensor_quantizer.py:402] layer4.1.conv2.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.189203 134165911848768 tensor_quantizer.py:402] layer4.2.conv1.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.189915 134165911848768 tensor_quantizer.py:402] layer4.2.conv1.quantizer_w: Overwriting amax.\n",
      "W1117 21:51:51.192203 134165911848768 tensor_quantizer.py:402] layer4.2.conv2.quantizer: Overwriting amax.\n",
      "W1117 21:51:51.192888 134165911848768 tensor_quantizer.py:402] layer4.2.conv2.quantizer_w: Overwriting amax.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:19<00:00, 39.80s/it]\n",
      "W1117 21:53:10.790431 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.791142 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.792350 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1117 21:53:10.793441 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.794545 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.795618 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.796675 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.797763 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.798811 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.799889 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.800963 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.802589 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.803675 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.804737 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.805704 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.806287 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.807301 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.808358 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.809425 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.810475 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.811510 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.812540 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.814084 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.815089 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.816154 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.817168 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.818259 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.819188 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.819796 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.821298 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.821874 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.822871 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.823252 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.824170 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.825613 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.826589 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.827570 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.828525 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.829515 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.830460 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.831405 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.832357 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.833323 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.834305 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.835251 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.836196 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.837161 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.838126 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.839079 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.840025 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.840964 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.841962 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.842965 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.843926 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.845399 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.845860 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.846827 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.847797 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.848784 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.849771 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.850727 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.851680 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.852650 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.853640 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.854628 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.855568 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.856539 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.857496 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.858433 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.859422 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.860880 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.861437 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 21:53:10.862287 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.863190 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.863853 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.864487 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.865142 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.865793 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.866422 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.867047 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.867689 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.868310 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.868966 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.869578 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.870219 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.870847 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.871472 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.872126 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.872745 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.873404 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1117 21:53:10.874004 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.874636 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.875267 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.875876 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.876517 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.877136 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.877810 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.878405 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.879041 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.879668 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.880303 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.880930 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.881583 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.882220 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.882835 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.883491 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.884104 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.884727 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.885346 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.885996 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.886645 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.887243 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.887913 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.888561 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.889206 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.889841 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.890494 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.891131 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.891757 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.892395 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.893009 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.893664 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.894291 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.894921 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.895540 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.896157 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.896806 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.897418 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.898064 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.898668 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.899307 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.899915 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.900543 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.901177 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.901834 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.902472 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.903073 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.903695 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.904335 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.904944 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.905599 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.906211 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.906847 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 21:53:10.907420 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1418 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6120 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0555 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2950 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5320 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0395 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2560 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0345 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5146 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2054 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4761 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2023 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4761 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3069 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1518 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3218 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1551 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3480 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1890 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4249 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1839 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4249 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0216 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2186 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1177 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2103 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1166 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2290 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1074 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2906 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0953 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3536 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1130 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4579 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1676 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4579 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7583 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2075 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.1451 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2001 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator quant)\n",
      "Skipping epoch 1 as pretrained model exists\n",
      "Skipping epoch 2 as pretrained model exists\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3]  [ 0/40]  eta: 0:29:41  lr: 0.0001  img/s: 2.878492948106378  loss: 0.1818 (0.1818)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 44.5340  data: 0.0662\n",
      "Epoch: [3]  [ 1/40]  eta: 0:29:20  lr: 0.0001  img/s: 2.799503281035007  loss: 0.1600 (0.1709)  acc1: 92.1875 (93.7500)  acc5: 100.0000 (100.0000)  time: 45.1389  data: 0.0438\n",
      "Epoch: [3]  [ 2/40]  eta: 0:28:43  lr: 0.0001  img/s: 2.7961661999991416  loss: 0.1600 (0.1601)  acc1: 94.5312 (94.0104)  acc5: 100.0000 (100.0000)  time: 45.3590  data: 0.0367\n",
      "Epoch: [3]  [ 3/40]  eta: 0:28:45  lr: 0.0001  img/s: 2.5376503658803293  loss: 0.1384 (0.1533)  acc1: 94.5312 (94.9219)  acc5: 100.0000 (99.8047)  time: 46.6347  data: 0.0328\n",
      "Epoch: [3]  [ 4/40]  eta: 0:28:23  lr: 0.0001  img/s: 2.5593578759479483  loss: 0.1600 (0.1586)  acc1: 95.3125 (95.0000)  acc5: 100.0000 (99.8438)  time: 47.3145  data: 0.0305\n",
      "Epoch: [3]  [ 5/40]  eta: 0:27:29  lr: 0.0001  img/s: 2.768827542750056  loss: 0.1600 (0.1680)  acc1: 95.3125 (95.1823)  acc5: 100.0000 (99.8698)  time: 47.1372  data: 0.0290\n",
      "Epoch: [3]  [ 6/40]  eta: 0:27:02  lr: 0.0001  img/s: 2.497797011169284  loss: 0.1600 (0.1652)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (99.8884)  time: 47.7271  data: 0.0280\n",
      "Epoch: [3]  [ 7/40]  eta: 0:26:19  lr: 0.0001  img/s: 2.6210182950496086  loss: 0.1485 (0.1579)  acc1: 95.3125 (95.6055)  acc5: 100.0000 (99.9023)  time: 47.8685  data: 0.0272\n",
      "Epoch: [3]  [ 8/40]  eta: 0:25:42  lr: 0.0001  img/s: 2.518261915436658  loss: 0.1485 (0.1519)  acc1: 96.0938 (95.8333)  acc5: 100.0000 (99.9132)  time: 48.1998  data: 0.0266\n",
      "Epoch: [3]  [ 9/40]  eta: 0:25:01  lr: 0.0001  img/s: 2.528593217784476  loss: 0.1485 (0.1642)  acc1: 95.3125 (95.6250)  acc5: 100.0000 (99.8438)  time: 48.4441  data: 0.0261\n",
      "Epoch: [3]  [10/40]  eta: 0:24:16  lr: 0.0001  img/s: 2.5843741811623344  loss: 0.1485 (0.1607)  acc1: 96.0938 (95.7386)  acc5: 100.0000 (99.8580)  time: 48.5450  data: 0.0260\n",
      "Epoch: [3]  [11/40]  eta: 0:23:34  lr: 0.0001  img/s: 2.4955469203022203  loss: 0.1384 (0.1573)  acc1: 96.0938 (95.8984)  acc5: 100.0000 (99.8698)  time: 48.7757  data: 0.0257\n",
      "Epoch: [3]  [12/40]  eta: 0:22:39  lr: 0.0001  img/s: 2.7869825879734007  loss: 0.1384 (0.1553)  acc1: 96.0938 (95.8534)  acc5: 100.0000 (99.8798)  time: 48.5583  data: 0.0254\n",
      "Epoch: [3]  [13/40]  eta: 0:21:44  lr: 0.0001  img/s: 2.826829806979669  loss: 0.1384 (0.1597)  acc1: 95.3125 (95.8147)  acc5: 100.0000 (99.8884)  time: 48.3257  data: 0.0251\n",
      "Epoch: [3]  [14/40]  eta: 0:20:52  lr: 0.0001  img/s: 2.7712328054438116  loss: 0.1485 (0.1631)  acc1: 95.3125 (95.7292)  acc5: 100.0000 (99.8438)  time: 48.1846  data: 0.0248\n",
      "Epoch: [3]  [15/40]  eta: 0:19:59  lr: 0.0001  img/s: 2.8406797066661773  loss: 0.1485 (0.1627)  acc1: 95.3125 (95.7520)  acc5: 100.0000 (99.8535)  time: 47.9906  data: 0.0246\n",
      "Epoch: [3]  [16/40]  eta: 0:19:09  lr: 0.0001  img/s: 2.7800015128501543  loss: 0.1563 (0.1671)  acc1: 95.3125 (95.6342)  acc5: 100.0000 (99.8621)  time: 47.8773  data: 0.0244\n",
      "Epoch: [3]  [17/40]  eta: 0:18:17  lr: 0.0001  img/s: 2.8421010442574155  loss: 0.1485 (0.1637)  acc1: 95.3125 (95.7465)  acc5: 100.0000 (99.8698)  time: 47.7207  data: 0.0243\n",
      "Epoch: [3]  [18/40]  eta: 0:17:26  lr: 0.0001  img/s: 2.856187057670015  loss: 0.1563 (0.1679)  acc1: 95.3125 (95.6414)  acc5: 100.0000 (99.8766)  time: 47.5690  data: 0.0241\n",
      "Epoch: [3]  [19/40]  eta: 0:16:36  lr: 0.0001  img/s: 2.8502902494893516  loss: 0.1485 (0.1644)  acc1: 95.3125 (95.7422)  acc5: 100.0000 (99.8828)  time: 47.4370  data: 0.0240\n",
      "Epoch: [3]  [20/40]  eta: 0:15:45  lr: 0.0001  img/s: 2.9076235428795703  loss: 0.1384 (0.1603)  acc1: 96.0938 (95.8705)  acc5: 100.0000 (99.8512)  time: 47.4124  data: 0.0217\n",
      "Epoch: [3]  [21/40]  eta: 0:14:55  lr: 0.0001  img/s: 2.899099655775228  loss: 0.1331 (0.1588)  acc1: 96.0938 (95.8807)  acc5: 100.0000 (99.8580)  time: 47.3339  data: 0.0217\n",
      "Epoch: [3]  [22/40]  eta: 0:14:06  lr: 0.0001  img/s: 2.8860276777040155  loss: 0.1331 (0.1619)  acc1: 96.0938 (95.7541)  acc5: 100.0000 (99.8641)  time: 47.2625  data: 0.0216\n",
      "Epoch: [3]  [23/40]  eta: 0:13:18  lr: 0.0001  img/s: 2.828722366168734  loss: 0.1315 (0.1590)  acc1: 96.0938 (95.8659)  acc5: 100.0000 (99.8698)  time: 47.0030  data: 0.0216\n",
      "Epoch: [3]  [24/40]  eta: 0:12:29  lr: 0.0001  img/s: 2.847192065474095  loss: 0.1280 (0.1577)  acc1: 96.0938 (95.9062)  acc5: 100.0000 (99.8750)  time: 46.7502  data: 0.0216\n",
      "Epoch: [3]  [25/40]  eta: 0:11:41  lr: 0.0001  img/s: 2.883799487974044  loss: 0.1280 (0.1570)  acc1: 96.0938 (95.9435)  acc5: 100.0000 (99.8798)  time: 46.6580  data: 0.0216\n",
      "Epoch: [3]  [26/40]  eta: 0:10:53  lr: 0.0001  img/s: 2.865304774202231  loss: 0.1280 (0.1574)  acc1: 96.0938 (95.9201)  acc5: 100.0000 (99.8843)  time: 46.3293  data: 0.0215\n",
      "Epoch: [3]  [27/40]  eta: 0:10:06  lr: 0.0001  img/s: 2.8623891849078733  loss: 0.1315 (0.1569)  acc1: 96.0938 (95.9542)  acc5: 100.0000 (99.8884)  time: 46.1234  data: 0.0215\n",
      "Epoch: [3]  [28/40]  eta: 0:09:19  lr: 0.0001  img/s: 2.7306295699484187  loss: 0.1315 (0.1553)  acc1: 96.0938 (95.9591)  acc5: 100.0000 (99.8922)  time: 45.9257  data: 0.0215\n",
      "Epoch: [3]  [29/40]  eta: 0:08:32  lr: 0.0001  img/s: 2.8147091299772296  loss: 0.1315 (0.1584)  acc1: 96.0938 (95.8594)  acc5: 100.0000 (99.8698)  time: 45.6685  data: 0.0215\n",
      "Epoch: [3]  [30/40]  eta: 0:07:45  lr: 0.0001  img/s: 2.7849003227630034  loss: 0.1315 (0.1565)  acc1: 96.0938 (95.8921)  acc5: 100.0000 (99.8740)  time: 45.4899  data: 0.0213\n",
      "Epoch: [3]  [31/40]  eta: 0:06:58  lr: 0.0001  img/s: 2.8779607808164993  loss: 0.1387 (0.1598)  acc1: 96.0938 (95.8252)  acc5: 100.0000 (99.8535)  time: 45.1491  data: 0.0212\n",
      "Epoch: [3]  [32/40]  eta: 0:06:11  lr: 0.0001  img/s: 2.8544571112738937  loss: 0.1387 (0.1571)  acc1: 96.0938 (95.9044)  acc5: 100.0000 (99.8580)  time: 45.0948  data: 0.0212\n",
      "Epoch: [3]  [33/40]  eta: 0:05:25  lr: 0.0001  img/s: 2.792953574315113  loss: 0.1387 (0.1614)  acc1: 96.0938 (95.8410)  acc5: 100.0000 (99.8621)  time: 45.1222  data: 0.0212\n",
      "Epoch: [3]  [34/40]  eta: 0:04:38  lr: 0.0001  img/s: 2.81372968195239  loss: 0.1280 (0.1602)  acc1: 96.0938 (95.8929)  acc5: 100.0000 (99.8661)  time: 45.0873  data: 0.0212\n",
      "Epoch: [3]  [35/40]  eta: 0:03:51  lr: 0.0001  img/s: 2.828722902722636  loss: 0.1280 (0.1617)  acc1: 96.0938 (95.8984)  acc5: 100.0000 (99.8698)  time: 45.0970  data: 0.0213\n",
      "Epoch: [3]  [36/40]  eta: 0:03:05  lr: 0.0001  img/s: 2.8071525646892743  loss: 0.1257 (0.1607)  acc1: 96.0938 (95.9037)  acc5: 100.0000 (99.8733)  time: 45.0747  data: 0.0213\n",
      "Epoch: [3]  [37/40]  eta: 0:02:18  lr: 0.0001  img/s: 2.897950352565368  loss: 0.1257 (0.1590)  acc1: 96.0938 (95.9293)  acc5: 100.0000 (99.8766)  time: 45.0314  data: 0.0214\n",
      "Epoch: [3]  [38/40]  eta: 0:01:32  lr: 0.0001  img/s: 2.8660646953415294  loss: 0.1257 (0.1594)  acc1: 96.0938 (95.9335)  acc5: 100.0000 (99.8798)  time: 45.0236  data: 0.0213\n",
      "Epoch: [3]  [39/40]  eta: 0:00:45  lr: 0.0001  img/s: 3.221704139914292  loss: 0.1280 (0.1665)  acc1: 96.0938 (95.9200)  acc5: 100.0000 (99.8800)  time: 42.9015  data: 0.0205\n",
      "Epoch: [3] Total time: 0:30:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:14<00:00, 37.04s/it]\n",
      "W1117 22:24:31.778615 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.779133 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.780140 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.780652 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.781156 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.781633 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.782118 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.782582 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.783048 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.783503 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.783973 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.784448 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.784918 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.785375 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.785883 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.786348 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.786812 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.787265 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.787738 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.788198 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.788664 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.789115 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.789611 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.790075 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.790405 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.790753 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.791115 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.791462 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.791812 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.792151 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.792510 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.792859 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.793209 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.793563 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.793905 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.794251 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.794604 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.794952 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.795299 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.795654 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.796007 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.796344 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.796684 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.797034 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.797400 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.797746 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.798099 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.798451 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.798799 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.799134 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.799475 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.799818 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.800168 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.800508 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.800854 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.801189 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.801559 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.801898 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.802247 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.802593 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.802943 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.803284 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.803624 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.803970 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.804325 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.804662 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.805025 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.805371 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.805738 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.806074 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.806414 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.806770 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:24:31.807300 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.807899 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.808459 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.808989 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.809529 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.810043 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.810588 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.811115 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.811633 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.812148 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.812686 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.813207 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.813738 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.814257 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1117 22:24:31.814792 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.815307 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.815825 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.816336 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.816868 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.817391 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.817903 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.818415 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.818941 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.819450 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.819958 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.820459 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.820996 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.823134 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.824089 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.824640 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.825178 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.825706 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.826228 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.826761 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.827286 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.827796 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.828315 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.828844 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.829371 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.829897 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.830410 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.830938 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.831460 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.831968 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.832481 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.833006 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.833547 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.834057 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.834564 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.835080 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.835600 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.836106 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.836611 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.837135 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.837675 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.838180 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.838690 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.839203 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.839725 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.840226 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.840745 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.841264 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.841826 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.842335 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.842846 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.843373 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.843894 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.844399 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.844932 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.845458 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.845974 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:24:31.846483 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=59.752037048339844 quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1418 calibrator=HistogramCalibrator scale=895.780029296875 quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5855 calibrator=HistogramCalibrator scale=207.50930786132812 quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0555 calibrator=HistogramCalibrator scale=2286.89501953125 quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2725 calibrator=HistogramCalibrator scale=430.46417236328125 quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator scale=3880.2880859375 quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5042 calibrator=HistogramCalibrator scale=238.7018280029297 quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0395 calibrator=HistogramCalibrator scale=3218.383544921875 quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2331 calibrator=HistogramCalibrator scale=496.1064758300781 quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0345 calibrator=HistogramCalibrator scale=3677.5966796875 quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4832 calibrator=HistogramCalibrator scale=246.8086700439453 quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator scale=3588.1611328125 quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1928 calibrator=HistogramCalibrator scale=618.389892578125 quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator scale=5040.06396484375 quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4480 calibrator=HistogramCalibrator scale=266.7640075683594 quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator scale=4422.54931640625 quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1917 calibrator=HistogramCalibrator scale=627.7931518554688 quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator scale=4453.1904296875 quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4480 calibrator=HistogramCalibrator scale=266.7640075683594 quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator scale=2309.54296875 quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2881 calibrator=HistogramCalibrator scale=413.79461669921875 quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator scale=5450.48095703125 quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1423 calibrator=HistogramCalibrator scale=836.7001342773438 quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator scale=5745.5625 quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2989 calibrator=HistogramCalibrator scale=394.6878967285156 quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=5273.30078125 quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1434 calibrator=HistogramCalibrator scale=819.0144653320312 quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator scale=5739.0859375 quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3203 calibrator=HistogramCalibrator scale=364.9691467285156 quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5767.67333984375 quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1701 calibrator=HistogramCalibrator scale=671.9784545898438 quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator scale=6969.6787109375 quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3845 calibrator=HistogramCalibrator scale=298.9211120605469 quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator scale=7435.51220703125 quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1685 calibrator=HistogramCalibrator scale=690.5293579101562 quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator scale=12672.001953125 quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3845 calibrator=HistogramCalibrator scale=298.9211120605469 quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0216 calibrator=HistogramCalibrator scale=5884.2431640625 quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2012 calibrator=HistogramCalibrator scale=580.9015502929688 quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator scale=23619.01953125 quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1085 calibrator=HistogramCalibrator scale=1078.935302734375 quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator scale=24773.11328125 quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1955 calibrator=HistogramCalibrator scale=603.7921752929688 quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator scale=18947.671875 quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1078 calibrator=HistogramCalibrator scale=1089.099853515625 quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator scale=21349.779296875 quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2140 calibrator=HistogramCalibrator scale=554.5504760742188 quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator scale=25490.427734375 quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1017 calibrator=HistogramCalibrator scale=1182.07666015625 quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25880.146484375 quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2741 calibrator=HistogramCalibrator scale=437.06158447265625 quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator scale=35528.43359375 quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0878 calibrator=HistogramCalibrator scale=1332.5909423828125 quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator scale=31579.73828125 quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3321 calibrator=HistogramCalibrator scale=359.1181640625 quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator scale=36529.60546875 quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1001 calibrator=HistogramCalibrator scale=1123.977294921875 quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator scale=26926.013671875 quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4302 calibrator=HistogramCalibrator scale=277.35498046875 quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator scale=33544.44921875 quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1597 calibrator=HistogramCalibrator scale=757.876953125 quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25662.263671875 quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4302 calibrator=HistogramCalibrator scale=277.35498046875 quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator scale=6248.654296875 quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7472 calibrator=HistogramCalibrator scale=167.47763061523438 quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator scale=53781.19140625 quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1982 calibrator=HistogramCalibrator scale=611.9537353515625 quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator scale=21318.76171875 quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.1267 calibrator=HistogramCalibrator scale=110.9034652709961 quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator scale=62854.109375 quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1952 calibrator=HistogramCalibrator scale=634.5655517578125 quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator scale=21765.048828125 quant)\n",
      "Model saved to ./saved/retrained_model_epoch_3.pth after epoch 3\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4]  [ 0/40]  eta: 0:29:06  lr: 0.0001  img/s: 2.9332588503465318  loss: 0.1356 (0.1356)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 43.6602  data: 0.0227\n",
      "Epoch: [4]  [ 1/40]  eta: 0:28:34  lr: 0.0001  img/s: 2.8920512397326212  loss: 0.1356 (0.1652)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 43.9700  data: 0.0217\n",
      "Epoch: [4]  [ 2/40]  eta: 0:27:59  lr: 0.0001  img/s: 2.8674970363942105  loss: 0.1356 (0.1473)  acc1: 93.7500 (94.5312)  acc5: 100.0000 (100.0000)  time: 44.1998  data: 0.0215\n",
      "Epoch: [4]  [ 3/40]  eta: 0:27:24  lr: 0.0001  img/s: 2.836498196419857  loss: 0.1114 (0.1335)  acc1: 93.7500 (95.5078)  acc5: 100.0000 (100.0000)  time: 44.4414  data: 0.0261\n",
      "Epoch: [4]  [ 4/40]  eta: 0:26:43  lr: 0.0001  img/s: 2.8515346620958413  loss: 0.1356 (0.1347)  acc1: 96.0938 (95.7812)  acc5: 100.0000 (100.0000)  time: 44.5349  data: 0.0251\n",
      "Epoch: [4]  [ 5/40]  eta: 0:26:07  lr: 0.0001  img/s: 2.7804822544503565  loss: 0.1285 (0.1337)  acc1: 95.3125 (95.7031)  acc5: 100.0000 (100.0000)  time: 44.7884  data: 0.0244\n",
      "Epoch: [4]  [ 6/40]  eta: 0:25:27  lr: 0.0001  img/s: 2.7966031344092577  loss: 0.1356 (0.1358)  acc1: 95.3125 (95.5357)  acc5: 100.0000 (100.0000)  time: 44.9317  data: 0.0239\n",
      "Epoch: [4]  [ 7/40]  eta: 0:24:46  lr: 0.0001  img/s: 2.799517178359295  loss: 0.1285 (0.1252)  acc1: 95.3125 (95.9961)  acc5: 100.0000 (100.0000)  time: 45.0331  data: 0.0235\n",
      "Epoch: [4]  [ 8/40]  eta: 0:24:02  lr: 0.0001  img/s: 2.822928239884228  loss: 0.1285 (0.1234)  acc1: 96.0938 (96.1806)  acc5: 100.0000 (100.0000)  time: 45.0699  data: 0.0233\n",
      "Epoch: [4]  [ 9/40]  eta: 0:23:18  lr: 0.0001  img/s: 2.8128290501294093  loss: 0.1285 (0.1327)  acc1: 95.3125 (96.0938)  acc5: 100.0000 (99.9219)  time: 45.1157  data: 0.0232\n",
      "Epoch: [4]  [10/40]  eta: 0:22:35  lr: 0.0001  img/s: 2.7958328740841805  loss: 0.1356 (0.1333)  acc1: 96.0938 (96.0938)  acc5: 100.0000 (99.9290)  time: 45.1782  data: 0.0230\n",
      "Epoch: [4]  [11/40]  eta: 0:21:56  lr: 0.0001  img/s: 2.681491384167099  loss: 0.1285 (0.1304)  acc1: 96.0938 (96.2891)  acc5: 100.0000 (99.9349)  time: 45.3937  data: 0.0235\n",
      "Epoch: [4]  [12/40]  eta: 0:21:09  lr: 0.0001  img/s: 2.8700685291781527  loss: 0.1285 (0.1280)  acc1: 96.0938 (96.2740)  acc5: 100.0000 (99.9399)  time: 45.3341  data: 0.0233\n",
      "Epoch: [4]  [13/40]  eta: 0:20:23  lr: 0.0001  img/s: 2.8488608171186702  loss: 0.1285 (0.1295)  acc1: 96.0938 (96.3170)  acc5: 100.0000 (99.8884)  time: 45.3068  data: 0.0231\n",
      "Epoch: [4]  [14/40]  eta: 0:19:41  lr: 0.0001  img/s: 2.702824092208177  loss: 0.1356 (0.1344)  acc1: 96.0938 (96.1979)  acc5: 100.0000 (99.8438)  time: 45.4449  data: 0.0230\n",
      "Epoch: [4]  [15/40]  eta: 0:18:57  lr: 0.0001  img/s: 2.751487413530887  loss: 0.1285 (0.1335)  acc1: 96.0938 (96.2891)  acc5: 100.0000 (99.8535)  time: 45.5134  data: 0.0229\n",
      "Epoch: [4]  [16/40]  eta: 0:18:12  lr: 0.0001  img/s: 2.814733877632081  loss: 0.1356 (0.1339)  acc1: 96.0938 (96.3235)  acc5: 100.0000 (99.8621)  time: 45.5124  data: 0.0228\n",
      "Epoch: [4]  [17/40]  eta: 0:17:28  lr: 0.0001  img/s: 2.730165230985052  loss: 0.1356 (0.1363)  acc1: 96.0938 (96.1806)  acc5: 100.0000 (99.8698)  time: 45.5898  data: 0.0227\n",
      "Epoch: [4]  [18/40]  eta: 0:16:43  lr: 0.0001  img/s: 2.7835395045993523  loss: 0.1393 (0.1387)  acc1: 96.0938 (96.1349)  acc5: 100.0000 (99.8766)  time: 45.6121  data: 0.0230\n",
      "Epoch: [4]  [19/40]  eta: 0:15:57  lr: 0.0001  img/s: 2.805090171188583  loss: 0.1356 (0.1366)  acc1: 96.0938 (96.2500)  acc5: 100.0000 (99.8828)  time: 45.6141  data: 0.0229\n",
      "Epoch: [4]  [20/40]  eta: 0:15:11  lr: 0.0001  img/s: 2.8313763089226027  loss: 0.1393 (0.1377)  acc1: 96.0938 (96.2426)  acc5: 100.0000 (99.8884)  time: 45.6925  data: 0.0228\n",
      "Epoch: [4]  [21/40]  eta: 0:14:26  lr: 0.0001  img/s: 2.7860274703907306  loss: 0.1285 (0.1339)  acc1: 96.0938 (96.3423)  acc5: 100.0000 (99.8935)  time: 45.7767  data: 0.0228\n",
      "Epoch: [4]  [22/40]  eta: 0:13:41  lr: 0.0001  img/s: 2.785138457543275  loss: 0.1362 (0.1340)  acc1: 96.0938 (96.3315)  acc5: 100.0000 (99.8981)  time: 45.8428  data: 0.0229\n",
      "Epoch: [4]  [23/40]  eta: 0:12:56  lr: 0.0001  img/s: 2.7746234805329046  loss: 0.1362 (0.1327)  acc1: 96.0938 (96.3867)  acc5: 100.0000 (99.9023)  time: 45.8921  data: 0.0219\n",
      "Epoch: [4]  [24/40]  eta: 0:12:10  lr: 0.0001  img/s: 2.794487200836253  loss: 0.1362 (0.1339)  acc1: 96.0938 (96.3750)  acc5: 100.0000 (99.8750)  time: 45.9380  data: 0.0219\n",
      "Epoch: [4]  [25/40]  eta: 0:11:24  lr: 0.0001  img/s: 2.83899882921573  loss: 0.1362 (0.1336)  acc1: 96.0938 (96.3642)  acc5: 100.0000 (99.8798)  time: 45.8905  data: 0.0219\n",
      "Epoch: [4]  [26/40]  eta: 0:10:38  lr: 0.0001  img/s: 2.839861183830625  loss: 0.1263 (0.1317)  acc1: 96.0938 (96.4120)  acc5: 100.0000 (99.8843)  time: 45.8557  data: 0.0219\n",
      "Epoch: [4]  [27/40]  eta: 0:09:52  lr: 0.0001  img/s: 2.8342433345202886  loss: 0.1362 (0.1325)  acc1: 96.0938 (96.3728)  acc5: 100.0000 (99.8884)  time: 45.8277  data: 0.0219\n",
      "Epoch: [4]  [28/40]  eta: 0:09:06  lr: 0.0001  img/s: 2.8575624655137153  loss: 0.1362 (0.1317)  acc1: 96.0938 (96.4440)  acc5: 100.0000 (99.8922)  time: 45.8002  data: 0.0219\n",
      "Epoch: [4]  [29/40]  eta: 0:08:21  lr: 0.0001  img/s: 2.785707065521694  loss: 0.1362 (0.1342)  acc1: 96.0938 (96.3542)  acc5: 100.0000 (99.8698)  time: 45.8222  data: 0.0218\n",
      "Epoch: [4]  [30/40]  eta: 0:07:35  lr: 0.0001  img/s: 2.8064514913234784  loss: 0.1289 (0.1340)  acc1: 96.0938 (96.3458)  acc5: 100.0000 (99.8740)  time: 45.8136  data: 0.0218\n",
      "Epoch: [4]  [31/40]  eta: 0:06:50  lr: 0.0001  img/s: 2.8095987309827053  loss: 0.1362 (0.1354)  acc1: 96.0938 (96.3623)  acc5: 100.0000 (99.8291)  time: 45.7043  data: 0.0214\n",
      "Epoch: [4]  [32/40]  eta: 0:06:04  lr: 0.0001  img/s: 2.8433916086125315  loss: 0.1362 (0.1339)  acc1: 96.0938 (96.3778)  acc5: 100.0000 (99.8343)  time: 45.7253  data: 0.0215\n",
      "Epoch: [4]  [33/40]  eta: 0:05:19  lr: 0.0001  img/s: 2.7867977467958083  loss: 0.1362 (0.1346)  acc1: 96.0938 (96.3465)  acc5: 100.0000 (99.8392)  time: 45.7753  data: 0.0215\n",
      "Epoch: [4]  [34/40]  eta: 0:04:33  lr: 0.0001  img/s: 2.8325500522066362  loss: 0.1289 (0.1333)  acc1: 96.0938 (96.3839)  acc5: 100.0000 (99.8438)  time: 45.6669  data: 0.0215\n",
      "Epoch: [4]  [35/40]  eta: 0:03:47  lr: 0.0001  img/s: 2.844722038324684  loss: 0.1362 (0.1337)  acc1: 96.0938 (96.3976)  acc5: 100.0000 (99.8481)  time: 45.5906  data: 0.0215\n",
      "Epoch: [4]  [36/40]  eta: 0:03:02  lr: 0.0001  img/s: 2.879295073478561  loss: 0.1289 (0.1316)  acc1: 96.0938 (96.4316)  acc5: 100.0000 (99.8522)  time: 45.5396  data: 0.0214\n",
      "Epoch: [4]  [37/40]  eta: 0:02:16  lr: 0.0001  img/s: 2.877891265490215  loss: 0.1263 (0.1300)  acc1: 96.8750 (96.4638)  acc5: 100.0000 (99.8561)  time: 45.4194  data: 0.0215\n",
      "Epoch: [4]  [38/40]  eta: 0:01:30  lr: 0.0001  img/s: 2.8414410272743207  loss: 0.1090 (0.1288)  acc1: 96.8750 (96.4944)  acc5: 100.0000 (99.8598)  time: 45.3721  data: 0.0211\n",
      "Epoch: [4]  [39/40]  eta: 0:00:44  lr: 0.0001  img/s: 3.198771833858193  loss: 0.1263 (0.1350)  acc1: 96.8750 (96.4800)  acc5: 100.0000 (99.8600)  time: 43.2147  data: 0.0202\n",
      "Epoch: [4] Total time: 0:29:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:14<00:00, 37.37s/it]\n",
      "W1117 22:55:23.306390 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.307151 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.308289 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.308887 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.309581 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.310149 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.310730 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.311317 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.311895 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.312455 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.313026 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.313628 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.314203 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.314763 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.315340 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.315923 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.316465 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.317024 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.317613 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.318156 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.318701 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.319250 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.319800 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.320341 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.320890 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.321310 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.321812 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.322273 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.322739 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.323202 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.323682 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.324139 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.324604 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.325056 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.325567 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.326033 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.326500 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.326949 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.327418 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.327888 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.328278 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.328709 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.329147 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.329583 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.330011 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.330434 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.330876 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.331302 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.331790 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.332228 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.332663 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.333082 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.333523 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.333952 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.334389 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.334810 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.335236 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.335647 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.336092 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.336511 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.336935 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.337348 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.337796 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.338220 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.338651 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.339067 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.339512 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.339933 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.340360 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.340791 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.341232 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.341676 134165911848768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 22:55:23.342273 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.342929 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.343557 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.344152 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.344771 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.345367 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.345996 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.346604 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.347205 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.349179 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.349853 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.350455 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.351064 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.351661 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1117 22:55:23.352282 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.352877 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.353487 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.354093 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.354695 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.355286 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.355890 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.356482 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.357075 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.357674 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.358274 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.358865 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.359526 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.360126 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.360736 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.361331 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.361939 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.362547 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.363156 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.363747 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.364363 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.364957 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.365574 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.366166 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.366781 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.367375 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.367966 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.368556 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.369165 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.369772 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.370363 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.370965 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.371573 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.372155 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.372752 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.373354 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.373968 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.374550 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.375151 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.375749 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.376352 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.376938 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.377571 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.378165 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.378759 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.379343 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.379944 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.380537 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.381133 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.381737 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.382355 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.382948 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.383544 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.384126 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.384741 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.385334 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.386031 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 22:55:23.386646 134165911848768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=59.752037048339844 quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1418 calibrator=HistogramCalibrator scale=895.780029296875 quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5655 calibrator=HistogramCalibrator scale=216.89956665039062 quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0556 calibrator=HistogramCalibrator scale=2286.89501953125 quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2578 calibrator=HistogramCalibrator scale=466.114990234375 quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator scale=3880.2880859375 quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4872 calibrator=HistogramCalibrator scale=251.8604278564453 quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0397 calibrator=HistogramCalibrator scale=3218.383544921875 quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2198 calibrator=HistogramCalibrator scale=544.7721557617188 quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0346 calibrator=HistogramCalibrator scale=3677.5966796875 quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4621 calibrator=HistogramCalibrator scale=262.83209228515625 quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator scale=3588.1611328125 quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1841 calibrator=HistogramCalibrator scale=658.8109741210938 quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator scale=5043.546875 quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4288 calibrator=HistogramCalibrator scale=283.473388671875 quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator scale=4422.54931640625 quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1851 calibrator=HistogramCalibrator scale=662.444091796875 quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator scale=4453.1904296875 quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4288 calibrator=HistogramCalibrator scale=283.473388671875 quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator scale=2309.54296875 quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2755 calibrator=HistogramCalibrator scale=440.75396728515625 quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator scale=5458.345703125 quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1359 calibrator=HistogramCalibrator scale=892.7423706054688 quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator scale=5756.8359375 quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2845 calibrator=HistogramCalibrator scale=424.84503173828125 quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=5273.30078125 quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1353 calibrator=HistogramCalibrator scale=885.68798828125 quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator scale=5735.44873046875 quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3019 calibrator=HistogramCalibrator scale=396.5440673828125 quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5767.67333984375 quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1579 calibrator=HistogramCalibrator scale=746.6427001953125 quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator scale=6964.83544921875 quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3568 calibrator=HistogramCalibrator scale=330.2921447753906 quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator scale=7435.51220703125 quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1570 calibrator=HistogramCalibrator scale=753.50927734375 quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator scale=12689.9384765625 quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3568 calibrator=HistogramCalibrator scale=330.2921447753906 quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator scale=5884.2431640625 quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1898 calibrator=HistogramCalibrator scale=631.0810546875 quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator scale=23619.01953125 quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1012 calibrator=HistogramCalibrator scale=1170.029541015625 quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator scale=24773.11328125 quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1823 calibrator=HistogramCalibrator scale=649.49755859375 quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator scale=18965.76953125 quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1016 calibrator=HistogramCalibrator scale=1177.7919921875 quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator scale=21349.779296875 quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2026 calibrator=HistogramCalibrator scale=593.4848022460938 quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator scale=25490.427734375 quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0967 calibrator=HistogramCalibrator scale=1248.19091796875 quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25880.146484375 quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2628 calibrator=HistogramCalibrator scale=463.3384704589844 quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator scale=35528.43359375 quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0825 calibrator=HistogramCalibrator scale=1446.6214599609375 quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator scale=31579.73828125 quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3170 calibrator=HistogramCalibrator scale=382.39434814453125 quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator scale=36529.60546875 quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0921 calibrator=HistogramCalibrator scale=1269.1939697265625 quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator scale=26926.013671875 quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4079 calibrator=HistogramCalibrator scale=295.2275085449219 quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator scale=33544.44921875 quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1548 calibrator=HistogramCalibrator scale=795.2786865234375 quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25662.263671875 quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4079 calibrator=HistogramCalibrator scale=295.2275085449219 quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator scale=6248.654296875 quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7365 calibrator=HistogramCalibrator scale=169.97177124023438 quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator scale=53835.30078125 quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1933 calibrator=HistogramCalibrator scale=640.6594848632812 quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator scale=21318.76171875 quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.1106 calibrator=HistogramCalibrator scale=112.72254943847656 quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator scale=62905.7578125 quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1917 calibrator=HistogramCalibrator scale=650.5660400390625 quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator scale=21765.048828125 quant)\n",
      "Model saved to ./saved/retrained_model_epoch_4.pth after epoch 4\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5]  [ 0/40]  eta: 0:28:06  lr: 0.0001  img/s: 3.0383591813009168  loss: 0.1451 (0.1451)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 42.1513  data: 0.0233\n",
      "Epoch: [5]  [ 1/40]  eta: 0:28:09  lr: 0.0001  img/s: 2.8795351851226303  loss: 0.1282 (0.1367)  acc1: 96.0938 (96.4844)  acc5: 100.0000 (100.0000)  time: 43.3122  data: 0.0223\n",
      "Epoch: [5]  [ 2/40]  eta: 0:27:49  lr: 0.0001  img/s: 2.8338368764406887  loss: 0.1282 (0.1288)  acc1: 96.0938 (96.3542)  acc5: 100.0000 (100.0000)  time: 43.9379  data: 0.0218\n",
      "Epoch: [5]  [ 3/40]  eta: 0:27:16  lr: 0.0001  img/s: 2.8368380070865085  loss: 0.1130 (0.1123)  acc1: 96.0938 (96.8750)  acc5: 100.0000 (100.0000)  time: 44.2388  data: 0.0216\n",
      "Epoch: [5]  [ 4/40]  eta: 0:26:35  lr: 0.0001  img/s: 2.8671100615712803  loss: 0.1130 (0.1095)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 44.3241  data: 0.0215\n",
      "Epoch: [5]  [ 5/40]  eta: 0:25:51  lr: 0.0001  img/s: 2.885727337344602  loss: 0.0984 (0.1061)  acc1: 96.8750 (97.0052)  acc5: 100.0000 (100.0000)  time: 44.3330  data: 0.0215\n",
      "Epoch: [5]  [ 6/40]  eta: 0:25:07  lr: 0.0001  img/s: 2.879626696914916  loss: 0.1130 (0.1086)  acc1: 96.8750 (96.9866)  acc5: 100.0000 (100.0000)  time: 44.3528  data: 0.0215\n",
      "Epoch: [5]  [ 7/40]  eta: 0:24:24  lr: 0.0001  img/s: 2.876337103778567  loss: 0.0984 (0.1037)  acc1: 96.8750 (97.0703)  acc5: 100.0000 (100.0000)  time: 44.3742  data: 0.0216\n",
      "Epoch: [5]  [ 8/40]  eta: 0:23:42  lr: 0.0001  img/s: 2.844665016982415  loss: 0.0984 (0.0980)  acc1: 96.8750 (97.2222)  acc5: 100.0000 (100.0000)  time: 44.4478  data: 0.0237\n",
      "Epoch: [5]  [ 9/40]  eta: 0:22:58  lr: 0.0001  img/s: 2.858868041623284  loss: 0.0984 (0.1023)  acc1: 96.8750 (97.0312)  acc5: 100.0000 (100.0000)  time: 44.4824  data: 0.0234\n",
      "Epoch: [5]  [10/40]  eta: 0:22:16  lr: 0.0001  img/s: 2.8364174673842792  loss: 0.1130 (0.1042)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 44.5429  data: 0.0232\n",
      "Epoch: [5]  [11/40]  eta: 0:21:30  lr: 0.0001  img/s: 2.9015800443059447  loss: 0.0984 (0.0982)  acc1: 96.8750 (97.1354)  acc5: 100.0000 (100.0000)  time: 44.5089  data: 0.0230\n",
      "Epoch: [5]  [12/40]  eta: 0:20:45  lr: 0.0001  img/s: 2.8918348467828654  loss: 0.0984 (0.0942)  acc1: 96.8750 (97.2957)  acc5: 100.0000 (100.0000)  time: 44.4918  data: 0.0230\n",
      "Epoch: [5]  [13/40]  eta: 0:20:02  lr: 0.0001  img/s: 2.829045080475122  loss: 0.0984 (0.0966)  acc1: 96.8750 (97.2656)  acc5: 100.0000 (100.0000)  time: 44.5471  data: 0.0229\n",
      "Epoch: [5]  [14/40]  eta: 0:19:18  lr: 0.0001  img/s: 2.8523164667875576  loss: 0.1079 (0.0974)  acc1: 96.8750 (97.3958)  acc5: 100.0000 (99.9479)  time: 44.5704  data: 0.0227\n",
      "Epoch: [5]  [15/40]  eta: 0:18:34  lr: 0.0001  img/s: 2.850829337858402  loss: 0.0984 (0.0962)  acc1: 96.8750 (97.4121)  acc5: 100.0000 (99.9512)  time: 44.5922  data: 0.0226\n",
      "Epoch: [5]  [16/40]  eta: 0:17:50  lr: 0.0001  img/s: 2.851964120613096  loss: 0.0984 (0.0958)  acc1: 97.6562 (97.4724)  acc5: 100.0000 (99.9540)  time: 44.6105  data: 0.0225\n",
      "Epoch: [5]  [17/40]  eta: 0:17:05  lr: 0.0001  img/s: 2.8863925882411543  loss: 0.0901 (0.0943)  acc1: 96.8750 (97.4392)  acc5: 100.0000 (99.9566)  time: 44.5980  data: 0.0234\n",
      "Epoch: [5]  [18/40]  eta: 0:16:21  lr: 0.0001  img/s: 2.8749081126853895  loss: 0.0984 (0.0950)  acc1: 96.8750 (97.4095)  acc5: 100.0000 (99.9589)  time: 44.5951  data: 0.0233\n",
      "Epoch: [5]  [19/40]  eta: 0:15:36  lr: 0.0001  img/s: 2.8785466571986715  loss: 0.0901 (0.0930)  acc1: 96.8750 (97.4609)  acc5: 100.0000 (99.9609)  time: 44.5897  data: 0.0232\n",
      "Epoch: [5]  [20/40]  eta: 0:14:51  lr: 0.0001  img/s: 2.8734552568259137  loss: 0.0890 (0.0916)  acc1: 97.6562 (97.5446)  acc5: 100.0000 (99.9256)  time: 44.7105  data: 0.0230\n",
      "Epoch: [5]  [21/40]  eta: 0:14:08  lr: 0.0001  img/s: 2.7459479575541113  loss: 0.0890 (0.0929)  acc1: 97.6562 (97.4787)  acc5: 100.0000 (99.9290)  time: 44.8186  data: 0.0230\n",
      "Epoch: [5]  [22/40]  eta: 0:13:24  lr: 0.0001  img/s: 2.847623826725628  loss: 0.0890 (0.0974)  acc1: 97.6562 (97.2486)  acc5: 100.0000 (99.9321)  time: 44.8077  data: 0.0230\n",
      "Epoch: [5]  [23/40]  eta: 0:12:43  lr: 0.0001  img/s: 2.5522383380288773  loss: 0.0890 (0.0962)  acc1: 97.6562 (97.2982)  acc5: 100.0000 (99.9349)  time: 45.0592  data: 0.0230\n",
      "Epoch: [5]  [24/40]  eta: 0:11:59  lr: 0.0001  img/s: 2.7577988383978087  loss: 0.0890 (0.0966)  acc1: 97.6562 (97.2812)  acc5: 100.0000 (99.9375)  time: 45.1516  data: 0.0269\n",
      "Epoch: [5]  [25/40]  eta: 0:11:14  lr: 0.0001  img/s: 2.84996773851768  loss: 0.0777 (0.0949)  acc1: 97.6562 (97.3558)  acc5: 100.0000 (99.9399)  time: 45.1794  data: 0.0268\n",
      "Epoch: [5]  [26/40]  eta: 0:10:30  lr: 0.0001  img/s: 2.7534840961830103  loss: 0.0707 (0.0933)  acc1: 97.6562 (97.3958)  acc5: 100.0000 (99.9421)  time: 45.2812  data: 0.0268\n",
      "Epoch: [5]  [27/40]  eta: 0:09:45  lr: 0.0001  img/s: 2.8125233146533914  loss: 0.0777 (0.0933)  acc1: 98.4375 (97.4330)  acc5: 100.0000 (99.9442)  time: 45.3316  data: 0.0268\n",
      "Epoch: [5]  [28/40]  eta: 0:09:00  lr: 0.0001  img/s: 2.852099024335912  loss: 0.0901 (0.0938)  acc1: 97.6562 (97.3599)  acc5: 100.0000 (99.9461)  time: 45.3248  data: 0.0258\n",
      "Epoch: [5]  [29/40]  eta: 0:08:15  lr: 0.0001  img/s: 2.8237099696978833  loss: 0.0901 (0.0966)  acc1: 97.6562 (97.2917)  acc5: 100.0000 (99.9219)  time: 45.3526  data: 0.0258\n",
      "Epoch: [5]  [30/40]  eta: 0:07:30  lr: 0.0001  img/s: 2.8403494338623982  loss: 0.0777 (0.0954)  acc1: 97.6562 (97.3034)  acc5: 100.0000 (99.9244)  time: 45.3495  data: 0.0258\n",
      "Epoch: [5]  [31/40]  eta: 0:06:45  lr: 0.0001  img/s: 2.8534842374828537  loss: 0.0901 (0.0997)  acc1: 97.6562 (97.2168)  acc5: 100.0000 (99.9023)  time: 45.3867  data: 0.0258\n",
      "Epoch: [5]  [32/40]  eta: 0:06:00  lr: 0.0001  img/s: 2.8573980729672432  loss: 0.0901 (0.0991)  acc1: 97.6562 (97.2301)  acc5: 100.0000 (99.9053)  time: 45.4133  data: 0.0257\n",
      "Epoch: [5]  [33/40]  eta: 0:05:15  lr: 0.0001  img/s: 2.8320689764586082  loss: 0.0901 (0.1004)  acc1: 97.6562 (97.1737)  acc5: 100.0000 (99.9081)  time: 45.4109  data: 0.0257\n",
      "Epoch: [5]  [34/40]  eta: 0:04:30  lr: 0.0001  img/s: 2.8344270411366757  loss: 0.0777 (0.0994)  acc1: 97.6562 (97.1875)  acc5: 100.0000 (99.9107)  time: 45.4251  data: 0.0258\n",
      "Epoch: [5]  [35/40]  eta: 0:03:45  lr: 0.0001  img/s: 2.8466300055576186  loss: 0.0901 (0.1007)  acc1: 96.8750 (97.1788)  acc5: 100.0000 (99.9132)  time: 45.4284  data: 0.0258\n"
     ]
    }
   ],
   "source": [
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "# load model if necessary\n",
    "load_epoch = 2\n",
    "model.load_state_dict(torch.load(f\"./saved/retrained_model_epoch_{load_epoch}.pth\"))\n",
    "calibrate_model(model, data_t)\n",
    "\n",
    "# finetune the model for one epoch based on data_t subset\n",
    "for epoch in range(EPOCHS):\n",
    "    EPOCH_NUM = epoch + 1\n",
    "    if EPOCH_NUM > load_epoch:\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", EPOCH_NUM, 1)\n",
    "        calibrate_model(model, data_t)\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        model_path = f\"./saved/retrained_model_epoch_{EPOCH_NUM}.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Model saved to {model_path} after epoch {EPOCH_NUM}\")\n",
    "    else:\n",
    "        print(f\"Skipping epoch {EPOCH_NUM} as pretrained model exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0bfa0",
   "metadata": {},
   "source": [
    "### Rerun model evaluation after re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ca5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cecab6",
   "metadata": {},
   "source": [
    "Accuracy after re-training:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
