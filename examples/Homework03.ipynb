{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a115dd",
   "metadata": {},
   "source": [
    "## Jakob Ohmayer (4742300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5eef0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab49b35",
   "metadata": {},
   "source": [
    "### Load model ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb694a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183a13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet import resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165c2d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_PLACES=cores\n",
      "env: OMP_PROC_BIND=close\n",
      "env: OMP_WAIT_POLICY=active\n"
     ]
    }
   ],
   "source": [
    "threads = 40\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "#maybe better performance\n",
    "%env OMP_PLACES=cores\n",
    "%env OMP_PROC_BIND=close\n",
    "%env OMP_WAIT_POLICY=active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa06300",
   "metadata": {},
   "source": [
    "### Choose approximate multiplier \n",
    "\n",
    "Here the multiplier SPR12_44 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562689c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "axx_mult = 'SPR12_44'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539e7e1",
   "metadata": {},
   "source": [
    "### Load model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc26796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_SPR12_44/build.ninja...\n",
      "Building extension module PyInit_conv2d_SPR12_44...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): AdaPT_Conv2d(\n",
       "    3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet34(pretrained=True, axx_mult = axx_mult)\n",
    "\n",
    "model.eval() # for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76721ed0",
   "metadata": {},
   "source": [
    "### Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63b4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def val_dataloader(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CIFAR10(root=\"datasets/cifar10_data\", train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    )\n",
    "dataset = CIFAR10(root=\"datasets/cifar10_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "data = val_dataloader()\n",
    "\n",
    "# data_t is used for calibration purposes and is a subset of train-set\n",
    "data_t = DataLoader(trainset_1, batch_size=128,\n",
    "                                            shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa74c5d",
   "metadata": {},
   "source": [
    "### Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters. Is rerun after each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "\n",
    "     for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "         model(image.cpu())\n",
    "         if i >= num_batches:\n",
    "             break\n",
    "\n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "def calibrate_model(model, data_t):\n",
    "    # It is a bit slow since we collect histograms on CPU\n",
    "    with torch.no_grad():\n",
    "        stats = collect_stats(model, data_t, num_batches=2)\n",
    "        amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "        # optional - test different calibration methods\n",
    "        #amax = compute_amax(model, method=\"mse\")\n",
    "        #amax = compute_amax(model, method=\"entropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733cec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:17<00:00,  8.85s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1117 18:53:02.591396 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.592206 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.592847 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.593490 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.594076 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.594689 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.595309 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.595905 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.596551 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.597161 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.597802 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.598406 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.599042 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.599666 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.600318 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.600924 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.601595 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.602194 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.602802 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.603414 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.603990 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.604620 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.605213 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.605803 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.606244 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.606747 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.607234 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.607713 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.608234 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.608735 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.609313 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.609851 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.610399 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.610899 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.611414 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.611913 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.612411 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.612901 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.613420 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.613846 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.614295 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.614761 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.615231 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.615675 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.616173 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.616659 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.617109 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.617583 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.618057 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.618512 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.618975 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.619429 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.619893 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.620342 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.620798 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.621259 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.621736 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.622199 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.622643 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.623098 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.623568 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.624011 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.624466 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.624915 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.625395 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.625855 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.626317 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.626759 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.627222 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.627691 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.628140 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.628583 138092802979648 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1117 18:53:02.629248 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.629749 138092802979648 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W1117 18:53:02.630409 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.631072 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.631713 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.632325 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.632947 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.633595 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.634252 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.634883 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.635519 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.636150 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.636774 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.637424 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1117 18:53:02.638047 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.638700 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.639321 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.639934 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.640563 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.641210 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.641852 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.642493 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.643128 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.643766 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.644378 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.645025 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.645663 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.646296 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.646913 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.647569 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.648185 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.648824 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.649463 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.650106 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.650729 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.651382 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.652083 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.652712 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.653362 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.654006 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.654643 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.655311 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.655940 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.656571 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.657218 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.657856 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.658491 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.659138 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.659759 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.660391 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.661032 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.661681 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.662295 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.662940 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.663568 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.664206 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.664830 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.665489 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.666094 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.666738 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.667376 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.668007 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.668625 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.669257 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.669891 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.670521 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.671153 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.671787 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.672426 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.673062 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.673717 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.674324 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1117 18:53:02.674945 138092802979648 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1418 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6620 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0555 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3258 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6022 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0395 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2948 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0345 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5686 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2238 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0253 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5296 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2200 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.5296 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3411 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1695 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3562 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1734 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3945 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2225 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5033 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2146 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.5033 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0216 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2483 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1362 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2426 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1317 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2586 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1192 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3164 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1072 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3819 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1291 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4973 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1772 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4973 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7772 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2412 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.1735 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2064 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator quant)\n"
     ]
    }
   ],
   "source": [
    "calibrate_model(model, data_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446f0bd",
   "metadata": {},
   "source": [
    "### Run model evaluation (before re-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44bfa498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [29:07<00:00, 22.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1747.6586084780001\n",
      "Accuracy of the network on the 10000 test images: 33.7740 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627d03b",
   "metadata": {},
   "source": [
    "Accuracy before re-training: 33.77%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204303c",
   "metadata": {},
   "source": [
    "### Run approximate-aware re-training for 15 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b8bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "# finetune the model for one epoch based on data_t subset\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", 0, 1)\n",
    "    calibrate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0bfa0",
   "metadata": {},
   "source": [
    "### Rerun model evaluation after re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ca5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cecab6",
   "metadata": {},
   "source": [
    "Accuracy after re-training:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
