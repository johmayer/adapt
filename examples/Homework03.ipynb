{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a115dd",
   "metadata": {},
   "source": [
    "## Jakob Ohmayer (4742300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5eef0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "import multiprocessing\n",
    "import requests\n",
    "import timeit\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab49b35",
   "metadata": {},
   "source": [
    "### Load model ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb694a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183a13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet import resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165c2d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_PLACES=cores\n",
      "env: OMP_PROC_BIND=close\n",
      "env: OMP_WAIT_POLICY=active\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of CPU cores\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "threads = num_cores * 2 # two threads are available on github codespaces per core\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "#maybe better performance\n",
    "%env OMP_PLACES=cores\n",
    "%env OMP_PROC_BIND=close\n",
    "%env OMP_WAIT_POLICY=active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa06300",
   "metadata": {},
   "source": [
    "### Choose approximate multiplier \n",
    "\n",
    "Here the multiplier SPR12_44 is used. The multiplier ``SPR12_44.h`` is included in the folder and needs to be placed under ``/adapt/cpu-kernels/axx_mults`` to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562689c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "axx_mult = 'SPR12_44'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539e7e1",
   "metadata": {},
   "source": [
    "### Load model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc26796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_SPR12_44/build.ninja...\n",
      "Building extension module PyInit_conv2d_SPR12_44...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...\n",
      "Loading extension module PyInit_conv2d_SPR12_44...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): AdaPT_Conv2d(\n",
       "    3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet34(pretrained=True, axx_mult = axx_mult)\n",
    "\n",
    "model.eval() # for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76721ed0",
   "metadata": {},
   "source": [
    "### Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63b4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def val_dataloader(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CIFAR10(root=\"datasets/cifar10_data\", train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    )\n",
    "dataset = CIFAR10(root=\"datasets/cifar10_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "data = val_dataloader()\n",
    "\n",
    "# data_t is used for calibration purposes and is a subset of train-set\n",
    "data_t = DataLoader(trainset_1, batch_size=128,\n",
    "                                            shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa74c5d",
   "metadata": {},
   "source": [
    "### Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters. Is rerun after each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946f0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "\n",
    "     for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "         model(image.cpu())\n",
    "         if i >= num_batches:\n",
    "             break\n",
    "\n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "def calibrate_model(model, data_t):\n",
    "    # It is a bit slow since we collect histograms on CPU\n",
    "    with torch.no_grad():\n",
    "        stats = collect_stats(model, data_t, num_batches=2)\n",
    "        amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "        # optional - test different calibration methods\n",
    "        #amax = compute_amax(model, method=\"mse\")\n",
    "        #amax = compute_amax(model, method=\"entropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7733cec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.25s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1118 13:17:27.472463 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.472995 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.473452 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.473854 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.474242 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.474998 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.475342 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.475764 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.476261 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.476659 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.477203 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.477574 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.478218 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.478542 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.479250 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.479638 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.480420 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.480747 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.481276 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.481631 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.482042 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.482454 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.482838 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.483299 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.483721 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.484297 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.484679 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.485108 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.485456 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.485976 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.486356 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.487019 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.487829 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.488214 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.488739 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.489109 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.489592 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.489999 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.490718 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.491120 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.491537 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.492056 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.492559 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.492938 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.493442 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.497691 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.498460 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.498847 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.499334 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.499773 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.500703 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.501214 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.501992 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.502514 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.502929 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.503534 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.503964 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.504562 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.505280 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.506365 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.506839 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.507246 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.508089 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.508543 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.509289 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.509821 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.510178 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.510816 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.511428 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.511888 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.512454 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.512798 129753190664000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1118 13:17:27.513935 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.514373 129753190664000 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W1118 13:17:27.514966 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.515829 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.516427 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.517022 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.517657 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.518842 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.519639 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.520351 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.522011 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.523117 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.523765 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.524573 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1118 13:17:27.525258 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.525842 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.526487 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.527717 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.528644 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.530295 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.531464 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.532527 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.533547 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.534652 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.535674 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.536504 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.537466 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.538452 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.539325 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.540434 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.541410 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.542229 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.543498 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.544449 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.545052 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.546090 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.547134 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.548302 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.549116 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.549947 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.550751 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.551301 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.552423 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.553246 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.553774 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.554853 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.555672 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.556250 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.557294 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.558193 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.558763 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.559702 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.560515 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.561109 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.562140 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.562859 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.563605 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.564799 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.565341 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.566410 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.566936 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.568033 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.568768 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.569317 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.569882 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.571169 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.572041 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.572887 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.573441 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.574455 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.574999 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.575586 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1118 13:17:27.576661 129753190664000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1418 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6744 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0555 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3303 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6067 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0395 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3021 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0345 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5703 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2226 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0253 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5302 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2195 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.5302 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3383 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1713 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3541 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1738 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3946 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2267 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5142 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2103 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.5142 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0216 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2477 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1363 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2458 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1320 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2586 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1191 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3133 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1083 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3787 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1297 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4931 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1756 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4931 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7797 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2347 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.1806 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2057 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator quant)\n"
     ]
    }
   ],
   "source": [
    "calibrate_model(model, data_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446f0bd",
   "metadata": {},
   "source": [
    "### Run model evaluation (before re-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44bfa498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [15:44<00:00, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944.601157688001\n",
      "Accuracy of the network on the 10000 test images: 33.7440 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627d03b",
   "metadata": {},
   "source": [
    "**Accuracy before re-training**: 33.74% \\\n",
    "This low accuracy is not suprising as firstly a approximate multiplier is used which will reduce the accuracy depending on how much approximation happens. Secondly the model ResNet34 used here is pretrained on ImageNet. For evaluation Cifar10 is used. Therefore the accuracy will be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204303c",
   "metadata": {},
   "source": [
    "### Run approximate-aware re-training for 15 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9facf9f2",
   "metadata": {},
   "source": [
    "As re-training takes quite long and brakes sometimes the individual models are saved after each epoch to enable loading these models to continue fine-tuning. Also the re-training was done using the python script ``Homework03.py`` included in the folder. Therefore the log output of the following cell is not correct. The majority of the output is included in the file ``training.log`` in the same folder. The difference in runtime is due to using a 2-core instance at the beginning and a 4-core instance later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcdb2d9",
   "metadata": {},
   "source": [
    "The pre-trained model after 15 epochs can be found [here](https://mega.nz/file/BRQWCLbJ#wA-qBEAAWPe3Ym9y2nB8v1PATPe_xRHGd1-U7OYnPNs) to reproduce the results. The model needs to be placed under a folder ``saved`` in the ``examples`` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1b8bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1118 13:17:36.388638 129753190664000 tensor_quantizer.py:402] conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.389351 129753190664000 tensor_quantizer.py:402] conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.390566 129753190664000 tensor_quantizer.py:402] layer1.0.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.391044 129753190664000 tensor_quantizer.py:402] layer1.0.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.392193 129753190664000 tensor_quantizer.py:402] layer1.0.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.392657 129753190664000 tensor_quantizer.py:402] layer1.0.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.393955 129753190664000 tensor_quantizer.py:402] layer1.1.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.394437 129753190664000 tensor_quantizer.py:402] layer1.1.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.395371 129753190664000 tensor_quantizer.py:402] layer1.1.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.395950 129753190664000 tensor_quantizer.py:402] layer1.1.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.397411 129753190664000 tensor_quantizer.py:402] layer1.2.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.398015 129753190664000 tensor_quantizer.py:402] layer1.2.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.399045 129753190664000 tensor_quantizer.py:402] layer1.2.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.399540 129753190664000 tensor_quantizer.py:402] layer1.2.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.400571 129753190664000 tensor_quantizer.py:402] layer2.0.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.401059 129753190664000 tensor_quantizer.py:402] layer2.0.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.402365 129753190664000 tensor_quantizer.py:402] layer2.0.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.402815 129753190664000 tensor_quantizer.py:402] layer2.0.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.403976 129753190664000 tensor_quantizer.py:402] layer2.0.downsample.0.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.404560 129753190664000 tensor_quantizer.py:402] layer2.0.downsample.0.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.405481 129753190664000 tensor_quantizer.py:402] layer2.1.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.406016 129753190664000 tensor_quantizer.py:402] layer2.1.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.406888 129753190664000 tensor_quantizer.py:402] layer2.1.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.407428 129753190664000 tensor_quantizer.py:402] layer2.1.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.408379 129753190664000 tensor_quantizer.py:402] layer2.2.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.408919 129753190664000 tensor_quantizer.py:402] layer2.2.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.409781 129753190664000 tensor_quantizer.py:402] layer2.2.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.410333 129753190664000 tensor_quantizer.py:402] layer2.2.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.411226 129753190664000 tensor_quantizer.py:402] layer2.3.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.411778 129753190664000 tensor_quantizer.py:402] layer2.3.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.412676 129753190664000 tensor_quantizer.py:402] layer2.3.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.413335 129753190664000 tensor_quantizer.py:402] layer2.3.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.414272 129753190664000 tensor_quantizer.py:402] layer3.0.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.414902 129753190664000 tensor_quantizer.py:402] layer3.0.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.415965 129753190664000 tensor_quantizer.py:402] layer3.0.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.416529 129753190664000 tensor_quantizer.py:402] layer3.0.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.417362 129753190664000 tensor_quantizer.py:402] layer3.0.downsample.0.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.417978 129753190664000 tensor_quantizer.py:402] layer3.0.downsample.0.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.419145 129753190664000 tensor_quantizer.py:402] layer3.1.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.419753 129753190664000 tensor_quantizer.py:402] layer3.1.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.420822 129753190664000 tensor_quantizer.py:402] layer3.1.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.421482 129753190664000 tensor_quantizer.py:402] layer3.1.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.422605 129753190664000 tensor_quantizer.py:402] layer3.2.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.423045 129753190664000 tensor_quantizer.py:402] layer3.2.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.424162 129753190664000 tensor_quantizer.py:402] layer3.2.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.424634 129753190664000 tensor_quantizer.py:402] layer3.2.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.425753 129753190664000 tensor_quantizer.py:402] layer3.3.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.426257 129753190664000 tensor_quantizer.py:402] layer3.3.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.427419 129753190664000 tensor_quantizer.py:402] layer3.3.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.427851 129753190664000 tensor_quantizer.py:402] layer3.3.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.428993 129753190664000 tensor_quantizer.py:402] layer3.4.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.429437 129753190664000 tensor_quantizer.py:402] layer3.4.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.430601 129753190664000 tensor_quantizer.py:402] layer3.4.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.431057 129753190664000 tensor_quantizer.py:402] layer3.4.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.432295 129753190664000 tensor_quantizer.py:402] layer3.5.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.432751 129753190664000 tensor_quantizer.py:402] layer3.5.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.433900 129753190664000 tensor_quantizer.py:402] layer3.5.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.434520 129753190664000 tensor_quantizer.py:402] layer3.5.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.435758 129753190664000 tensor_quantizer.py:402] layer4.0.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.436227 129753190664000 tensor_quantizer.py:402] layer4.0.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.437709 129753190664000 tensor_quantizer.py:402] layer4.0.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.438122 129753190664000 tensor_quantizer.py:402] layer4.0.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.439224 129753190664000 tensor_quantizer.py:402] layer4.0.downsample.0.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.439674 129753190664000 tensor_quantizer.py:402] layer4.0.downsample.0.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.441034 129753190664000 tensor_quantizer.py:402] layer4.1.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.441670 129753190664000 tensor_quantizer.py:402] layer4.1.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.443117 129753190664000 tensor_quantizer.py:402] layer4.1.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.443548 129753190664000 tensor_quantizer.py:402] layer4.1.conv2.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.444975 129753190664000 tensor_quantizer.py:402] layer4.2.conv1.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.445615 129753190664000 tensor_quantizer.py:402] layer4.2.conv1.quantizer_w: Overwriting amax.\n",
      "W1118 13:17:36.446993 129753190664000 tensor_quantizer.py:402] layer4.2.conv2.quantizer: Overwriting amax.\n",
      "W1118 13:17:36.447642 129753190664000 tensor_quantizer.py:402] layer4.2.conv2.quantizer_w: Overwriting amax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping epoch 1 as pretrained model exists\n",
      "Skipping epoch 2 as pretrained model exists\n",
      "Skipping epoch 3 as pretrained model exists\n",
      "Skipping epoch 4 as pretrained model exists\n",
      "Skipping epoch 5 as pretrained model exists\n",
      "Skipping epoch 6 as pretrained model exists\n",
      "Skipping epoch 7 as pretrained model exists\n",
      "Skipping epoch 8 as pretrained model exists\n",
      "Skipping epoch 9 as pretrained model exists\n",
      "Skipping epoch 10 as pretrained model exists\n",
      "Skipping epoch 11 as pretrained model exists\n",
      "Skipping epoch 12 as pretrained model exists\n",
      "Skipping epoch 13 as pretrained model exists\n",
      "Skipping epoch 14 as pretrained model exists\n",
      "Skipping epoch 15 as pretrained model exists\n"
     ]
    }
   ],
   "source": [
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "# load model if necessary\n",
    "load_epoch = 15\n",
    "if load_epoch > 0:\n",
    "    model.load_state_dict(torch.load(f\"./saved/retrained_model_epoch_{load_epoch}.pth\"))\n",
    "    #calibrate_model(model, data_t)\n",
    "\n",
    "# finetune the model for one epoch based on data_t subset\n",
    "for epoch in range(EPOCHS):\n",
    "    EPOCH_NUM = epoch + 1\n",
    "    if EPOCH_NUM > load_epoch:\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", EPOCH_NUM, 1)\n",
    "        calibrate_model(model, data_t)\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        model_path = f\"./saved/retrained_model_epoch_{EPOCH_NUM}.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Model saved to {model_path} after epoch {EPOCH_NUM}\")\n",
    "    else:\n",
    "        print(f\"Skipping epoch {EPOCH_NUM} as pretrained model exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0bfa0",
   "metadata": {},
   "source": [
    "### Rerun model evaluation after re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be1ca5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [15:45<00:00, 12.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945.4669707930002\n",
      "Accuracy of the network on the 10000 test images: 87.8205 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cecab6",
   "metadata": {},
   "source": [
    "**Accuracy after re-training**: 87.14% \\\n",
    "(runtime speed before and after fine-tuning is the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7fe06",
   "metadata": {},
   "source": [
    "As expected the accuracy increases from 33.74% to 87.14%. This is due to two reasons. Firstly now the re-training happens with the Cifar10 dataset compared to the pre-trained model trained on ImageNet. While the pre-trained model will without approximation still have a decent performance as both datasets are similar re-training will increase the accuracy for this specific domain. \\\n",
    "The bigger impact comes from doing approximate aware re-training. The model adjusts the weights to facilitate an approximate multiplication in the process. As a neural network is inherently unprecise and deals with that problem internally already this works quite well and will increase the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
