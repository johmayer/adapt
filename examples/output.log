Using /root/.cache/torch_extensions as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_SPR12_44/build.ninja...
Building extension module PyInit_conv2d_SPR12_44...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Using /root/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module PyInit_conv2d_SPR12_44, skipping build step...
Loading extension module PyInit_conv2d_SPR12_44...
Files already downloaded and verified
Files already downloaded and verified
  0%|                                                     | 0/2 [00:00<?, ?it/s] 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.31s/it]100%|█████████████████████████████████████████████| 2/2 [00:04<00:00,  2.63s/it]100%|█████████████████████████████████████████████| 2/2 [00:08<00:00,  4.21s/it]
WARNING: Logging before flag parsing goes to stderr.
W1118 08:26:52.143500 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.143712 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.143819 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.143882 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.143953 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144013 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144081 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144140 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144207 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144264 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144330 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144389 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144454 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144511 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144579 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144636 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144705 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144763 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144829 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144887 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.144954 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145012 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145076 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145133 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145198 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145255 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145320 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145378 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145445 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145503 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145567 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145625 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145693 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145750 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145815 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145872 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145936 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.145994 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146065 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146123 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146189 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146249 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146314 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146370 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146441 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146499 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146566 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146625 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146692 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146750 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146816 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146872 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146935 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.146992 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147056 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147114 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147179 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147235 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147301 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147358 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147423 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147479 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147545 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147604 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147670 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147728 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147815 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147879 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.147951 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.148012 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.148079 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.148139 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:26:52.148335 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
W1118 08:26:52.148401 128479731717952 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.
conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)
W1118 08:26:52.148602 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1418 calibrator=HistogramCalibrator quant)
W1118 08:26:52.148803 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6880 calibrator=HistogramCalibrator quant)
W1118 08:26:52.148984 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0555 calibrator=HistogramCalibrator quant)
W1118 08:26:52.149169 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3217 calibrator=HistogramCalibrator quant)
W1118 08:26:52.149348 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator quant)
W1118 08:26:52.149538 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6153 calibrator=HistogramCalibrator quant)
W1118 08:26:52.149716 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0395 calibrator=HistogramCalibrator quant)
W1118 08:26:52.149899 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2943 calibrator=HistogramCalibrator quant)
W1118 08:26:52.150073 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0345 calibrator=HistogramCalibrator quant)
W1118 08:26:52.150254 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5789 calibrator=HistogramCalibrator quant)
W1118 08:26:52.150430 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)
W1118 08:26:52.150611 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2234 calibrator=HistogramCalibrator quant)
W1118 08:26:52.150786 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0253 calibrator=HistogramCalibrator quant)
W1118 08:26:52.150966 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5416 calibrator=HistogramCalibrator quant)
W1118 08:26:52.151139 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)
W1118 08:26:52.151317 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2200 calibrator=HistogramCalibrator quant)
W1118 08:26:52.151489 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator quant)
W1118 08:26:52.151666 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.5416 calibrator=HistogramCalibrator quant)
W1118 08:26:52.151859 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator quant)
W1118 08:26:52.152041 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3424 calibrator=HistogramCalibrator quant)
W1118 08:26:52.152214 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator quant)
W1118 08:26:52.152401 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1683 calibrator=HistogramCalibrator quant)
W1118 08:26:52.152585 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator quant)
W1118 08:26:52.152774 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3579 calibrator=HistogramCalibrator quant)
W1118 08:26:52.152949 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator quant)
W1118 08:26:52.153126 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1768 calibrator=HistogramCalibrator quant)
W1118 08:26:52.153301 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator quant)
W1118 08:26:52.153485 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3985 calibrator=HistogramCalibrator quant)
W1118 08:26:52.153658 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator quant)
W1118 08:26:52.153838 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2248 calibrator=HistogramCalibrator quant)
W1118 08:26:52.154011 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator quant)
W1118 08:26:52.154190 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5086 calibrator=HistogramCalibrator quant)
W1118 08:26:52.154364 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator quant)
W1118 08:26:52.154542 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2138 calibrator=HistogramCalibrator quant)
W1118 08:26:52.154716 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)
W1118 08:26:52.154897 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.5086 calibrator=HistogramCalibrator quant)
W1118 08:26:52.155074 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0216 calibrator=HistogramCalibrator quant)
W1118 08:26:52.155258 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2492 calibrator=HistogramCalibrator quant)
W1118 08:26:52.155432 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator quant)
W1118 08:26:52.155608 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1366 calibrator=HistogramCalibrator quant)
W1118 08:26:52.155801 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator quant)
W1118 08:26:52.155984 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2455 calibrator=HistogramCalibrator quant)
W1118 08:26:52.156162 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator quant)
W1118 08:26:52.156343 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1325 calibrator=HistogramCalibrator quant)
W1118 08:26:52.156518 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator quant)
W1118 08:26:52.156699 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2599 calibrator=HistogramCalibrator quant)
W1118 08:26:52.156872 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator quant)
W1118 08:26:52.157054 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1201 calibrator=HistogramCalibrator quant)
W1118 08:26:52.157228 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator quant)
W1118 08:26:52.157408 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3175 calibrator=HistogramCalibrator quant)
W1118 08:26:52.157581 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator quant)
W1118 08:26:52.157760 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1107 calibrator=HistogramCalibrator quant)
W1118 08:26:52.157932 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)
W1118 08:26:52.158110 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3815 calibrator=HistogramCalibrator quant)
W1118 08:26:52.158282 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator quant)
W1118 08:26:52.158468 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1304 calibrator=HistogramCalibrator quant)
W1118 08:26:52.158642 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator quant)
W1118 08:26:52.158831 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5018 calibrator=HistogramCalibrator quant)
W1118 08:26:52.159008 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator quant)
W1118 08:26:52.159188 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1761 calibrator=HistogramCalibrator quant)
W1118 08:26:52.159362 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator quant)
W1118 08:26:52.159539 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.5018 calibrator=HistogramCalibrator quant)
W1118 08:26:52.159713 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator quant)
W1118 08:26:52.159913 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7776 calibrator=HistogramCalibrator quant)
W1118 08:26:52.160087 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)
W1118 08:26:52.160264 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2366 calibrator=HistogramCalibrator quant)
W1118 08:26:52.160437 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator quant)
W1118 08:26:52.160615 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.1797 calibrator=HistogramCalibrator quant)
W1118 08:26:52.160791 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator quant)
W1118 08:26:52.160968 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2077 calibrator=HistogramCalibrator quant)
W1118 08:26:52.161142 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator quant)
W1118 08:26:52.377022 128479731717952 tensor_quantizer.py:402] conv1.quantizer: Overwriting amax.
W1118 08:26:52.377223 128479731717952 tensor_quantizer.py:402] conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.377683 128479731717952 tensor_quantizer.py:402] layer1.0.conv1.quantizer: Overwriting amax.
W1118 08:26:52.377804 128479731717952 tensor_quantizer.py:402] layer1.0.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.378098 128479731717952 tensor_quantizer.py:402] layer1.0.conv2.quantizer: Overwriting amax.
W1118 08:26:52.378208 128479731717952 tensor_quantizer.py:402] layer1.0.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.378522 128479731717952 tensor_quantizer.py:402] layer1.1.conv1.quantizer: Overwriting amax.
W1118 08:26:52.378641 128479731717952 tensor_quantizer.py:402] layer1.1.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.378985 128479731717952 tensor_quantizer.py:402] layer1.1.conv2.quantizer: Overwriting amax.
W1118 08:26:52.379100 128479731717952 tensor_quantizer.py:402] layer1.1.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.379415 128479731717952 tensor_quantizer.py:402] layer1.2.conv1.quantizer: Overwriting amax.
W1118 08:26:52.379528 128479731717952 tensor_quantizer.py:402] layer1.2.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.379848 128479731717952 tensor_quantizer.py:402] layer1.2.conv2.quantizer: Overwriting amax.
W1118 08:26:52.379962 128479731717952 tensor_quantizer.py:402] layer1.2.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.380331 128479731717952 tensor_quantizer.py:402] layer2.0.conv1.quantizer: Overwriting amax.
W1118 08:26:52.380444 128479731717952 tensor_quantizer.py:402] layer2.0.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.380775 128479731717952 tensor_quantizer.py:402] layer2.0.conv2.quantizer: Overwriting amax.
W1118 08:26:52.380890 128479731717952 tensor_quantizer.py:402] layer2.0.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.381126 128479731717952 tensor_quantizer.py:402] layer2.0.downsample.0.quantizer: Overwriting amax.
W1118 08:26:52.381230 128479731717952 tensor_quantizer.py:402] layer2.0.downsample.0.quantizer_w: Overwriting amax.
W1118 08:26:52.381562 128479731717952 tensor_quantizer.py:402] layer2.1.conv1.quantizer: Overwriting amax.
W1118 08:26:52.381671 128479731717952 tensor_quantizer.py:402] layer2.1.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.381993 128479731717952 tensor_quantizer.py:402] layer2.1.conv2.quantizer: Overwriting amax.
W1118 08:26:52.382101 128479731717952 tensor_quantizer.py:402] layer2.1.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.382435 128479731717952 tensor_quantizer.py:402] layer2.2.conv1.quantizer: Overwriting amax.
W1118 08:26:52.382544 128479731717952 tensor_quantizer.py:402] layer2.2.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.382867 128479731717952 tensor_quantizer.py:402] layer2.2.conv2.quantizer: Overwriting amax.
W1118 08:26:52.382976 128479731717952 tensor_quantizer.py:402] layer2.2.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.383293 128479731717952 tensor_quantizer.py:402] layer2.3.conv1.quantizer: Overwriting amax.
W1118 08:26:52.383401 128479731717952 tensor_quantizer.py:402] layer2.3.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.383733 128479731717952 tensor_quantizer.py:402] layer2.3.conv2.quantizer: Overwriting amax.
W1118 08:26:52.383864 128479731717952 tensor_quantizer.py:402] layer2.3.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.384286 128479731717952 tensor_quantizer.py:402] layer3.0.conv1.quantizer: Overwriting amax.
W1118 08:26:52.384406 128479731717952 tensor_quantizer.py:402] layer3.0.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.384823 128479731717952 tensor_quantizer.py:402] layer3.0.conv2.quantizer: Overwriting amax.
W1118 08:26:52.384935 128479731717952 tensor_quantizer.py:402] layer3.0.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.385182 128479731717952 tensor_quantizer.py:402] layer3.0.downsample.0.quantizer: Overwriting amax.
W1118 08:26:52.385289 128479731717952 tensor_quantizer.py:402] layer3.0.downsample.0.quantizer_w: Overwriting amax.
W1118 08:26:52.385698 128479731717952 tensor_quantizer.py:402] layer3.1.conv1.quantizer: Overwriting amax.
W1118 08:26:52.385806 128479731717952 tensor_quantizer.py:402] layer3.1.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.386201 128479731717952 tensor_quantizer.py:402] layer3.1.conv2.quantizer: Overwriting amax.
W1118 08:26:52.386309 128479731717952 tensor_quantizer.py:402] layer3.1.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.386715 128479731717952 tensor_quantizer.py:402] layer3.2.conv1.quantizer: Overwriting amax.
W1118 08:26:52.386823 128479731717952 tensor_quantizer.py:402] layer3.2.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.387246 128479731717952 tensor_quantizer.py:402] layer3.2.conv2.quantizer: Overwriting amax.
W1118 08:26:52.387358 128479731717952 tensor_quantizer.py:402] layer3.2.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.387822 128479731717952 tensor_quantizer.py:402] layer3.3.conv1.quantizer: Overwriting amax.
W1118 08:26:52.387935 128479731717952 tensor_quantizer.py:402] layer3.3.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.388354 128479731717952 tensor_quantizer.py:402] layer3.3.conv2.quantizer: Overwriting amax.
W1118 08:26:52.388471 128479731717952 tensor_quantizer.py:402] layer3.3.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.388873 128479731717952 tensor_quantizer.py:402] layer3.4.conv1.quantizer: Overwriting amax.
W1118 08:26:52.388972 128479731717952 tensor_quantizer.py:402] layer3.4.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.389369 128479731717952 tensor_quantizer.py:402] layer3.4.conv2.quantizer: Overwriting amax.
W1118 08:26:52.389469 128479731717952 tensor_quantizer.py:402] layer3.4.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.389869 128479731717952 tensor_quantizer.py:402] layer3.5.conv1.quantizer: Overwriting amax.
W1118 08:26:52.389967 128479731717952 tensor_quantizer.py:402] layer3.5.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.390347 128479731717952 tensor_quantizer.py:402] layer3.5.conv2.quantizer: Overwriting amax.
W1118 08:26:52.390441 128479731717952 tensor_quantizer.py:402] layer3.5.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.390999 128479731717952 tensor_quantizer.py:402] layer4.0.conv1.quantizer: Overwriting amax.
W1118 08:26:52.391094 128479731717952 tensor_quantizer.py:402] layer4.0.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.391874 128479731717952 tensor_quantizer.py:402] layer4.0.conv2.quantizer: Overwriting amax.
W1118 08:26:52.391984 128479731717952 tensor_quantizer.py:402] layer4.0.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.392290 128479731717952 tensor_quantizer.py:402] layer4.0.downsample.0.quantizer: Overwriting amax.
W1118 08:26:52.392384 128479731717952 tensor_quantizer.py:402] layer4.0.downsample.0.quantizer_w: Overwriting amax.
W1118 08:26:52.393124 128479731717952 tensor_quantizer.py:402] layer4.1.conv1.quantizer: Overwriting amax.
W1118 08:26:52.393221 128479731717952 tensor_quantizer.py:402] layer4.1.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.393978 128479731717952 tensor_quantizer.py:402] layer4.1.conv2.quantizer: Overwriting amax.
W1118 08:26:52.394094 128479731717952 tensor_quantizer.py:402] layer4.1.conv2.quantizer_w: Overwriting amax.
W1118 08:26:52.394864 128479731717952 tensor_quantizer.py:402] layer4.2.conv1.quantizer: Overwriting amax.
W1118 08:26:52.394970 128479731717952 tensor_quantizer.py:402] layer4.2.conv1.quantizer_w: Overwriting amax.
W1118 08:26:52.395726 128479731717952 tensor_quantizer.py:402] layer4.2.conv2.quantizer: Overwriting amax.
W1118 08:26:52.395855 128479731717952 tensor_quantizer.py:402] layer4.2.conv2.quantizer_w: Overwriting amax.
  0%|                                                     | 0/2 [00:00<?, ?it/s] 50%|██████████████████████▌                      | 1/2 [00:12<00:12, 12.72s/it]100%|█████████████████████████████████████████████| 2/2 [00:25<00:00, 12.47s/it]100%|█████████████████████████████████████████████| 2/2 [00:37<00:00, 18.67s/it]
W1118 08:27:29.740082 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740207 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740285 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740344 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740408 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740463 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740523 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740577 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740636 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740690 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740749 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740803 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740862 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740916 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.740977 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741031 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741089 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741141 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741199 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741252 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741311 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741364 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741423 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741476 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741535 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741589 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741647 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741700 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741760 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741813 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741870 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741923 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.741982 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742034 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742092 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742144 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742201 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742253 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742313 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742366 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742424 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742478 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742536 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742588 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742859 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742929 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.742994 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743049 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743108 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743162 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743223 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743276 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743334 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743386 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743443 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743495 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743553 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743606 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743666 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743718 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743791 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743849 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743907 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.743959 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.744018 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.744070 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.744125 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.744177 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.744234 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.744285 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.744340 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.744391 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:27:29.744571 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)
W1118 08:27:29.744760 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1418 calibrator=HistogramCalibrator quant)
W1118 08:27:29.744933 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6255 calibrator=HistogramCalibrator quant)
W1118 08:27:29.745094 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0555 calibrator=HistogramCalibrator quant)
W1118 08:27:29.745258 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2895 calibrator=HistogramCalibrator quant)
W1118 08:27:29.745416 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator quant)
W1118 08:27:29.745638 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5549 calibrator=HistogramCalibrator quant)
W1118 08:27:29.745807 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0395 calibrator=HistogramCalibrator quant)
W1118 08:27:29.745968 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2565 calibrator=HistogramCalibrator quant)
W1118 08:27:29.746123 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0345 calibrator=HistogramCalibrator quant)
W1118 08:27:29.746306 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5255 calibrator=HistogramCalibrator quant)
W1118 08:27:29.746467 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)
W1118 08:27:29.746630 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2023 calibrator=HistogramCalibrator quant)
W1118 08:27:29.746800 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator quant)
W1118 08:27:29.746965 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4911 calibrator=HistogramCalibrator quant)
W1118 08:27:29.747122 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator quant)
W1118 08:27:29.747281 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2027 calibrator=HistogramCalibrator quant)
W1118 08:27:29.747437 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator quant)
W1118 08:27:29.747602 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4911 calibrator=HistogramCalibrator quant)
W1118 08:27:29.747761 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator quant)
W1118 08:27:29.747940 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3079 calibrator=HistogramCalibrator quant)
W1118 08:27:29.748097 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator quant)
W1118 08:27:29.748257 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1515 calibrator=HistogramCalibrator quant)
W1118 08:27:29.748414 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator quant)
W1118 08:27:29.748630 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3224 calibrator=HistogramCalibrator quant)
W1118 08:27:29.748795 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator quant)
W1118 08:27:29.748954 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1555 calibrator=HistogramCalibrator quant)
W1118 08:27:29.749121 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator quant)
W1118 08:27:29.749286 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3490 calibrator=HistogramCalibrator quant)
W1118 08:27:29.749442 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator quant)
W1118 08:27:29.749604 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1869 calibrator=HistogramCalibrator quant)
W1118 08:27:29.749760 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator quant)
W1118 08:27:29.749926 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4267 calibrator=HistogramCalibrator quant)
W1118 08:27:29.750081 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator quant)
W1118 08:27:29.750248 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1834 calibrator=HistogramCalibrator quant)
W1118 08:27:29.750404 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)
W1118 08:27:29.750575 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4267 calibrator=HistogramCalibrator quant)
W1118 08:27:29.750731 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0216 calibrator=HistogramCalibrator quant)
W1118 08:27:29.750897 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2166 calibrator=HistogramCalibrator quant)
W1118 08:27:29.751054 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator quant)
W1118 08:27:29.751224 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1185 calibrator=HistogramCalibrator quant)
W1118 08:27:29.751380 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator quant)
W1118 08:27:29.751611 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2111 calibrator=HistogramCalibrator quant)
W1118 08:27:29.751786 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator quant)
W1118 08:27:29.751954 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1179 calibrator=HistogramCalibrator quant)
W1118 08:27:29.752111 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator quant)
W1118 08:27:29.752273 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2314 calibrator=HistogramCalibrator quant)
W1118 08:27:29.752438 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator quant)
W1118 08:27:29.752599 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1084 calibrator=HistogramCalibrator quant)
W1118 08:27:29.752762 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator quant)
W1118 08:27:29.752923 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2913 calibrator=HistogramCalibrator quant)
W1118 08:27:29.753080 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator quant)
W1118 08:27:29.753240 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0964 calibrator=HistogramCalibrator quant)
W1118 08:27:29.753395 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)
W1118 08:27:29.753564 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3554 calibrator=HistogramCalibrator quant)
W1118 08:27:29.753719 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator quant)
W1118 08:27:29.753880 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1120 calibrator=HistogramCalibrator quant)
W1118 08:27:29.754035 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator quant)
W1118 08:27:29.754197 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4581 calibrator=HistogramCalibrator quant)
W1118 08:27:29.754407 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator quant)
W1118 08:27:29.754578 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1667 calibrator=HistogramCalibrator quant)
W1118 08:27:29.754734 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator quant)
W1118 08:27:29.754899 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4581 calibrator=HistogramCalibrator quant)
W1118 08:27:29.755056 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator quant)
W1118 08:27:29.755217 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7586 calibrator=HistogramCalibrator quant)
W1118 08:27:29.755372 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)
W1118 08:27:29.755532 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2102 calibrator=HistogramCalibrator quant)
W1118 08:27:29.755686 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator quant)
W1118 08:27:29.755864 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.1439 calibrator=HistogramCalibrator quant)
W1118 08:27:29.756021 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator quant)
W1118 08:27:29.756180 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2017 calibrator=HistogramCalibrator quant)
W1118 08:27:29.756334 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator quant)
Skipping epoch 1 as pretrained model exists
Skipping epoch 2 as pretrained model exists
Skipping epoch 3 as pretrained model exists
Skipping epoch 4 as pretrained model exists
Skipping epoch 5 as pretrained model exists
Skipping epoch 6 as pretrained model exists
Epoch 7/15
Epoch: [7]  [ 0/40]  eta: 0:15:15  lr: 0.0001  img/s: 5.59860648984665  loss: 0.1198 (0.1198)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 22.8873  data: 0.0245
Epoch: [7]  [ 1/40]  eta: 0:14:32  lr: 0.0001  img/s: 5.8621685284928295  loss: 0.1148 (0.1173)  acc1: 96.8750 (97.2656)  acc5: 100.0000 (100.0000)  time: 22.3732  data: 0.0243
Epoch: [7]  [ 2/40]  eta: 0:13:58  lr: 0.0001  img/s: 5.965194509710388  loss: 0.1148 (0.0954)  acc1: 97.6562 (97.9167)  acc5: 100.0000 (100.0000)  time: 22.0763  data: 0.0245
Epoch: [7]  [ 3/40]  eta: 0:13:31  lr: 0.0001  img/s: 5.97025533037351  loss: 0.0809 (0.0918)  acc1: 96.8750 (97.6562)  acc5: 100.0000 (100.0000)  time: 21.9231  data: 0.0243
Epoch: [7]  [ 4/40]  eta: 0:13:06  lr: 0.0001  img/s: 5.962501331195823  loss: 0.0809 (0.0881)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 21.8367  data: 0.0242
Epoch: [7]  [ 5/40]  eta: 0:12:40  lr: 0.0001  img/s: 6.045709617606673  loss: 0.0735 (0.0826)  acc1: 97.6562 (98.0469)  acc5: 100.0000 (100.0000)  time: 21.7299  data: 0.0241
Epoch: [7]  [ 6/40]  eta: 0:12:16  lr: 0.0001  img/s: 6.0315080402706815  loss: 0.0735 (0.0794)  acc1: 97.6562 (97.9911)  acc5: 100.0000 (100.0000)  time: 21.6608  data: 0.0241
Epoch: [7]  [ 7/40]  eta: 0:11:53  lr: 0.0001  img/s: 6.0281042775495415  loss: 0.0628 (0.0773)  acc1: 97.6562 (98.0469)  acc5: 100.0000 (100.0000)  time: 21.6105  data: 0.0242
Epoch: [7]  [ 8/40]  eta: 0:11:29  lr: 0.0001  img/s: 6.068279426708337  loss: 0.0735 (0.0794)  acc1: 97.6562 (97.9167)  acc5: 100.0000 (100.0000)  time: 21.5557  data: 0.0242
Epoch: [7]  [ 9/40]  eta: 0:11:15  lr: 0.0001  img/s: 5.336892329108324  loss: 0.0735 (0.0812)  acc1: 97.6562 (97.9688)  acc5: 100.0000 (99.9219)  time: 21.8009  data: 0.0241
Epoch: [7]  [10/40]  eta: 0:10:52  lr: 0.0001  img/s: 5.992853818327816  loss: 0.0809 (0.0833)  acc1: 97.6562 (97.9403)  acc5: 100.0000 (99.9290)  time: 21.7634  data: 0.0246
Epoch: [7]  [11/40]  eta: 0:10:29  lr: 0.0001  img/s: 6.057334120863282  loss: 0.0735 (0.0814)  acc1: 97.6562 (97.9818)  acc5: 100.0000 (99.9349)  time: 21.7127  data: 0.0245
Epoch: [7]  [12/40]  eta: 0:10:07  lr: 0.0001  img/s: 5.9871633348511155  loss: 0.0809 (0.0825)  acc1: 97.6562 (97.8365)  acc5: 100.0000 (99.9399)  time: 21.6889  data: 0.0245
Epoch: [7]  [13/40]  eta: 0:09:44  lr: 0.0001  img/s: 6.016668048479601  loss: 0.0809 (0.0825)  acc1: 97.6562 (97.8237)  acc5: 100.0000 (99.9442)  time: 21.6610  data: 0.0244
Epoch: [7]  [14/40]  eta: 0:09:22  lr: 0.0001  img/s: 6.035711967774454  loss: 0.0825 (0.0894)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (99.8958)  time: 21.6323  data: 0.0244
Epoch: [7]  [15/40]  eta: 0:08:59  lr: 0.0001  img/s: 6.08127896962125  loss: 0.0825 (0.0918)  acc1: 97.6562 (97.5586)  acc5: 100.0000 (99.9023)  time: 21.5973  data: 0.0244
Epoch: [7]  [16/40]  eta: 0:08:37  lr: 0.0001  img/s: 6.103809169739201  loss: 0.0825 (0.0900)  acc1: 97.6562 (97.6103)  acc5: 100.0000 (99.9081)  time: 21.5624  data: 0.0249
Epoch: [7]  [17/40]  eta: 0:08:15  lr: 0.0001  img/s: 6.074323905771304  loss: 0.0809 (0.0864)  acc1: 97.6562 (97.7431)  acc5: 100.0000 (99.9132)  time: 21.5365  data: 0.0248
Epoch: [7]  [18/40]  eta: 0:07:52  lr: 0.0001  img/s: 6.154389147164083  loss: 0.0825 (0.0886)  acc1: 97.6562 (97.6974)  acc5: 100.0000 (99.9178)  time: 21.4989  data: 0.0248
Epoch: [7]  [19/40]  eta: 0:07:30  lr: 0.0001  img/s: 6.17588364957281  loss: 0.0809 (0.0864)  acc1: 97.6562 (97.7734)  acc5: 100.0000 (99.9219)  time: 21.4616  data: 0.0249
Epoch: [7]  [20/40]  eta: 0:07:08  lr: 0.0001  img/s: 6.217169758566657  loss: 0.0797 (0.0861)  acc1: 97.6562 (97.8051)  acc5: 100.0000 (99.9256)  time: 21.3478  data: 0.0249
Epoch: [7]  [21/40]  eta: 0:06:46  lr: 0.0001  img/s: 6.177954783643355  loss: 0.0797 (0.0862)  acc1: 97.6562 (97.7628)  acc5: 100.0000 (99.9290)  time: 21.2920  data: 0.0249
Epoch: [7]  [22/40]  eta: 0:06:24  lr: 0.0001  img/s: 6.177952082155726  loss: 0.0809 (0.0881)  acc1: 97.6562 (97.7242)  acc5: 100.0000 (99.9321)  time: 21.2550  data: 0.0248
Epoch: [7]  [23/40]  eta: 0:06:02  lr: 0.0001  img/s: 6.195563583797686  loss: 0.0797 (0.0859)  acc1: 97.6562 (97.7865)  acc5: 100.0000 (99.9349)  time: 21.2160  data: 0.0248
Epoch: [7]  [24/40]  eta: 0:05:40  lr: 0.0001  img/s: 6.201767778632603  loss: 0.0825 (0.0858)  acc1: 97.6562 (97.7500)  acc5: 100.0000 (99.9375)  time: 21.1746  data: 0.0248
Epoch: [7]  [25/40]  eta: 0:05:19  lr: 0.0001  img/s: 6.197423652931013  loss: 0.0825 (0.0856)  acc1: 97.6562 (97.7764)  acc5: 100.0000 (99.9399)  time: 21.1487  data: 0.0249
Epoch: [7]  [26/40]  eta: 0:04:57  lr: 0.0001  img/s: 6.181916425380577  loss: 0.0825 (0.0849)  acc1: 97.6562 (97.7720)  acc5: 100.0000 (99.9421)  time: 21.1229  data: 0.0248
Epoch: [7]  [27/40]  eta: 0:04:36  lr: 0.0001  img/s: 6.209048011043614  loss: 0.0825 (0.0839)  acc1: 97.6562 (97.8237)  acc5: 100.0000 (99.9442)  time: 21.0920  data: 0.0249
Epoch: [7]  [28/40]  eta: 0:04:14  lr: 0.0001  img/s: 6.189887299899813  loss: 0.0807 (0.0833)  acc1: 97.6562 (97.8448)  acc5: 100.0000 (99.9461)  time: 21.0713  data: 0.0249
Epoch: [7]  [29/40]  eta: 0:03:53  lr: 0.0001  img/s: 6.194628678639383  loss: 0.0807 (0.0853)  acc1: 97.6562 (97.8125)  acc5: 100.0000 (99.9219)  time: 20.9053  data: 0.0249
Epoch: [7]  [30/40]  eta: 0:03:31  lr: 0.0001  img/s: 6.193041530336526  loss: 0.0807 (0.0855)  acc1: 97.6562 (97.7823)  acc5: 100.0000 (99.9244)  time: 20.8705  data: 0.0247
Epoch: [7]  [31/40]  eta: 0:03:10  lr: 0.0001  img/s: 6.229487925521503  loss: 0.0825 (0.0867)  acc1: 97.6562 (97.7783)  acc5: 100.0000 (99.9268)  time: 20.8414  data: 0.0247
Epoch: [7]  [32/40]  eta: 0:02:49  lr: 0.0001  img/s: 6.2319885395774595  loss: 0.0807 (0.0858)  acc1: 97.6562 (97.8220)  acc5: 100.0000 (99.9290)  time: 20.7994  data: 0.0248
Epoch: [7]  [33/40]  eta: 0:02:27  lr: 0.0001  img/s: 6.214044132567176  loss: 0.0807 (0.0865)  acc1: 97.6562 (97.8401)  acc5: 100.0000 (99.9311)  time: 20.7657  data: 0.0248
Epoch: [7]  [34/40]  eta: 0:02:06  lr: 0.0001  img/s: 6.211566591410735  loss: 0.0797 (0.0856)  acc1: 98.4375 (97.8571)  acc5: 100.0000 (99.9330)  time: 20.7357  data: 0.0248
Epoch: [7]  [35/40]  eta: 0:01:45  lr: 0.0001  img/s: 6.206129477348307  loss: 0.0797 (0.0868)  acc1: 98.4375 (97.8082)  acc5: 100.0000 (99.9349)  time: 20.7145  data: 0.0248
Epoch: [7]  [36/40]  eta: 0:01:24  lr: 0.0001  img/s: 6.2127334353639245  loss: 0.0807 (0.0871)  acc1: 98.4375 (97.8252)  acc5: 100.0000 (99.9367)  time: 20.6956  data: 0.0244
Epoch: [7]  [37/40]  eta: 0:01:03  lr: 0.0001  img/s: 6.209566373208566  loss: 0.0807 (0.0857)  acc1: 98.4375 (97.8824)  acc5: 100.0000 (99.9383)  time: 20.6727  data: 0.0243
Epoch: [7]  [38/40]  eta: 0:00:42  lr: 0.0001  img/s: 6.2073243504895395  loss: 0.0797 (0.0849)  acc1: 98.4375 (97.8566)  acc5: 100.0000 (99.9399)  time: 20.6638  data: 0.0243
Epoch: [7]  [39/40]  eta: 0:00:20  lr: 0.0001  img/s: 5.525495269285795  loss: 0.0807 (0.0911)  acc1: 98.4375 (97.8600)  acc5: 100.0000 (99.9400)  time: 19.6986  data: 0.0231
Epoch: [7] Total time: 0:13:43
  0%|                                                     | 0/2 [00:00<?, ?it/s] 50%|██████████████████████▌                      | 1/2 [00:12<00:12, 12.31s/it]100%|█████████████████████████████████████████████| 2/2 [00:24<00:00, 12.26s/it]100%|█████████████████████████████████████████████| 2/2 [00:36<00:00, 18.41s/it]
W1118 08:41:49.796727 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.796857 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.796922 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.796969 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797018 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797061 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797110 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797153 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797202 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797244 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797292 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797334 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797380 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797421 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797470 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797511 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797556 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797598 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797645 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797688 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797736 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797777 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797823 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797864 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797911 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797953 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.797999 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798040 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798089 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798130 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798176 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798218 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798267 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798308 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798354 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798395 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798440 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798499 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798562 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798618 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798676 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798731 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798789 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798842 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798896 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.798951 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799041 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799129 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799200 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799256 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799317 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799371 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799429 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799483 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799541 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799595 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799652 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799706 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799782 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799841 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799901 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.799954 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.800012 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.800065 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.800123 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.800176 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.800235 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.800288 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.800347 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.800400 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.800456 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.800509 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:41:49.800689 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=59.752037048339844 quant)
W1118 08:41:49.800889 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1418 calibrator=HistogramCalibrator scale=895.780029296875 quant)
W1118 08:41:49.801071 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5983 calibrator=HistogramCalibrator scale=203.0408477783203 quant)
W1118 08:41:49.801239 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0555 calibrator=HistogramCalibrator scale=2286.89501953125 quant)
W1118 08:41:49.801410 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2675 calibrator=HistogramCalibrator scale=438.61700439453125 quant)
W1118 08:41:49.801576 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator scale=3882.4990234375 quant)
W1118 08:41:49.801749 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5197 calibrator=HistogramCalibrator scale=228.85116577148438 quant)
W1118 08:41:49.801932 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0395 calibrator=HistogramCalibrator scale=3218.383544921875 quant)
W1118 08:41:49.802100 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2335 calibrator=HistogramCalibrator scale=495.1520690917969 quant)
W1118 08:41:49.802264 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0345 calibrator=HistogramCalibrator scale=3677.5966796875 quant)
W1118 08:41:49.802431 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4947 calibrator=HistogramCalibrator scale=241.65455627441406 quant)
W1118 08:41:49.802595 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator scale=3584.5087890625 quant)
W1118 08:41:49.802763 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1895 calibrator=HistogramCalibrator scale=627.7383422851562 quant)
W1118 08:41:49.802926 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator scale=5040.06396484375 quant)
W1118 08:41:49.803094 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4620 calibrator=HistogramCalibrator scale=258.5799865722656 quant)
W1118 08:41:49.803268 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator scale=4415.408203125 quant)
W1118 08:41:49.803435 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1920 calibrator=HistogramCalibrator scale=626.5051879882812 quant)
W1118 08:41:49.803597 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator scale=4453.1904296875 quant)
W1118 08:41:49.803784 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4620 calibrator=HistogramCalibrator scale=258.5799865722656 quant)
W1118 08:41:49.803954 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator scale=2309.54296875 quant)
W1118 08:41:49.804124 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2893 calibrator=HistogramCalibrator scale=412.5054931640625 quant)
W1118 08:41:49.804286 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator scale=5450.48095703125 quant)
W1118 08:41:49.804454 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1424 calibrator=HistogramCalibrator scale=838.434814453125 quant)
W1118 08:41:49.804630 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator scale=5756.8359375 quant)
W1118 08:41:49.804799 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3014 calibrator=HistogramCalibrator scale=393.8868103027344 quant)
W1118 08:41:49.804965 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=5273.30078125 quant)
W1118 08:41:49.805134 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1445 calibrator=HistogramCalibrator scale=816.521240234375 quant)
W1118 08:41:49.805298 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator scale=5735.44873046875 quant)
W1118 08:41:49.805469 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3214 calibrator=HistogramCalibrator scale=363.9205627441406 quant)
W1118 08:41:49.805635 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5767.67333984375 quant)
W1118 08:41:49.805804 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1690 calibrator=HistogramCalibrator scale=679.603271484375 quant)
W1118 08:41:49.805964 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator scale=6969.6787109375 quant)
W1118 08:41:49.806134 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3828 calibrator=HistogramCalibrator scale=297.6195373535156 quant)
W1118 08:41:49.806298 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator scale=7424.8056640625 quant)
W1118 08:41:49.806464 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1668 calibrator=HistogramCalibrator scale=692.5808715820312 quant)
W1118 08:41:49.806628 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator scale=12689.9384765625 quant)
W1118 08:41:49.806801 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3828 calibrator=HistogramCalibrator scale=297.6195373535156 quant)
W1118 08:41:49.806966 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0216 calibrator=HistogramCalibrator scale=5884.2431640625 quant)
W1118 08:41:49.807146 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1998 calibrator=HistogramCalibrator scale=586.415283203125 quant)
W1118 08:41:49.807321 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator scale=23594.54296875 quant)
W1118 08:41:49.807488 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1079 calibrator=HistogramCalibrator scale=1071.57421875 quant)
W1118 08:41:49.807651 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator scale=24752.171875 quant)
W1118 08:41:49.807839 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1936 calibrator=HistogramCalibrator scale=601.5784912109375 quant)
W1118 08:41:49.807982 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator scale=18965.76953125 quant)
W1118 08:41:49.808125 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1095 calibrator=HistogramCalibrator scale=1077.6136474609375 quant)
W1118 08:41:49.808261 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator scale=21349.779296875 quant)
W1118 08:41:49.808403 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2143 calibrator=HistogramCalibrator scale=548.8043823242188 quant)
W1118 08:41:49.808539 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator scale=25468.126953125 quant)
W1118 08:41:49.808682 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1013 calibrator=HistogramCalibrator scale=1172.005859375 quant)
W1118 08:41:49.808817 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25880.146484375 quant)
W1118 08:41:49.808958 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2743 calibrator=HistogramCalibrator scale=436.0123291015625 quant)
W1118 08:41:49.809093 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator scale=35586.0625 quant)
W1118 08:41:49.809233 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0883 calibrator=HistogramCalibrator scale=1316.769775390625 quant)
W1118 08:41:49.809369 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator scale=31525.89453125 quant)
W1118 08:41:49.809525 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3361 calibrator=HistogramCalibrator scale=357.31585693359375 quant)
W1118 08:41:49.809710 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator scale=36465.29296875 quant)
W1118 08:41:49.809879 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1011 calibrator=HistogramCalibrator scale=1134.1688232421875 quant)
W1118 08:41:49.810043 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator scale=26926.013671875 quant)
W1118 08:41:49.810212 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4307 calibrator=HistogramCalibrator scale=277.2099609375 quant)
W1118 08:41:49.810376 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator scale=33544.44921875 quant)
W1118 08:41:49.810542 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1600 calibrator=HistogramCalibrator scale=761.6741943359375 quant)
W1118 08:41:49.810706 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25662.263671875 quant)
W1118 08:41:49.810872 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4307 calibrator=HistogramCalibrator scale=277.2099609375 quant)
W1118 08:41:49.811042 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator scale=6248.654296875 quant)
W1118 08:41:49.811208 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7452 calibrator=HistogramCalibrator scale=167.41177368164062 quant)
W1118 08:41:49.811369 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator scale=53889.515625 quant)
W1118 08:41:49.811531 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1997 calibrator=HistogramCalibrator scale=604.1978759765625 quant)
W1118 08:41:49.811690 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator scale=21318.76171875 quant)
W1118 08:41:49.811877 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.1263 calibrator=HistogramCalibrator scale=111.02532958984375 quant)
W1118 08:41:49.812023 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator scale=62957.48828125 quant)
W1118 08:41:49.812177 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1970 calibrator=HistogramCalibrator scale=629.7628784179688 quant)
W1118 08:41:49.812356 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator scale=21765.048828125 quant)
Model saved to ./saved/retrained_model_epoch_7.pth after epoch 7
Epoch 8/15
Epoch: [8]  [ 0/40]  eta: 0:13:47  lr: 0.0001  img/s: 6.192443568430405  loss: 0.0934 (0.0934)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 20.6948  data: 0.0245
Epoch: [8]  [ 1/40]  eta: 0:13:29  lr: 0.0001  img/s: 6.150601648271832  loss: 0.0693 (0.0813)  acc1: 97.6562 (98.0469)  acc5: 100.0000 (100.0000)  time: 20.7650  data: 0.0243
Epoch: [8]  [ 2/40]  eta: 0:13:07  lr: 0.0001  img/s: 6.206180199159472  loss: 0.0693 (0.0715)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 20.7264  data: 0.0244
Epoch: [8]  [ 3/40]  eta: 0:12:45  lr: 0.0001  img/s: 6.220227182326915  loss: 0.0518 (0.0641)  acc1: 98.4375 (98.8281)  acc5: 100.0000 (100.0000)  time: 20.6953  data: 0.0243
Epoch: [8]  [ 4/40]  eta: 0:12:24  lr: 0.0001  img/s: 6.204026436015838  loss: 0.0693 (0.0692)  acc1: 98.4375 (98.5938)  acc5: 100.0000 (100.0000)  time: 20.6874  data: 0.0242
Epoch: [8]  [ 5/40]  eta: 0:12:04  lr: 0.0001  img/s: 6.177118288490459  loss: 0.0693 (0.0761)  acc1: 98.4375 (98.6979)  acc5: 100.0000 (100.0000)  time: 20.6970  data: 0.0241
Epoch: [8]  [ 6/40]  eta: 0:11:43  lr: 0.0001  img/s: 6.2091771985040864  loss: 0.0693 (0.0722)  acc1: 99.2188 (98.7723)  acc5: 100.0000 (100.0000)  time: 20.6886  data: 0.0241
Epoch: [8]  [ 7/40]  eta: 0:11:22  lr: 0.0001  img/s: 6.210531081811898  loss: 0.0664 (0.0715)  acc1: 98.4375 (98.7305)  acc5: 100.0000 (100.0000)  time: 20.6818  data: 0.0240
Epoch: [8]  [ 8/40]  eta: 0:11:01  lr: 0.0001  img/s: 6.222024717831131  loss: 0.0664 (0.0671)  acc1: 99.2188 (98.8715)  acc5: 100.0000 (100.0000)  time: 20.6723  data: 0.0240
Epoch: [8]  [ 9/40]  eta: 0:10:40  lr: 0.0001  img/s: 6.199457646818883  loss: 0.0664 (0.0795)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (99.8438)  time: 20.6724  data: 0.0243
Epoch: [8]  [10/40]  eta: 0:10:19  lr: 0.0001  img/s: 6.2208943155082945  loss: 0.0664 (0.0762)  acc1: 99.2188 (98.5085)  acc5: 100.0000 (99.8580)  time: 20.6658  data: 0.0243
Epoch: [8]  [11/40]  eta: 0:09:59  lr: 0.0001  img/s: 6.224402649891133  loss: 0.0518 (0.0732)  acc1: 99.2188 (98.5677)  acc5: 100.0000 (99.8698)  time: 20.6594  data: 0.0243
Epoch: [8]  [12/40]  eta: 0:09:38  lr: 0.0001  img/s: 6.219404202316167  loss: 0.0664 (0.0737)  acc1: 99.2188 (98.5577)  acc5: 100.0000 (99.8798)  time: 20.6552  data: 0.0242
Epoch: [8]  [13/40]  eta: 0:09:17  lr: 0.0001  img/s: 6.217089266744626  loss: 0.0664 (0.0784)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (99.8884)  time: 20.6521  data: 0.0242
Epoch: [8]  [14/40]  eta: 0:08:56  lr: 0.0001  img/s: 6.21862941232853  loss: 0.0693 (0.0814)  acc1: 98.4375 (98.3333)  acc5: 100.0000 (99.8438)  time: 20.6491  data: 0.0242
Epoch: [8]  [15/40]  eta: 0:08:36  lr: 0.0001  img/s: 6.220427249598509  loss: 0.0693 (0.0824)  acc1: 98.4375 (98.2910)  acc5: 100.0000 (99.8535)  time: 20.6461  data: 0.0241
Epoch: [8]  [16/40]  eta: 0:08:15  lr: 0.0001  img/s: 6.229704203095508  loss: 0.0693 (0.0805)  acc1: 98.4375 (98.2996)  acc5: 100.0000 (99.8621)  time: 20.6416  data: 0.0241
Epoch: [8]  [17/40]  eta: 0:07:54  lr: 0.0001  img/s: 6.243405120178919  loss: 0.0664 (0.0779)  acc1: 98.4375 (98.3941)  acc5: 100.0000 (99.8698)  time: 20.6352  data: 0.0241
Epoch: [8]  [18/40]  eta: 0:07:33  lr: 0.0001  img/s: 6.234130201841394  loss: 0.0693 (0.0789)  acc1: 98.4375 (98.3964)  acc5: 100.0000 (99.8766)  time: 20.6310  data: 0.0241
Epoch: [8]  [19/40]  eta: 0:07:13  lr: 0.0001  img/s: 6.248008617202722  loss: 0.0664 (0.0767)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (99.8828)  time: 20.6250  data: 0.0240
Epoch: [8]  [20/40]  eta: 0:06:52  lr: 0.0001  img/s: 6.190381268240432  loss: 0.0664 (0.0774)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (99.8512)  time: 20.6253  data: 0.0240
Epoch: [8]  [21/40]  eta: 0:06:31  lr: 0.0001  img/s: 6.250814061575124  loss: 0.0664 (0.0786)  acc1: 98.4375 (98.3665)  acc5: 100.0000 (99.8580)  time: 20.6086  data: 0.0240
Epoch: [8]  [22/40]  eta: 0:06:11  lr: 0.0001  img/s: 6.241561179362065  loss: 0.0795 (0.0803)  acc1: 98.4375 (98.3016)  acc5: 100.0000 (99.8641)  time: 20.6027  data: 0.0240
Epoch: [8]  [23/40]  eta: 0:05:50  lr: 0.0001  img/s: 6.211937737252269  loss: 0.0795 (0.0781)  acc1: 98.4375 (98.3724)  acc5: 100.0000 (99.8698)  time: 20.6041  data: 0.0240
Epoch: [8]  [24/40]  eta: 0:05:29  lr: 0.0001  img/s: 6.230943823959255  loss: 0.0795 (0.0783)  acc1: 98.4375 (98.3438)  acc5: 100.0000 (99.8750)  time: 20.5996  data: 0.0240
Epoch: [8]  [25/40]  eta: 0:05:09  lr: 0.0001  img/s: 6.26824883706534  loss: 0.0795 (0.0787)  acc1: 98.4375 (98.3173)  acc5: 100.0000 (99.8798)  time: 20.5846  data: 0.0240
Epoch: [8]  [26/40]  eta: 0:04:48  lr: 0.0001  img/s: 6.253100798714137  loss: 0.0795 (0.0773)  acc1: 98.4375 (98.3507)  acc5: 100.0000 (99.8843)  time: 20.5773  data: 0.0240
Epoch: [8]  [27/40]  eta: 0:04:27  lr: 0.0001  img/s: 6.228477214911218  loss: 0.0833 (0.0779)  acc1: 98.4375 (98.3538)  acc5: 100.0000 (99.8884)  time: 20.5743  data: 0.0240
Epoch: [8]  [28/40]  eta: 0:04:07  lr: 0.0001  img/s: 6.26185898108051  loss: 0.0869 (0.0788)  acc1: 98.4375 (98.3297)  acc5: 100.0000 (99.8922)  time: 20.5678  data: 0.0240
Epoch: [8]  [29/40]  eta: 0:03:46  lr: 0.0001  img/s: 6.228494123638606  loss: 0.0869 (0.0804)  acc1: 98.4375 (98.2812)  acc5: 100.0000 (99.8698)  time: 20.5628  data: 0.0238
Epoch: [8]  [30/40]  eta: 0:03:26  lr: 0.0001  img/s: 6.192836077290551  loss: 0.0869 (0.0804)  acc1: 97.6562 (98.2359)  acc5: 100.0000 (99.8740)  time: 20.5675  data: 0.0238
Epoch: [8]  [31/40]  eta: 0:03:05  lr: 0.0001  img/s: 6.246733269596679  loss: 0.0869 (0.0806)  acc1: 97.6562 (98.2178)  acc5: 100.0000 (99.8779)  time: 20.5638  data: 0.0238
Epoch: [8]  [32/40]  eta: 0:02:44  lr: 0.0001  img/s: 6.266172751741081  loss: 0.0869 (0.0795)  acc1: 97.6562 (98.2481)  acc5: 100.0000 (99.8816)  time: 20.5561  data: 0.0237
Epoch: [8]  [33/40]  eta: 0:02:24  lr: 0.0001  img/s: 6.237461584350659  loss: 0.0869 (0.0813)  acc1: 97.6562 (98.2077)  acc5: 100.0000 (99.8851)  time: 20.5527  data: 0.0237
Epoch: [8]  [34/40]  eta: 0:02:03  lr: 0.0001  img/s: 6.219716982562729  loss: 0.0866 (0.0809)  acc1: 97.6562 (98.1920)  acc5: 100.0000 (99.8884)  time: 20.5525  data: 0.0237
Epoch: [8]  [35/40]  eta: 0:01:42  lr: 0.0001  img/s: 6.273752059851355  loss: 0.0866 (0.0812)  acc1: 97.6562 (98.1771)  acc5: 100.0000 (99.8915)  time: 20.5445  data: 0.0245
Epoch: [8]  [36/40]  eta: 0:01:22  lr: 0.0001  img/s: 6.225787439822486  loss: 0.0866 (0.0801)  acc1: 97.6562 (98.2052)  acc5: 100.0000 (99.8944)  time: 20.5452  data: 0.0245
Epoch: [8]  [37/40]  eta: 0:01:01  lr: 0.0001  img/s: 6.227827599102356  loss: 0.0866 (0.0794)  acc1: 97.6562 (98.2319)  acc5: 100.0000 (99.8972)  time: 20.5478  data: 0.0245
Epoch: [8]  [38/40]  eta: 0:00:41  lr: 0.0001  img/s: 6.233941123465591  loss: 0.0833 (0.0790)  acc1: 97.6562 (98.2572)  acc5: 100.0000 (99.8998)  time: 20.5478  data: 0.0245
Epoch: [8]  [39/40]  eta: 0:00:20  lr: 0.0001  img/s: 5.854672205216932  loss: 0.0866 (0.0853)  acc1: 97.6562 (98.2600)  acc5: 100.0000 (99.9000)  time: 19.5907  data: 0.0234
Epoch: [8] Total time: 0:13:24
  0%|                                                     | 0/2 [00:00<?, ?it/s] 50%|██████████████████████▌                      | 1/2 [00:12<00:12, 12.35s/it]100%|█████████████████████████████████████████████| 2/2 [00:24<00:00, 12.30s/it]100%|█████████████████████████████████████████████| 2/2 [00:36<00:00, 18.46s/it]
W1118 08:55:51.171792 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.171909 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.171975 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172022 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172072 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172115 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172164 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172208 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172255 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172298 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172346 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172389 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172435 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172476 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172526 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172569 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172616 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172659 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172706 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172749 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172798 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172840 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172887 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172928 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.172975 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173016 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173061 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173103 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173152 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173194 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173240 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173282 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173332 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173374 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173420 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173460 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173506 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173547 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173594 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173635 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173680 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173721 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173766 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173807 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173853 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173894 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173941 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.173982 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174027 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174068 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174114 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174156 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174202 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174243 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174289 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174330 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174376 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174417 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174465 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174507 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174553 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174594 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174640 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174681 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174727 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174769 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174814 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174854 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174900 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174941 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.174986 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.175027 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 08:55:51.175196 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=59.752037048339844 quant)
W1118 08:55:51.175366 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1418 calibrator=HistogramCalibrator scale=895.780029296875 quant)
W1118 08:55:51.175522 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5749 calibrator=HistogramCalibrator scale=212.28311157226562 quant)
W1118 08:55:51.175667 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=2286.89501953125 quant)
W1118 08:55:51.175835 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2511 calibrator=HistogramCalibrator scale=474.6937255859375 quant)
W1118 08:55:51.175980 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator scale=3882.4990234375 quant)
W1118 08:55:51.176127 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4985 calibrator=HistogramCalibrator scale=244.36289978027344 quant)
W1118 08:55:51.176269 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0397 calibrator=HistogramCalibrator scale=3218.383544921875 quant)
W1118 08:55:51.176409 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2186 calibrator=HistogramCalibrator scale=543.9622802734375 quant)
W1118 08:55:51.176550 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0347 calibrator=HistogramCalibrator scale=3677.5966796875 quant)
W1118 08:55:51.176691 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4722 calibrator=HistogramCalibrator scale=256.713134765625 quant)
W1118 08:55:51.176828 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator scale=3584.5087890625 quant)
W1118 08:55:51.176972 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1804 calibrator=HistogramCalibrator scale=670.1603393554688 quant)
W1118 08:55:51.177109 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator scale=5040.06396484375 quant)
W1118 08:55:51.177252 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4392 calibrator=HistogramCalibrator scale=274.9142761230469 quant)
W1118 08:55:51.177388 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator scale=4415.408203125 quant)
W1118 08:55:51.177526 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1852 calibrator=HistogramCalibrator scale=661.31103515625 quant)
W1118 08:55:51.177661 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator scale=4453.1904296875 quant)
W1118 08:55:51.177801 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4392 calibrator=HistogramCalibrator scale=274.9142761230469 quant)
W1118 08:55:51.177937 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator scale=2309.54296875 quant)
W1118 08:55:51.178078 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2782 calibrator=HistogramCalibrator scale=439.0372619628906 quant)
W1118 08:55:51.178215 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator scale=5450.48095703125 quant)
W1118 08:55:51.178354 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1355 calibrator=HistogramCalibrator scale=892.0152587890625 quant)
W1118 08:55:51.178489 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0221 calibrator=HistogramCalibrator scale=5756.8359375 quant)
W1118 08:55:51.178629 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2869 calibrator=HistogramCalibrator scale=421.30596923828125 quant)
W1118 08:55:51.178765 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=5273.30078125 quant)
W1118 08:55:51.178903 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1364 calibrator=HistogramCalibrator scale=878.90087890625 quant)
W1118 08:55:51.179037 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator scale=5735.44873046875 quant)
W1118 08:55:51.179181 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3031 calibrator=HistogramCalibrator scale=395.166259765625 quant)
W1118 08:55:51.179315 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5767.67333984375 quant)
W1118 08:55:51.179454 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1567 calibrator=HistogramCalibrator scale=751.669677734375 quant)
W1118 08:55:51.179588 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator scale=6969.6787109375 quant)
W1118 08:55:51.179729 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3561 calibrator=HistogramCalibrator scale=331.7650451660156 quant)
W1118 08:55:51.180019 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator scale=7424.8056640625 quant)
W1118 08:55:51.180162 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1561 calibrator=HistogramCalibrator scale=761.4615478515625 quant)
W1118 08:55:51.180298 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator scale=12689.9384765625 quant)
W1118 08:55:51.180439 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3561 calibrator=HistogramCalibrator scale=331.7650451660156 quant)
W1118 08:55:51.180576 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator scale=5884.2431640625 quant)
W1118 08:55:51.180720 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1878 calibrator=HistogramCalibrator scale=635.5390625 quant)
W1118 08:55:51.180854 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator scale=23594.54296875 quant)
W1118 08:55:51.180994 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1010 calibrator=HistogramCalibrator scale=1176.9154052734375 quant)
W1118 08:55:51.181130 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator scale=24752.171875 quant)
W1118 08:55:51.181272 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1825 calibrator=HistogramCalibrator scale=656.0224609375 quant)
W1118 08:55:51.181410 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator scale=18983.900390625 quant)
W1118 08:55:51.181551 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1028 calibrator=HistogramCalibrator scale=1159.4891357421875 quant)
W1118 08:55:51.181685 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator scale=21349.779296875 quant)
W1118 08:55:51.181826 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2029 calibrator=HistogramCalibrator scale=592.7086791992188 quant)
W1118 08:55:51.181960 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator scale=25490.427734375 quant)
W1118 08:55:51.182102 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0963 calibrator=HistogramCalibrator scale=1253.84423828125 quant)
W1118 08:55:51.182237 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25880.146484375 quant)
W1118 08:55:51.182377 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2618 calibrator=HistogramCalibrator scale=462.9416198730469 quant)
W1118 08:55:51.182513 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator scale=35586.0625 quant)
W1118 08:55:51.182652 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0827 calibrator=HistogramCalibrator scale=1438.93310546875 quant)
W1118 08:55:51.182787 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator scale=31525.89453125 quant)
W1118 08:55:51.182927 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3172 calibrator=HistogramCalibrator scale=377.8980407714844 quant)
W1118 08:55:51.183062 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator scale=36465.29296875 quant)
W1118 08:55:51.183199 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0926 calibrator=HistogramCalibrator scale=1256.6553955078125 quant)
W1118 08:55:51.183334 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator scale=26951.70703125 quant)
W1118 08:55:51.183475 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4070 calibrator=HistogramCalibrator scale=294.87432861328125 quant)
W1118 08:55:51.183611 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator scale=33513.1875 quant)
W1118 08:55:51.183751 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1555 calibrator=HistogramCalibrator scale=793.6007080078125 quant)
W1118 08:55:51.183907 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25662.263671875 quant)
W1118 08:55:51.184048 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4070 calibrator=HistogramCalibrator scale=294.87432861328125 quant)
W1118 08:55:51.184185 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator scale=6245.236328125 quant)
W1118 08:55:51.184325 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7367 calibrator=HistogramCalibrator scale=170.4126434326172 quant)
W1118 08:55:51.184460 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator scale=53889.515625 quant)
W1118 08:55:51.184600 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1940 calibrator=HistogramCalibrator scale=635.9757080078125 quant)
W1118 08:55:51.184735 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator scale=21318.76171875 quant)
W1118 08:55:51.184874 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.1148 calibrator=HistogramCalibrator scale=112.76009368896484 quant)
W1118 08:55:51.185009 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator scale=62957.48828125 quant)
W1118 08:55:51.185148 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1923 calibrator=HistogramCalibrator scale=644.7572021484375 quant)
W1118 08:55:51.185283 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator scale=21765.048828125 quant)
Model saved to ./saved/retrained_model_epoch_8.pth after epoch 8
Epoch 9/15
Epoch: [9]  [ 0/40]  eta: 0:13:45  lr: 0.0001  img/s: 6.2091784193117325  loss: 0.0608 (0.0608)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 20.6403  data: 0.0256
Epoch: [9]  [ 1/40]  eta: 0:13:24  lr: 0.0001  img/s: 6.220527432210566  loss: 0.0608 (0.0784)  acc1: 97.6562 (98.0469)  acc5: 100.0000 (100.0000)  time: 20.6207  data: 0.0248
Epoch: [9]  [ 2/40]  eta: 0:13:03  lr: 0.0001  img/s: 6.216257256070182  loss: 0.0608 (0.0682)  acc1: 98.4375 (98.6979)  acc5: 100.0000 (100.0000)  time: 20.6187  data: 0.0244
Epoch: [9]  [ 3/40]  eta: 0:12:41  lr: 0.0001  img/s: 6.254431791597798  loss: 0.0549 (0.0649)  acc1: 98.4375 (98.8281)  acc5: 100.0000 (100.0000)  time: 20.5864  data: 0.0243
Epoch: [9]  [ 4/40]  eta: 0:12:21  lr: 0.0001  img/s: 6.204112540632985  loss: 0.0608 (0.0671)  acc1: 98.4375 (98.5938)  acc5: 100.0000 (100.0000)  time: 20.6002  data: 0.0243
Epoch: [9]  [ 5/40]  eta: 0:12:00  lr: 0.0001  img/s: 6.241956819153877  loss: 0.0608 (0.0733)  acc1: 97.6562 (98.3073)  acc5: 100.0000 (100.0000)  time: 20.5885  data: 0.0242
Epoch: [9]  [ 6/40]  eta: 0:11:39  lr: 0.0001  img/s: 6.228607717982134  loss: 0.0644 (0.0720)  acc1: 98.4375 (98.3259)  acc5: 100.0000 (100.0000)  time: 20.5864  data: 0.0241
Epoch: [9]  [ 7/40]  eta: 0:11:19  lr: 0.0001  img/s: 6.243324455834965  loss: 0.0639 (0.0710)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 20.5789  data: 0.0241
Epoch: [9]  [ 8/40]  eta: 0:10:58  lr: 0.0001  img/s: 6.220004428039664  loss: 0.0639 (0.0660)  acc1: 98.4375 (98.6111)  acc5: 100.0000 (100.0000)  time: 20.5815  data: 0.0240
Epoch: [9]  [ 9/40]  eta: 0:10:38  lr: 0.0001  img/s: 6.21594691078372  loss: 0.0639 (0.0680)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 20.5849  data: 0.0240
Epoch: [9]  [10/40]  eta: 0:10:17  lr: 0.0001  img/s: 6.242330371788412  loss: 0.0644 (0.0712)  acc1: 98.4375 (98.2244)  acc5: 100.0000 (100.0000)  time: 20.5799  data: 0.0240
Epoch: [9]  [11/40]  eta: 0:09:56  lr: 0.0001  img/s: 6.244514666052967  loss: 0.0639 (0.0695)  acc1: 98.4375 (98.2422)  acc5: 100.0000 (100.0000)  time: 20.5750  data: 0.0240
Epoch: [9]  [12/40]  eta: 0:09:36  lr: 0.0001  img/s: 6.228991165619239  loss: 0.0639 (0.0686)  acc1: 98.4375 (98.2572)  acc5: 100.0000 (100.0000)  time: 20.5748  data: 0.0239
Epoch: [9]  [13/40]  eta: 0:09:15  lr: 0.0001  img/s: 6.250695725923635  loss: 0.0639 (0.0753)  acc1: 98.4375 (98.2143)  acc5: 100.0000 (99.9442)  time: 20.5696  data: 0.0239
Epoch: [9]  [14/40]  eta: 0:08:54  lr: 0.0001  img/s: 6.211096901080118  loss: 0.0644 (0.0782)  acc1: 98.4375 (98.2292)  acc5: 100.0000 (99.8958)  time: 20.5738  data: 0.0239
Epoch: [9]  [15/40]  eta: 0:08:34  lr: 0.0001  img/s: 6.223560816902322  loss: 0.0644 (0.0784)  acc1: 98.4375 (98.2422)  acc5: 100.0000 (99.9023)  time: 20.5748  data: 0.0239
Epoch: [9]  [16/40]  eta: 0:08:13  lr: 0.0001  img/s: 6.269740996968756  loss: 0.0760 (0.0798)  acc1: 98.4375 (98.1618)  acc5: 100.0000 (99.9081)  time: 20.5669  data: 0.0239
Epoch: [9]  [17/40]  eta: 0:07:52  lr: 0.0001  img/s: 6.261448034752231  loss: 0.0644 (0.0772)  acc1: 98.4375 (98.2639)  acc5: 100.0000 (99.9132)  time: 20.5613  data: 0.0239
Epoch: [9]  [18/40]  eta: 0:07:32  lr: 0.0001  img/s: 6.220394456710696  loss: 0.0738 (0.0770)  acc1: 98.4375 (98.2319)  acc5: 100.0000 (99.9178)  time: 20.5634  data: 0.0239
Epoch: [9]  [19/40]  eta: 0:07:11  lr: 0.0001  img/s: 6.243600291498546  loss: 0.0644 (0.0761)  acc1: 98.4375 (98.2812)  acc5: 100.0000 (99.9219)  time: 20.5615  data: 0.0239
Epoch: [9]  [20/40]  eta: 0:06:51  lr: 0.0001  img/s: 6.2747098289112175  loss: 0.0738 (0.0766)  acc1: 98.4375 (98.2515)  acc5: 100.0000 (99.9256)  time: 20.5506  data: 0.0238
Epoch: [9]  [21/40]  eta: 0:06:30  lr: 0.0001  img/s: 6.238502760021787  loss: 0.0644 (0.0757)  acc1: 98.4375 (98.2955)  acc5: 100.0000 (99.9290)  time: 20.5476  data: 0.0238
Epoch: [9]  [22/40]  eta: 0:06:09  lr: 0.0001  img/s: 6.231092438047643  loss: 0.0660 (0.0753)  acc1: 98.4375 (98.2677)  acc5: 100.0000 (99.9321)  time: 20.5452  data: 0.0238
Epoch: [9]  [23/40]  eta: 0:05:49  lr: 0.0001  img/s: 6.198262436735823  loss: 0.0660 (0.0742)  acc1: 98.4375 (98.3073)  acc5: 100.0000 (99.9349)  time: 20.5545  data: 0.0238
Epoch: [9]  [24/40]  eta: 0:05:28  lr: 0.0001  img/s: 6.225164081790727  loss: 0.0660 (0.0757)  acc1: 98.4375 (98.2500)  acc5: 100.0000 (99.9375)  time: 20.5510  data: 0.0238
Epoch: [9]  [25/40]  eta: 0:05:08  lr: 0.0001  img/s: 6.230326229386327  loss: 0.0644 (0.0752)  acc1: 98.4375 (98.2873)  acc5: 100.0000 (99.9399)  time: 20.5529  data: 0.0238
Epoch: [9]  [26/40]  eta: 0:04:47  lr: 0.0001  img/s: 6.220106542793216  loss: 0.0639 (0.0745)  acc1: 98.4375 (98.2928)  acc5: 100.0000 (99.9421)  time: 20.5543  data: 0.0238
Epoch: [9]  [27/40]  eta: 0:04:27  lr: 0.0001  img/s: 6.237671458555468  loss: 0.0660 (0.0744)  acc1: 98.4375 (98.2980)  acc5: 100.0000 (99.9442)  time: 20.5552  data: 0.0238
Epoch: [9]  [28/40]  eta: 0:04:06  lr: 0.0001  img/s: 6.2348422441961056  loss: 0.0660 (0.0734)  acc1: 98.4375 (98.3297)  acc5: 100.0000 (99.9461)  time: 20.5528  data: 0.0239
Epoch: [9]  [29/40]  eta: 0:03:46  lr: 0.0001  img/s: 6.220055232773183  loss: 0.0660 (0.0752)  acc1: 98.4375 (98.2812)  acc5: 100.0000 (99.9219)  time: 20.5521  data: 0.0238
Epoch: [9]  [30/40]  eta: 0:03:25  lr: 0.0001  img/s: 6.1734160647336545  loss: 0.0658 (0.0749)  acc1: 98.4375 (98.2611)  acc5: 100.0000 (99.9244)  time: 20.5636  data: 0.0238
Epoch: [9]  [31/40]  eta: 0:03:05  lr: 0.0001  img/s: 6.221442486624072  loss: 0.0660 (0.0750)  acc1: 98.4375 (98.2910)  acc5: 100.0000 (99.9268)  time: 20.5674  data: 0.0238
Epoch: [9]  [32/40]  eta: 0:02:44  lr: 0.0001  img/s: 6.208670389364629  loss: 0.0660 (0.0745)  acc1: 98.4375 (98.2955)  acc5: 100.0000 (99.9290)  time: 20.5707  data: 0.0238
Epoch: [9]  [33/40]  eta: 0:02:24  lr: 0.0001  img/s: 6.205550934307349  loss: 0.0660 (0.0751)  acc1: 98.4375 (98.2537)  acc5: 100.0000 (99.9311)  time: 20.5782  data: 0.0238
Epoch: [9]  [34/40]  eta: 0:02:03  lr: 0.0001  img/s: 6.200567667375168  loss: 0.0660 (0.0753)  acc1: 98.4375 (98.2589)  acc5: 100.0000 (99.9330)  time: 20.5799  data: 0.0238
Epoch: [9]  [35/40]  eta: 0:01:42  lr: 0.0001  img/s: 6.204178285734149  loss: 0.0658 (0.0749)  acc1: 98.4375 (98.2422)  acc5: 100.0000 (99.9349)  time: 20.5832  data: 0.0238
Epoch: [9]  [36/40]  eta: 0:01:22  lr: 0.0001  img/s: 6.196116095400706  loss: 0.0635 (0.0745)  acc1: 98.4375 (98.2475)  acc5: 100.0000 (99.9367)  time: 20.5953  data: 0.0239
Epoch: [9]  [37/40]  eta: 0:01:01  lr: 0.0001  img/s: 6.208565777831118  loss: 0.0635 (0.0737)  acc1: 98.4375 (98.2936)  acc5: 100.0000 (99.9383)  time: 20.6040  data: 0.0239
Epoch: [9]  [38/40]  eta: 0:00:41  lr: 0.0001  img/s: 6.221038702078952  loss: 0.0623 (0.0732)  acc1: 98.4375 (98.2973)  acc5: 100.0000 (99.9399)  time: 20.6039  data: 0.0238
Epoch: [9]  [39/40]  eta: 0:00:20  lr: 0.0001  img/s: 5.640587229447272  loss: 0.0635 (0.0801)  acc1: 98.4375 (98.2800)  acc5: 100.0000 (99.9400)  time: 19.6487  data: 0.0228
Epoch: [9] Total time: 0:13:24
  0%|                                                     | 0/2 [00:00<?, ?it/s] 50%|██████████████████████▌                      | 1/2 [00:12<00:12, 12.31s/it]100%|█████████████████████████████████████████████| 2/2 [00:24<00:00, 12.31s/it]100%|█████████████████████████████████████████████| 2/2 [00:36<00:00, 18.44s/it]
W1118 09:09:52.410499 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.410637 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.410729 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.410797 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.410874 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.410943 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411021 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411087 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411162 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411225 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411299 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411363 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411436 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411501 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411579 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411642 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411715 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411799 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411872 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.411936 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412008 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412068 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412138 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412206 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412274 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412334 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412405 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412465 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412539 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412600 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412670 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412731 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412806 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412865 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412936 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.412996 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413065 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413127 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413199 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413259 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413328 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413388 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413457 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413516 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413644 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413722 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413800 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413873 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.413949 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414015 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414088 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414150 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414221 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414284 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414356 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414418 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414489 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414551 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414626 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414689 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414759 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414821 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414892 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.414955 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.415027 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.415089 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.415160 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.415224 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.415293 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.415356 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.415427 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.415492 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:09:52.415691 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=59.752037048339844 quant)
W1118 09:09:52.415920 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1418 calibrator=HistogramCalibrator scale=895.780029296875 quant)
W1118 09:09:52.416106 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5570 calibrator=HistogramCalibrator scale=220.92529296875 quant)
W1118 09:09:52.416276 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=2282.03173828125 quant)
W1118 09:09:52.416449 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2388 calibrator=HistogramCalibrator scale=505.7852783203125 quant)
W1118 09:09:52.416616 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator scale=3880.2880859375 quant)
W1118 09:09:52.416797 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4830 calibrator=HistogramCalibrator scale=254.77137756347656 quant)
W1118 09:09:52.416960 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0397 calibrator=HistogramCalibrator scale=3197.1748046875 quant)
W1118 09:09:52.417101 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2074 calibrator=HistogramCalibrator scale=580.8626098632812 quant)
W1118 09:09:52.417236 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0347 calibrator=HistogramCalibrator scale=3660.60595703125 quant)
W1118 09:09:52.417373 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4551 calibrator=HistogramCalibrator scale=268.95025634765625 quant)
W1118 09:09:52.417505 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator scale=3579.044677734375 quant)
W1118 09:09:52.417654 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1738 calibrator=HistogramCalibrator scale=703.8790893554688 quant)
W1118 09:09:52.417787 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator scale=5040.06396484375 quant)
W1118 09:09:52.417925 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4239 calibrator=HistogramCalibrator scale=289.1553039550781 quant)
W1118 09:09:52.418059 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator scale=4413.033203125 quant)
W1118 09:09:52.418196 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1793 calibrator=HistogramCalibrator scale=685.665283203125 quant)
W1118 09:09:52.418328 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator scale=4450.294921875 quant)
W1118 09:09:52.418465 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4239 calibrator=HistogramCalibrator scale=289.1553039550781 quant)
W1118 09:09:52.418598 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator scale=2309.54296875 quant)
W1118 09:09:52.418736 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2680 calibrator=HistogramCalibrator scale=456.5148620605469 quant)
W1118 09:09:52.418869 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator scale=5450.48095703125 quant)
W1118 09:09:52.419017 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1307 calibrator=HistogramCalibrator scale=937.5035400390625 quant)
W1118 09:09:52.419178 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5756.8359375 quant)
W1118 09:09:52.419317 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2768 calibrator=HistogramCalibrator scale=442.6379089355469 quant)
W1118 09:09:52.419458 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=5262.02490234375 quant)
W1118 09:09:52.419598 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1302 calibrator=HistogramCalibrator scale=930.9130859375 quant)
W1118 09:09:52.419731 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator scale=5717.33251953125 quant)
W1118 09:09:52.419905 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2897 calibrator=HistogramCalibrator scale=418.9714660644531 quant)
W1118 09:09:52.420039 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5767.67333984375 quant)
W1118 09:09:52.420177 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1485 calibrator=HistogramCalibrator scale=810.643798828125 quant)
W1118 09:09:52.420309 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator scale=6964.83544921875 quant)
W1118 09:09:52.420447 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3359 calibrator=HistogramCalibrator scale=356.6474304199219 quant)
W1118 09:09:52.420578 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator scale=7424.8056640625 quant)
W1118 09:09:52.420715 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1482 calibrator=HistogramCalibrator scale=813.5557250976562 quant)
W1118 09:09:52.420845 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator scale=12689.9384765625 quant)
W1118 09:09:52.420982 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3359 calibrator=HistogramCalibrator scale=356.6474304199219 quant)
W1118 09:09:52.421114 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator scale=5907.0205078125 quant)
W1118 09:09:52.421266 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1782 calibrator=HistogramCalibrator scale=676.2560424804688 quant)
W1118 09:09:52.421425 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator scale=23570.1171875 quant)
W1118 09:09:52.421601 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0955 calibrator=HistogramCalibrator scale=1257.934814453125 quant)
W1118 09:09:52.421787 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator scale=24752.171875 quant)
W1118 09:09:52.421993 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1726 calibrator=HistogramCalibrator scale=695.9325561523438 quant)
W1118 09:09:52.422160 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator scale=18983.900390625 quant)
W1118 09:09:52.422305 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0984 calibrator=HistogramCalibrator scale=1235.3345947265625 quant)
W1118 09:09:52.422438 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator scale=21349.779296875 quant)
W1118 09:09:52.422577 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1958 calibrator=HistogramCalibrator scale=625.9091186523438 quant)
W1118 09:09:52.422709 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator scale=25490.427734375 quant)
W1118 09:09:52.422847 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0926 calibrator=HistogramCalibrator scale=1318.639404296875 quant)
W1118 09:09:52.422989 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25880.146484375 quant)
W1118 09:09:52.423129 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2513 calibrator=HistogramCalibrator scale=485.162841796875 quant)
W1118 09:09:52.423260 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator scale=35586.0625 quant)
W1118 09:09:52.423397 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0787 calibrator=HistogramCalibrator scale=1536.0389404296875 quant)
W1118 09:09:52.423528 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator scale=31525.89453125 quant)
W1118 09:09:52.423675 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3043 calibrator=HistogramCalibrator scale=400.4270935058594 quant)
W1118 09:09:52.423871 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator scale=36465.29296875 quant)
W1118 09:09:52.424056 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0863 calibrator=HistogramCalibrator scale=1370.7939453125 quant)
W1118 09:09:52.424232 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator scale=26951.70703125 quant)
W1118 09:09:52.424413 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3852 calibrator=HistogramCalibrator scale=312.0470886230469 quant)
W1118 09:09:52.424585 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator scale=33513.1875 quant)
W1118 09:09:52.424763 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1517 calibrator=HistogramCalibrator scale=816.973876953125 quant)
W1118 09:09:52.424937 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25662.263671875 quant)
W1118 09:09:52.425116 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3852 calibrator=HistogramCalibrator scale=312.0470886230469 quant)
W1118 09:09:52.425288 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator scale=6245.236328125 quant)
W1118 09:09:52.425470 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7287 calibrator=HistogramCalibrator scale=172.37893676757812 quant)
W1118 09:09:52.425644 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator scale=53943.83984375 quant)
W1118 09:09:52.425821 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1903 calibrator=HistogramCalibrator scale=654.5476684570312 quant)
W1118 09:09:52.425993 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator scale=21318.76171875 quant)
W1118 09:09:52.426172 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.1032 calibrator=HistogramCalibrator scale=113.92637634277344 quant)
W1118 09:09:52.426341 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator scale=62957.48828125 quant)
W1118 09:09:52.426527 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1893 calibrator=HistogramCalibrator scale=660.4829711914062 quant)
W1118 09:09:52.426709 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator scale=21765.048828125 quant)
Model saved to ./saved/retrained_model_epoch_9.pth after epoch 9
Epoch 10/15
Epoch: [10]  [ 0/40]  eta: 0:13:43  lr: 0.0001  img/s: 6.225675969765348  loss: 0.0692 (0.0692)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 20.5845  data: 0.0245
Epoch: [10]  [ 1/40]  eta: 0:13:23  lr: 0.0001  img/s: 6.20980705538695  loss: 0.0692 (0.0815)  acc1: 96.8750 (98.0469)  acc5: 100.0000 (100.0000)  time: 20.6104  data: 0.0241
Epoch: [10]  [ 2/40]  eta: 0:13:02  lr: 0.0001  img/s: 6.223973082414216  loss: 0.0917 (0.0849)  acc1: 96.8750 (97.6562)  acc5: 100.0000 (100.0000)  time: 20.6033  data: 0.0239
Epoch: [10]  [ 3/40]  eta: 0:12:41  lr: 0.0001  img/s: 6.24189375435221  loss: 0.0692 (0.0765)  acc1: 96.8750 (98.0469)  acc5: 100.0000 (100.0000)  time: 20.5851  data: 0.0239
Epoch: [10]  [ 4/40]  eta: 0:12:21  lr: 0.0001  img/s: 6.196689733436721  loss: 0.0917 (0.0830)  acc1: 96.8750 (97.8125)  acc5: 100.0000 (100.0000)  time: 20.6040  data: 0.0238
Epoch: [10]  [ 5/40]  eta: 0:12:02  lr: 0.0001  img/s: 6.142341658665115  loss: 0.0692 (0.0787)  acc1: 96.8750 (97.9167)  acc5: 100.0000 (100.0000)  time: 20.6471  data: 0.0237
Epoch: [10]  [ 6/40]  eta: 0:11:42  lr: 0.0001  img/s: 6.180352218580591  loss: 0.0692 (0.0751)  acc1: 97.6562 (97.8795)  acc5: 100.0000 (100.0000)  time: 20.6596  data: 0.0237
Epoch: [10]  [ 7/40]  eta: 0:11:21  lr: 0.0001  img/s: 6.241065249112091  loss: 0.0692 (0.0769)  acc1: 96.8750 (97.6562)  acc5: 100.0000 (100.0000)  time: 20.6437  data: 0.0237
Epoch: [10]  [ 8/40]  eta: 0:11:00  lr: 0.0001  img/s: 6.239259452167988  loss: 0.0692 (0.0706)  acc1: 97.6562 (97.9167)  acc5: 100.0000 (100.0000)  time: 20.6321  data: 0.0237
Epoch: [10]  [ 9/40]  eta: 0:10:39  lr: 0.0001  img/s: 6.1917885930187415  loss: 0.0692 (0.0722)  acc1: 96.8750 (97.8125)  acc5: 100.0000 (100.0000)  time: 20.6385  data: 0.0237
Epoch: [10]  [10/40]  eta: 0:10:19  lr: 0.0001  img/s: 6.2127490365059455  loss: 0.0692 (0.0710)  acc1: 97.6562 (97.9403)  acc5: 100.0000 (100.0000)  time: 20.6374  data: 0.0237
Epoch: [10]  [11/40]  eta: 0:09:58  lr: 0.0001  img/s: 6.156791905741232  loss: 0.0692 (0.0709)  acc1: 97.6562 (97.9167)  acc5: 100.0000 (100.0000)  time: 20.6521  data: 0.0237
Epoch: [10]  [12/40]  eta: 0:09:38  lr: 0.0001  img/s: 6.194811805751746  loss: 0.0692 (0.0676)  acc1: 97.6562 (98.0769)  acc5: 100.0000 (100.0000)  time: 20.6547  data: 0.0236
Epoch: [10]  [13/40]  eta: 0:09:17  lr: 0.0001  img/s: 6.227222540050611  loss: 0.0692 (0.0691)  acc1: 97.6562 (98.0469)  acc5: 100.0000 (100.0000)  time: 20.6493  data: 0.0237
Epoch: [10]  [14/40]  eta: 0:08:56  lr: 0.0001  img/s: 6.232734607402413  loss: 0.0703 (0.0726)  acc1: 97.6562 (98.0208)  acc5: 100.0000 (99.9479)  time: 20.6434  data: 0.0237
Epoch: [10]  [15/40]  eta: 0:08:36  lr: 0.0001  img/s: 6.194699655193036  loss: 0.0703 (0.0729)  acc1: 97.6562 (97.9980)  acc5: 100.0000 (99.9512)  time: 20.6460  data: 0.0237
Epoch: [10]  [16/40]  eta: 0:08:15  lr: 0.0001  img/s: 6.198171771601586  loss: 0.0703 (0.0718)  acc1: 97.6562 (98.0239)  acc5: 100.0000 (99.9540)  time: 20.6477  data: 0.0237
Epoch: [10]  [17/40]  eta: 0:07:54  lr: 0.0001  img/s: 6.2230055641250965  loss: 0.0692 (0.0702)  acc1: 97.6562 (98.0903)  acc5: 100.0000 (99.9566)  time: 20.6446  data: 0.0237
Epoch: [10]  [18/40]  eta: 0:07:34  lr: 0.0001  img/s: 6.203072490844282  loss: 0.0692 (0.0692)  acc1: 97.6562 (98.1086)  acc5: 100.0000 (99.9589)  time: 20.6454  data: 0.0236
Epoch: [10]  [19/40]  eta: 0:07:13  lr: 0.0001  img/s: 6.213306774878114  loss: 0.0648 (0.0690)  acc1: 97.6562 (98.0859)  acc5: 100.0000 (99.9219)  time: 20.6443  data: 0.0236
Epoch: [10]  [20/40]  eta: 0:06:52  lr: 0.0001  img/s: 6.210949598519425  loss: 0.0648 (0.0689)  acc1: 97.6562 (98.1027)  acc5: 100.0000 (99.9256)  time: 20.6467  data: 0.0236
Epoch: [10]  [21/40]  eta: 0:06:32  lr: 0.0001  img/s: 6.191723181647887  loss: 0.0648 (0.0703)  acc1: 97.6562 (98.0824)  acc5: 100.0000 (99.9290)  time: 20.6497  data: 0.0236
Epoch: [10]  [22/40]  eta: 0:06:11  lr: 0.0001  img/s: 6.218841478841165  loss: 0.0648 (0.0721)  acc1: 97.6562 (98.0299)  acc5: 100.0000 (99.9321)  time: 20.6506  data: 0.0236
Epoch: [10]  [23/40]  eta: 0:05:50  lr: 0.0001  img/s: 6.219183452151002  loss: 0.0648 (0.0705)  acc1: 97.6562 (98.0794)  acc5: 100.0000 (99.9349)  time: 20.6543  data: 0.0236
Epoch: [10]  [24/40]  eta: 0:05:30  lr: 0.0001  img/s: 6.191106625996192  loss: 0.0648 (0.0714)  acc1: 97.6562 (98.0625)  acc5: 100.0000 (99.9375)  time: 20.6552  data: 0.0236
Epoch: [10]  [25/40]  eta: 0:05:09  lr: 0.0001  img/s: 6.187242224849097  loss: 0.0648 (0.0701)  acc1: 97.6562 (98.1370)  acc5: 100.0000 (99.9399)  time: 20.6481  data: 0.0240
Epoch: [10]  [26/40]  eta: 0:04:49  lr: 0.0001  img/s: 6.2028387803174105  loss: 0.0648 (0.0688)  acc1: 97.6562 (98.1771)  acc5: 100.0000 (99.9421)  time: 20.6444  data: 0.0240
Epoch: [10]  [27/40]  eta: 0:04:28  lr: 0.0001  img/s: 6.232840830962456  loss: 0.0584 (0.0684)  acc1: 98.4375 (98.1864)  acc5: 100.0000 (99.9442)  time: 20.6457  data: 0.0240
Epoch: [10]  [28/40]  eta: 0:04:07  lr: 0.0001  img/s: 6.234361787806198  loss: 0.0584 (0.0677)  acc1: 98.4375 (98.2489)  acc5: 100.0000 (99.9461)  time: 20.6465  data: 0.0240
Epoch: [10]  [29/40]  eta: 0:03:47  lr: 0.0001  img/s: 6.226630308629116  loss: 0.0584 (0.0716)  acc1: 98.4375 (98.1510)  acc5: 100.0000 (99.9219)  time: 20.6407  data: 0.0240
Epoch: [10]  [30/40]  eta: 0:03:26  lr: 0.0001  img/s: 6.185694066025268  loss: 0.0648 (0.0717)  acc1: 98.4375 (98.1603)  acc5: 100.0000 (99.9244)  time: 20.6453  data: 0.0240
Epoch: [10]  [31/40]  eta: 0:03:05  lr: 0.0001  img/s: 6.2176236620208964  loss: 0.0648 (0.0720)  acc1: 98.4375 (98.1689)  acc5: 100.0000 (99.9268)  time: 20.6351  data: 0.0240
Epoch: [10]  [32/40]  eta: 0:02:45  lr: 0.0001  img/s: 6.222566597182018  loss: 0.0675 (0.0724)  acc1: 98.4375 (98.2008)  acc5: 100.0000 (99.9290)  time: 20.6305  data: 0.0241
Epoch: [10]  [33/40]  eta: 0:02:24  lr: 0.0001  img/s: 6.198901103557705  loss: 0.0675 (0.0732)  acc1: 98.4375 (98.1847)  acc5: 100.0000 (99.9311)  time: 20.6352  data: 0.0240
Epoch: [10]  [34/40]  eta: 0:02:03  lr: 0.0001  img/s: 6.200285022993113  loss: 0.0648 (0.0727)  acc1: 98.4375 (98.2143)  acc5: 100.0000 (99.9330)  time: 20.6405  data: 0.0240
Epoch: [10]  [35/40]  eta: 0:01:43  lr: 0.0001  img/s: 6.222851420961199  loss: 0.0580 (0.0719)  acc1: 98.4375 (98.2422)  acc5: 100.0000 (99.9349)  time: 20.6359  data: 0.0240
Epoch: [10]  [36/40]  eta: 0:01:22  lr: 0.0001  img/s: 6.246285934465802  loss: 0.0580 (0.0707)  acc1: 98.4375 (98.2897)  acc5: 100.0000 (99.9367)  time: 20.6279  data: 0.0240
Epoch: [10]  [37/40]  eta: 0:01:01  lr: 0.0001  img/s: 6.240119245852877  loss: 0.0580 (0.0697)  acc1: 98.4375 (98.3347)  acc5: 100.0000 (99.9383)  time: 20.6251  data: 0.0240
Epoch: [10]  [38/40]  eta: 0:00:41  lr: 0.0001  img/s: 6.19958328552136  loss: 0.0643 (0.0695)  acc1: 98.4375 (98.3373)  acc5: 100.0000 (99.9399)  time: 20.6257  data: 0.0240
Epoch: [10]  [39/40]  eta: 0:00:20  lr: 0.0001  img/s: 5.863315937340877  loss: 0.0643 (0.0742)  acc1: 98.4375 (98.3400)  acc5: 100.0000 (99.9400)  time: 19.6628  data: 0.0230
Epoch: [10] Total time: 0:13:26
  0%|                                                     | 0/2 [00:00<?, ?it/s] 50%|██████████████████████▌                      | 1/2 [00:12<00:12, 12.24s/it]100%|█████████████████████████████████████████████| 2/2 [00:24<00:00, 12.32s/it]100%|█████████████████████████████████████████████| 2/2 [00:37<00:00, 18.53s/it]
W1118 09:23:55.764553 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.764686 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.764784 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.764852 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.764928 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.764994 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765074 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765140 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765213 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765280 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765354 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765420 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765493 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765558 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765637 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765701 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765775 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765838 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765911 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.765976 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766050 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766114 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766188 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766251 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766324 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766387 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766459 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766523 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766597 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766661 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766733 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766797 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766871 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.766933 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767006 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767070 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767142 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767206 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767285 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767350 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767423 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767487 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767558 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767621 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767699 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767785 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767863 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.767931 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768007 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768072 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768144 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768207 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768279 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768344 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768416 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768480 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768551 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768615 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768689 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768761 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768832 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768897 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.768969 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.769032 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.769105 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.769169 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.769241 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.769304 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.769377 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.769441 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.769512 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.769576 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:23:55.769775 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=59.752037048339844 quant)
W1118 09:23:55.769992 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1412 calibrator=HistogramCalibrator scale=895.780029296875 quant)
W1118 09:23:55.770199 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5378 calibrator=HistogramCalibrator scale=228.0137939453125 quant)
W1118 09:23:55.770391 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=2282.03173828125 quant)
W1118 09:23:55.770587 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2293 calibrator=HistogramCalibrator scale=531.785888671875 quant)
W1118 09:23:55.770775 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator scale=3882.4990234375 quant)
W1118 09:23:55.770977 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4690 calibrator=HistogramCalibrator scale=262.9159240722656 quant)
W1118 09:23:55.771176 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0397 calibrator=HistogramCalibrator scale=3197.1748046875 quant)
W1118 09:23:55.771366 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1989 calibrator=HistogramCalibrator scale=612.376708984375 quant)
W1118 09:23:55.771550 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0347 calibrator=HistogramCalibrator scale=3660.60595703125 quant)
W1118 09:23:55.771739 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4419 calibrator=HistogramCalibrator scale=279.072021484375 quant)
W1118 09:23:55.771949 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator scale=3579.044677734375 quant)
W1118 09:23:55.772143 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1678 calibrator=HistogramCalibrator scale=730.7998046875 quant)
W1118 09:23:55.772327 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator scale=5040.06396484375 quant)
W1118 09:23:55.772520 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4125 calibrator=HistogramCalibrator scale=299.6148376464844 quant)
W1118 09:23:55.772712 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator scale=4410.66064453125 quant)
W1118 09:23:55.772903 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1747 calibrator=HistogramCalibrator scale=708.4093017578125 quant)
W1118 09:23:55.773085 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator scale=4450.294921875 quant)
W1118 09:23:55.773277 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4125 calibrator=HistogramCalibrator scale=299.6148376464844 quant)
W1118 09:23:55.773459 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0551 calibrator=HistogramCalibrator scale=2309.54296875 quant)
W1118 09:23:55.773648 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2600 calibrator=HistogramCalibrator scale=473.86993408203125 quant)
W1118 09:23:55.773832 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator scale=5450.48095703125 quant)
W1118 09:23:55.774030 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1267 calibrator=HistogramCalibrator scale=971.3310546875 quant)
W1118 09:23:55.774229 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5764.3759765625 quant)
W1118 09:23:55.774418 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2675 calibrator=HistogramCalibrator scale=458.76904296875 quant)
W1118 09:23:55.774601 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=5269.53662109375 quant)
W1118 09:23:55.774789 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1254 calibrator=HistogramCalibrator scale=975.6912231445312 quant)
W1118 09:23:55.774970 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator scale=5724.5654296875 quant)
W1118 09:23:55.775166 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2784 calibrator=HistogramCalibrator scale=438.335693359375 quant)
W1118 09:23:55.775349 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5767.67333984375 quant)
W1118 09:23:55.775538 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1412 calibrator=HistogramCalibrator scale=854.9847412109375 quant)
W1118 09:23:55.775721 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator scale=6964.83544921875 quant)
W1118 09:23:55.775931 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3205 calibrator=HistogramCalibrator scale=378.07147216796875 quant)
W1118 09:23:55.776114 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator scale=7424.8056640625 quant)
W1118 09:23:55.776305 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1420 calibrator=HistogramCalibrator scale=857.2281494140625 quant)
W1118 09:23:55.776487 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator scale=12689.9384765625 quant)
W1118 09:23:55.776678 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3205 calibrator=HistogramCalibrator scale=378.07147216796875 quant)
W1118 09:23:55.776860 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator scale=5907.0205078125 quant)
W1118 09:23:55.777066 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1721 calibrator=HistogramCalibrator scale=712.6494750976562 quant)
W1118 09:23:55.777266 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator scale=23570.1171875 quant)
W1118 09:23:55.777460 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0907 calibrator=HistogramCalibrator scale=1330.22998046875 quant)
W1118 09:23:55.777642 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator scale=24731.267578125 quant)
W1118 09:23:55.777835 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1667 calibrator=HistogramCalibrator scale=735.800048828125 quant)
W1118 09:23:55.778017 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator scale=18983.900390625 quant)
W1118 09:23:55.778208 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0941 calibrator=HistogramCalibrator scale=1291.2943115234375 quant)
W1118 09:23:55.778391 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator scale=21349.779296875 quant)
W1118 09:23:55.778583 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1884 calibrator=HistogramCalibrator scale=648.53955078125 quant)
W1118 09:23:55.778766 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator scale=25490.427734375 quant)
W1118 09:23:55.778958 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0895 calibrator=HistogramCalibrator scale=1370.8382568359375 quant)
W1118 09:23:55.779139 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25880.146484375 quant)
W1118 09:23:55.779331 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2429 calibrator=HistogramCalibrator scale=505.37799072265625 quant)
W1118 09:23:55.779514 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator scale=35586.0625 quant)
W1118 09:23:55.779715 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0759 calibrator=HistogramCalibrator scale=1613.8310546875 quant)
W1118 09:23:55.779917 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator scale=31525.89453125 quant)
W1118 09:23:55.780115 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2944 calibrator=HistogramCalibrator scale=417.3089904785156 quant)
W1118 09:23:55.780313 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator scale=36465.29296875 quant)
W1118 09:23:55.780502 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0817 calibrator=HistogramCalibrator scale=1471.3541259765625 quant)
W1118 09:23:55.780685 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator scale=26951.70703125 quant)
W1118 09:23:55.780875 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3714 calibrator=HistogramCalibrator scale=329.7339782714844 quant)
W1118 09:23:55.781058 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator scale=33513.1875 quant)
W1118 09:23:55.781249 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1480 calibrator=HistogramCalibrator scale=837.0399169921875 quant)
W1118 09:23:55.781431 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25662.263671875 quant)
W1118 09:23:55.781619 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3714 calibrator=HistogramCalibrator scale=329.7339782714844 quant)
W1118 09:23:55.781800 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator scale=6245.236328125 quant)
W1118 09:23:55.781991 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7222 calibrator=HistogramCalibrator scale=174.2942657470703 quant)
W1118 09:23:55.782173 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator scale=53943.83984375 quant)
W1118 09:23:55.782360 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1877 calibrator=HistogramCalibrator scale=667.3819580078125 quant)
W1118 09:23:55.782541 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator scale=21318.76171875 quant)
W1118 09:23:55.782730 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.0935 calibrator=HistogramCalibrator scale=115.11701965332031 quant)
W1118 09:23:55.782920 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator scale=63009.3046875 quant)
W1118 09:23:55.783116 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1848 calibrator=HistogramCalibrator scale=671.0039672851562 quant)
W1118 09:23:55.783313 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator scale=21765.048828125 quant)
Model saved to ./saved/retrained_model_epoch_10.pth after epoch 10
Epoch 11/15
Epoch: [11]  [ 0/40]  eta: 0:13:45  lr: 0.0001  img/s: 6.209831979376272  loss: 0.0997 (0.0997)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 20.6367  data: 0.0242
Epoch: [11]  [ 1/40]  eta: 0:13:23  lr: 0.0001  img/s: 6.22357228802505  loss: 0.0782 (0.0890)  acc1: 97.6562 (97.6562)  acc5: 100.0000 (100.0000)  time: 20.6137  data: 0.0240
Epoch: [11]  [ 2/40]  eta: 0:13:04  lr: 0.0001  img/s: 6.199491507916616  loss: 0.0793 (0.0858)  acc1: 97.6562 (97.9167)  acc5: 100.0000 (100.0000)  time: 20.6327  data: 0.0239
Epoch: [11]  [ 3/40]  eta: 0:12:42  lr: 0.0001  img/s: 6.224600315587755  loss: 0.0782 (0.0727)  acc1: 97.6562 (98.4375)  acc5: 100.0000 (100.0000)  time: 20.6213  data: 0.0238
Epoch: [11]  [ 4/40]  eta: 0:12:22  lr: 0.0001  img/s: 6.19355372212023  loss: 0.0782 (0.0711)  acc1: 98.4375 (98.5938)  acc5: 100.0000 (100.0000)  time: 20.6352  data: 0.0238
Epoch: [11]  [ 5/40]  eta: 0:12:01  lr: 0.0001  img/s: 6.231978918263464  loss: 0.0782 (0.0775)  acc1: 97.6562 (98.1771)  acc5: 100.0000 (100.0000)  time: 20.6231  data: 0.0238
Epoch: [11]  [ 6/40]  eta: 0:11:40  lr: 0.0001  img/s: 6.229073339112488  loss: 0.0782 (0.0716)  acc1: 98.4375 (98.3259)  acc5: 100.0000 (100.0000)  time: 20.6160  data: 0.0239
Epoch: [11]  [ 7/40]  eta: 0:11:19  lr: 0.0001  img/s: 6.256468970142347  loss: 0.0647 (0.0689)  acc1: 98.4375 (98.3398)  acc5: 100.0000 (100.0000)  time: 20.5993  data: 0.0239
Epoch: [11]  [ 8/40]  eta: 0:10:58  lr: 0.0001  img/s: 6.259710416566248  loss: 0.0647 (0.0641)  acc1: 98.4375 (98.5243)  acc5: 100.0000 (100.0000)  time: 20.5860  data: 0.0247
Epoch: [11]  [ 9/40]  eta: 0:10:38  lr: 0.0001  img/s: 6.231666711896772  loss: 0.0647 (0.0719)  acc1: 98.4375 (98.2812)  acc5: 100.0000 (100.0000)  time: 20.5838  data: 0.0246
Epoch: [11]  [10/40]  eta: 0:10:17  lr: 0.0001  img/s: 6.223324982876525  loss: 0.0647 (0.0704)  acc1: 98.4375 (98.2955)  acc5: 100.0000 (100.0000)  time: 20.5845  data: 0.0246
Epoch: [11]  [11/40]  eta: 0:09:56  lr: 0.0001  img/s: 6.2807101220595625  loss: 0.0647 (0.0701)  acc1: 98.4375 (98.2422)  acc5: 100.0000 (100.0000)  time: 20.5695  data: 0.0246
Epoch: [11]  [12/40]  eta: 0:09:35  lr: 0.0001  img/s: 6.242451221681569  loss: 0.0647 (0.0686)  acc1: 98.4375 (98.3173)  acc5: 100.0000 (100.0000)  time: 20.5664  data: 0.0245
Epoch: [11]  [13/40]  eta: 0:09:15  lr: 0.0001  img/s: 6.254728867336898  loss: 0.0647 (0.0710)  acc1: 98.4375 (98.2143)  acc5: 100.0000 (100.0000)  time: 20.5608  data: 0.0244
Epoch: [11]  [14/40]  eta: 0:08:54  lr: 0.0001  img/s: 6.226711336568909  loss: 0.0658 (0.0728)  acc1: 98.4375 (98.1771)  acc5: 100.0000 (99.9479)  time: 20.5621  data: 0.0244
Epoch: [11]  [15/40]  eta: 0:08:33  lr: 0.0001  img/s: 6.255370186625664  loss: 0.0658 (0.0732)  acc1: 97.6562 (98.1445)  acc5: 100.0000 (99.9512)  time: 20.5574  data: 0.0244
Epoch: [11]  [16/40]  eta: 0:08:13  lr: 0.0001  img/s: 6.272077656570621  loss: 0.0725 (0.0731)  acc1: 98.4375 (98.1618)  acc5: 100.0000 (99.9540)  time: 20.5500  data: 0.0244
Epoch: [11]  [17/40]  eta: 0:07:52  lr: 0.0001  img/s: 6.233130142095783  loss: 0.0658 (0.0719)  acc1: 98.4375 (98.1771)  acc5: 100.0000 (99.9566)  time: 20.5506  data: 0.0244
Epoch: [11]  [18/40]  eta: 0:07:32  lr: 0.0001  img/s: 6.2334269346237265  loss: 0.0725 (0.0736)  acc1: 98.4375 (98.1497)  acc5: 100.0000 (99.9589)  time: 20.5514  data: 0.0247
Epoch: [11]  [19/40]  eta: 0:07:11  lr: 0.0001  img/s: 6.2641918878321965  loss: 0.0658 (0.0716)  acc1: 98.4375 (98.2422)  acc5: 100.0000 (99.9609)  time: 20.5467  data: 0.0247
Epoch: [11]  [20/40]  eta: 0:06:50  lr: 0.0001  img/s: 6.269049584178  loss: 0.0658 (0.0716)  acc1: 98.4375 (98.2515)  acc5: 100.0000 (99.9628)  time: 20.5369  data: 0.0247
Epoch: [11]  [21/40]  eta: 0:06:30  lr: 0.0001  img/s: 6.237427886942359  loss: 0.0647 (0.0708)  acc1: 98.4375 (98.2955)  acc5: 100.0000 (99.9645)  time: 20.5346  data: 0.0247
Epoch: [11]  [22/40]  eta: 0:06:09  lr: 0.0001  img/s: 6.264333174727086  loss: 0.0586 (0.0703)  acc1: 98.4375 (98.3356)  acc5: 100.0000 (99.9660)  time: 20.5239  data: 0.0247
Epoch: [11]  [23/40]  eta: 0:05:49  lr: 0.0001  img/s: 6.255524851707248  loss: 0.0586 (0.0690)  acc1: 98.4375 (98.3724)  acc5: 100.0000 (99.9674)  time: 20.5188  data: 0.0247
Epoch: [11]  [24/40]  eta: 0:05:28  lr: 0.0001  img/s: 6.238483187207723  loss: 0.0586 (0.0700)  acc1: 98.4375 (98.3750)  acc5: 100.0000 (99.9688)  time: 20.5114  data: 0.0246
Epoch: [11]  [25/40]  eta: 0:05:08  lr: 0.0001  img/s: 6.245333044466668  loss: 0.0557 (0.0694)  acc1: 98.4375 (98.4075)  acc5: 100.0000 (99.9700)  time: 20.5092  data: 0.0247
Epoch: [11]  [26/40]  eta: 0:04:47  lr: 0.0001  img/s: 6.243296866348716  loss: 0.0557 (0.0681)  acc1: 98.4375 (98.4664)  acc5: 100.0000 (99.9711)  time: 20.5068  data: 0.0246
Epoch: [11]  [27/40]  eta: 0:04:26  lr: 0.0001  img/s: 6.237001736731408  loss: 0.0586 (0.0684)  acc1: 98.4375 (98.4654)  acc5: 100.0000 (99.9721)  time: 20.5100  data: 0.0246
Epoch: [11]  [28/40]  eta: 0:04:06  lr: 0.0001  img/s: 6.2094424122875385  loss: 0.0586 (0.0678)  acc1: 98.4375 (98.4914)  acc5: 100.0000 (99.9731)  time: 20.5184  data: 0.0247
Epoch: [11]  [29/40]  eta: 0:03:45  lr: 0.0001  img/s: 6.221772199967954  loss: 0.0586 (0.0699)  acc1: 98.4375 (98.4115)  acc5: 100.0000 (99.9740)  time: 20.5200  data: 0.0247
Epoch: [11]  [30/40]  eta: 0:03:25  lr: 0.0001  img/s: 6.220331394436688  loss: 0.0586 (0.0690)  acc1: 98.4375 (98.4123)  acc5: 100.0000 (99.9748)  time: 20.5205  data: 0.0247
Epoch: [11]  [31/40]  eta: 0:03:04  lr: 0.0001  img/s: 6.236148451977829  loss: 0.0538 (0.0682)  acc1: 98.4375 (98.4619)  acc5: 100.0000 (99.9756)  time: 20.5277  data: 0.0246
Epoch: [11]  [32/40]  eta: 0:02:44  lr: 0.0001  img/s: 6.257049535592565  loss: 0.0538 (0.0670)  acc1: 98.4375 (98.5085)  acc5: 100.0000 (99.9763)  time: 20.5253  data: 0.0246
Epoch: [11]  [33/40]  eta: 0:02:23  lr: 0.0001  img/s: 6.246163991339362  loss: 0.0538 (0.0699)  acc1: 98.4375 (98.4145)  acc5: 100.0000 (99.9770)  time: 20.5268  data: 0.0247
Epoch: [11]  [34/40]  eta: 0:02:03  lr: 0.0001  img/s: 6.239328192138116  loss: 0.0536 (0.0688)  acc1: 98.4375 (98.4598)  acc5: 100.0000 (99.9777)  time: 20.5247  data: 0.0246
Epoch: [11]  [35/40]  eta: 0:01:42  lr: 0.0001  img/s: 6.284300391178804  loss: 0.0513 (0.0681)  acc1: 98.4375 (98.4592)  acc5: 100.0000 (99.9783)  time: 20.5199  data: 0.0246
Epoch: [11]  [36/40]  eta: 0:01:22  lr: 0.0001  img/s: 6.207475787509355  loss: 0.0502 (0.0675)  acc1: 98.4375 (98.4586)  acc5: 100.0000 (99.9789)  time: 20.5307  data: 0.0248
Epoch: [11]  [37/40]  eta: 0:01:01  lr: 0.0001  img/s: 6.247164894258067  loss: 0.0460 (0.0669)  acc1: 99.2188 (98.4786)  acc5: 100.0000 (99.9794)  time: 20.5284  data: 0.0248
Epoch: [11]  [38/40]  eta: 0:00:41  lr: 0.0001  img/s: 6.191911564424449  loss: 0.0452 (0.0658)  acc1: 99.2188 (98.5176)  acc5: 100.0000 (99.9800)  time: 20.5349  data: 0.0244
Epoch: [11]  [39/40]  eta: 0:00:20  lr: 0.0001  img/s: 5.9107681732995045  loss: 0.0460 (0.0710)  acc1: 99.2188 (98.5200)  acc5: 100.0000 (99.9800)  time: 19.5798  data: 0.0233
Epoch: [11] Total time: 0:13:22
  0%|                                                     | 0/2 [00:00<?, ?it/s] 50%|██████████████████████▌                      | 1/2 [00:12<00:12, 12.24s/it]100%|█████████████████████████████████████████████| 2/2 [00:24<00:00, 12.25s/it]100%|█████████████████████████████████████████████| 2/2 [00:36<00:00, 18.39s/it]
W1118 09:37:55.220982 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221109 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221189 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221250 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221314 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221371 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221435 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221492 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221553 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221609 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221669 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221723 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221782 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221837 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221900 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.221955 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222014 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222069 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222128 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222183 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222244 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222298 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222358 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222412 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222471 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222531 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222591 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222645 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222707 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222762 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222831 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222885 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222945 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.222999 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223057 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223110 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223169 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223222 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223283 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223337 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223396 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223449 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223508 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223562 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223718 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223830 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223913 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.223973 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224034 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224090 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224150 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224204 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224263 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224316 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224374 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224427 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224484 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224537 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224597 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224649 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224708 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224760 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224816 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224869 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224929 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.224982 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.225039 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.225092 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.225149 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.225202 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.225259 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.225311 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:37:55.225489 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=59.752037048339844 quant)
W1118 09:37:55.225683 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1412 calibrator=HistogramCalibrator scale=899.2946166992188 quant)
W1118 09:37:55.225862 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5268 calibrator=HistogramCalibrator scale=236.13137817382812 quant)
W1118 09:37:55.226028 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=2282.03173828125 quant)
W1118 09:37:55.226198 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2210 calibrator=HistogramCalibrator scale=553.809326171875 quant)
W1118 09:37:55.226361 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator scale=3882.4990234375 quant)
W1118 09:37:55.226583 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4570 calibrator=HistogramCalibrator scale=270.7601318359375 quant)
W1118 09:37:55.226767 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0397 calibrator=HistogramCalibrator scale=3197.1748046875 quant)
W1118 09:37:55.226935 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1923 calibrator=HistogramCalibrator scale=638.3515625 quant)
W1118 09:37:55.227097 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0347 calibrator=HistogramCalibrator scale=3662.486083984375 quant)
W1118 09:37:55.227262 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4316 calibrator=HistogramCalibrator scale=287.4163818359375 quant)
W1118 09:37:55.227422 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator scale=3579.044677734375 quant)
W1118 09:37:55.227587 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1626 calibrator=HistogramCalibrator scale=756.925048828125 quant)
W1118 09:37:55.227750 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator scale=5040.06396484375 quant)
W1118 09:37:55.227931 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4021 calibrator=HistogramCalibrator scale=307.87762451171875 quant)
W1118 09:37:55.228078 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator scale=4410.66064453125 quant)
W1118 09:37:55.228217 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1711 calibrator=HistogramCalibrator scale=726.8463134765625 quant)
W1118 09:37:55.228352 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator scale=4450.294921875 quant)
W1118 09:37:55.228491 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4021 calibrator=HistogramCalibrator scale=307.87762451171875 quant)
W1118 09:37:55.228624 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0551 calibrator=HistogramCalibrator scale=2306.1630859375 quant)
W1118 09:37:55.228762 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2529 calibrator=HistogramCalibrator scale=488.40087890625 quant)
W1118 09:37:55.228903 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator scale=5454.41064453125 quant)
W1118 09:37:55.229052 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1233 calibrator=HistogramCalibrator scale=1002.3309326171875 quant)
W1118 09:37:55.229237 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5771.935546875 quant)
W1118 09:37:55.229426 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2595 calibrator=HistogramCalibrator scale=474.68408203125 quant)
W1118 09:37:55.229606 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=5269.53662109375 quant)
W1118 09:37:55.229791 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1218 calibrator=HistogramCalibrator scale=1012.8726196289062 quant)
W1118 09:37:55.229977 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator scale=5724.5654296875 quant)
W1118 09:37:55.230168 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2694 calibrator=HistogramCalibrator scale=456.2269592285156 quant)
W1118 09:37:55.230347 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5767.67333984375 quant)
W1118 09:37:55.230530 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1354 calibrator=HistogramCalibrator scale=899.12109375 quant)
W1118 09:37:55.230709 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator scale=6964.83544921875 quant)
W1118 09:37:55.230896 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3080 calibrator=HistogramCalibrator scale=396.2749328613281 quant)
W1118 09:37:55.231075 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator scale=7419.46435546875 quant)
W1118 09:37:55.231261 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1370 calibrator=HistogramCalibrator scale=894.2604370117188 quant)
W1118 09:37:55.231439 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator scale=12689.9384765625 quant)
W1118 09:37:55.231623 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3080 calibrator=HistogramCalibrator scale=396.2749328613281 quant)
W1118 09:37:55.231821 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator scale=5907.0205078125 quant)
W1118 09:37:55.232021 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1664 calibrator=HistogramCalibrator scale=737.9207153320312 quant)
W1118 09:37:55.232204 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator scale=23570.1171875 quant)
W1118 09:37:55.232388 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0863 calibrator=HistogramCalibrator scale=1399.9598388671875 quant)
W1118 09:37:55.232569 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator scale=24710.396484375 quant)
W1118 09:37:55.232756 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1603 calibrator=HistogramCalibrator scale=761.8325805664062 quant)
W1118 09:37:55.232935 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator scale=18983.900390625 quant)
W1118 09:37:55.233120 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0909 calibrator=HistogramCalibrator scale=1350.146728515625 quant)
W1118 09:37:55.233299 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator scale=21349.779296875 quant)
W1118 09:37:55.233484 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1819 calibrator=HistogramCalibrator scale=674.1987915039062 quant)
W1118 09:37:55.233663 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator scale=25490.427734375 quant)
W1118 09:37:55.233849 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0871 calibrator=HistogramCalibrator scale=1418.9847412109375 quant)
W1118 09:37:55.234027 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25880.146484375 quant)
W1118 09:37:55.234211 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2351 calibrator=HistogramCalibrator scale=522.8048095703125 quant)
W1118 09:37:55.234388 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator scale=35586.0625 quant)
W1118 09:37:55.234585 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0738 calibrator=HistogramCalibrator scale=1672.347900390625 quant)
W1118 09:37:55.234764 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator scale=31525.89453125 quant)
W1118 09:37:55.234956 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2847 calibrator=HistogramCalibrator scale=431.346923828125 quant)
W1118 09:37:55.235136 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator scale=36465.29296875 quant)
W1118 09:37:55.235318 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0782 calibrator=HistogramCalibrator scale=1553.845947265625 quant)
W1118 09:37:55.235495 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator scale=26951.70703125 quant)
W1118 09:37:55.235680 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3596 calibrator=HistogramCalibrator scale=341.9155578613281 quant)
W1118 09:37:55.235877 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator scale=33513.1875 quant)
W1118 09:37:55.236061 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1449 calibrator=HistogramCalibrator scale=858.1163940429688 quant)
W1118 09:37:55.236239 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25662.263671875 quant)
W1118 09:37:55.236422 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3596 calibrator=HistogramCalibrator scale=341.9155578613281 quant)
W1118 09:37:55.236601 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator scale=6245.236328125 quant)
W1118 09:37:55.236786 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7173 calibrator=HistogramCalibrator scale=175.85745239257812 quant)
W1118 09:37:55.236963 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator scale=53943.83984375 quant)
W1118 09:37:55.237144 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1848 calibrator=HistogramCalibrator scale=676.7193603515625 quant)
W1118 09:37:55.237322 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator scale=21318.76171875 quant)
W1118 09:37:55.237506 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.0856 calibrator=HistogramCalibrator scale=116.13914489746094 quant)
W1118 09:37:55.237692 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator scale=63009.3046875 quant)
W1118 09:37:55.237883 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1815 calibrator=HistogramCalibrator scale=687.2215576171875 quant)
W1118 09:37:55.238063 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator scale=21765.048828125 quant)
Model saved to ./saved/retrained_model_epoch_11.pth after epoch 11
Epoch 12/15
Epoch: [12]  [ 0/40]  eta: 0:13:40  lr: 0.0001  img/s: 6.253331756850812  loss: 0.0347 (0.0347)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 20.5024  data: 0.0334
Epoch: [12]  [ 1/40]  eta: 0:13:24  lr: 0.0001  img/s: 6.177286308593467  loss: 0.0347 (0.0540)  acc1: 97.6562 (98.8281)  acc5: 100.0000 (100.0000)  time: 20.6235  data: 0.0284
Epoch: [12]  [ 2/40]  eta: 0:13:04  lr: 0.0001  img/s: 6.189475184652977  loss: 0.0636 (0.0572)  acc1: 98.4375 (98.6979)  acc5: 100.0000 (100.0000)  time: 20.6503  data: 0.0268
Epoch: [12]  [ 3/40]  eta: 0:12:44  lr: 0.0001  img/s: 6.189947890820375  loss: 0.0347 (0.0496)  acc1: 98.4375 (99.0234)  acc5: 100.0000 (100.0000)  time: 20.6635  data: 0.0262
Epoch: [12]  [ 4/40]  eta: 0:12:24  lr: 0.0001  img/s: 6.194802942207353  loss: 0.0347 (0.0465)  acc1: 100.0000 (99.2188)  acc5: 100.0000 (100.0000)  time: 20.6679  data: 0.0256
Epoch: [12]  [ 5/40]  eta: 0:12:03  lr: 0.0001  img/s: 6.191815157856534  loss: 0.0347 (0.0479)  acc1: 98.4375 (99.0885)  acc5: 100.0000 (100.0000)  time: 20.6727  data: 0.0254
Epoch: [12]  [ 6/40]  eta: 0:11:42  lr: 0.0001  img/s: 6.204900713262452  loss: 0.0422 (0.0471)  acc1: 99.2188 (99.1071)  acc5: 100.0000 (100.0000)  time: 20.6698  data: 0.0251
Epoch: [12]  [ 7/40]  eta: 0:11:21  lr: 0.0001  img/s: 6.262735096845865  loss: 0.0422 (0.0493)  acc1: 98.4375 (99.0234)  acc5: 100.0000 (100.0000)  time: 20.6439  data: 0.0249
Epoch: [12]  [ 8/40]  eta: 0:11:00  lr: 0.0001  img/s: 6.214449239569603  loss: 0.0422 (0.0480)  acc1: 98.4375 (98.9583)  acc5: 100.0000 (100.0000)  time: 20.6413  data: 0.0248
Epoch: [12]  [ 9/40]  eta: 0:10:39  lr: 0.0001  img/s: 6.19827345698818  loss: 0.0422 (0.0531)  acc1: 98.4375 (98.8281)  acc5: 100.0000 (100.0000)  time: 20.6446  data: 0.0247
Epoch: [12]  [10/40]  eta: 0:10:19  lr: 0.0001  img/s: 6.212598564241484  loss: 0.0547 (0.0546)  acc1: 98.4375 (98.7926)  acc5: 100.0000 (100.0000)  time: 20.6430  data: 0.0246
Epoch: [12]  [11/40]  eta: 0:09:58  lr: 0.0001  img/s: 6.248466016826684  loss: 0.0547 (0.0563)  acc1: 98.4375 (98.5677)  acc5: 100.0000 (100.0000)  time: 20.6319  data: 0.0245
Epoch: [12]  [12/40]  eta: 0:09:37  lr: 0.0001  img/s: 6.188405154440427  loss: 0.0547 (0.0549)  acc1: 98.4375 (98.6779)  acc5: 100.0000 (100.0000)  time: 20.6377  data: 0.0245
Epoch: [12]  [13/40]  eta: 0:09:16  lr: 0.0001  img/s: 6.251584881262766  loss: 0.0547 (0.0549)  acc1: 98.4375 (98.6607)  acc5: 100.0000 (100.0000)  time: 20.6277  data: 0.0244
Epoch: [12]  [14/40]  eta: 0:08:56  lr: 0.0001  img/s: 6.240279105415136  loss: 0.0557 (0.0590)  acc1: 98.4375 (98.6458)  acc5: 100.0000 (99.9479)  time: 20.6216  data: 0.0243
Epoch: [12]  [15/40]  eta: 0:08:35  lr: 0.0001  img/s: 6.180227642698701  loss: 0.0557 (0.0592)  acc1: 98.4375 (98.6328)  acc5: 100.0000 (99.9512)  time: 20.6286  data: 0.0243
Epoch: [12]  [16/40]  eta: 0:08:15  lr: 0.0001  img/s: 6.200946667548637  loss: 0.0616 (0.0601)  acc1: 98.4375 (98.6673)  acc5: 100.0000 (99.9540)  time: 20.6308  data: 0.0242
Epoch: [12]  [17/40]  eta: 0:07:54  lr: 0.0001  img/s: 6.196976628052374  loss: 0.0557 (0.0584)  acc1: 98.4375 (98.6979)  acc5: 100.0000 (99.9566)  time: 20.6335  data: 0.0242
Epoch: [12]  [18/40]  eta: 0:07:33  lr: 0.0001  img/s: 6.211254558488169  loss: 0.0557 (0.0579)  acc1: 98.4375 (98.7253)  acc5: 100.0000 (99.9589)  time: 20.6334  data: 0.0242
Epoch: [12]  [19/40]  eta: 0:07:13  lr: 0.0001  img/s: 6.248573504547704  loss: 0.0557 (0.0583)  acc1: 98.4375 (98.7500)  acc5: 100.0000 (99.9609)  time: 20.6271  data: 0.0241
Epoch: [12]  [20/40]  eta: 0:06:52  lr: 0.0001  img/s: 6.200870892455538  loss: 0.0586 (0.0583)  acc1: 98.4375 (98.7723)  acc5: 100.0000 (99.9628)  time: 20.6353  data: 0.0237
Epoch: [12]  [21/40]  eta: 0:06:31  lr: 0.0001  img/s: 6.21637789029181  loss: 0.0557 (0.0572)  acc1: 98.4375 (98.7926)  acc5: 100.0000 (99.9645)  time: 20.6288  data: 0.0237
Epoch: [12]  [22/40]  eta: 0:06:11  lr: 0.0001  img/s: 6.190569141074779  loss: 0.0547 (0.0561)  acc1: 99.2188 (98.8451)  acc5: 100.0000 (99.9660)  time: 20.6287  data: 0.0237
Epoch: [12]  [23/40]  eta: 0:05:50  lr: 0.0001  img/s: 6.2212414168521555  loss: 0.0547 (0.0554)  acc1: 99.2188 (98.8607)  acc5: 100.0000 (99.9674)  time: 20.6235  data: 0.0238
Epoch: [12]  [24/40]  eta: 0:05:30  lr: 0.0001  img/s: 6.22755431109594  loss: 0.0557 (0.0560)  acc1: 98.4375 (98.8125)  acc5: 100.0000 (99.9688)  time: 20.6181  data: 0.0238
Epoch: [12]  [25/40]  eta: 0:05:09  lr: 0.0001  img/s: 6.192977592687678  loss: 0.0586 (0.0565)  acc1: 98.4375 (98.7981)  acc5: 100.0000 (99.9700)  time: 20.6179  data: 0.0238
Epoch: [12]  [26/40]  eta: 0:04:48  lr: 0.0001  img/s: 6.2346707165305695  loss: 0.0586 (0.0559)  acc1: 98.4375 (98.8137)  acc5: 100.0000 (99.9711)  time: 20.6129  data: 0.0238
Epoch: [12]  [27/40]  eta: 0:04:28  lr: 0.0001  img/s: 6.215146144305245  loss: 0.0557 (0.0552)  acc1: 99.2188 (98.8281)  acc5: 100.0000 (99.9721)  time: 20.6207  data: 0.0238
Epoch: [12]  [28/40]  eta: 0:04:07  lr: 0.0001  img/s: 6.255576165469176  loss: 0.0557 (0.0550)  acc1: 99.2188 (98.8685)  acc5: 100.0000 (99.9731)  time: 20.6139  data: 0.0237
Epoch: [12]  [29/40]  eta: 0:03:46  lr: 0.0001  img/s: 6.2425275809032215  loss: 0.0557 (0.0562)  acc1: 99.2188 (98.8281)  acc5: 100.0000 (99.9479)  time: 20.6066  data: 0.0237
Epoch: [12]  [30/40]  eta: 0:03:26  lr: 0.0001  img/s: 6.194197636438863  loss: 0.0497 (0.0556)  acc1: 99.2188 (98.8155)  acc5: 100.0000 (99.9496)  time: 20.6097  data: 0.0237
Epoch: [12]  [31/40]  eta: 0:03:05  lr: 0.0001  img/s: 6.199286915205915  loss: 0.0497 (0.0564)  acc1: 99.2188 (98.7793)  acc5: 100.0000 (99.9512)  time: 20.6178  data: 0.0237
Epoch: [12]  [32/40]  eta: 0:02:44  lr: 0.0001  img/s: 6.221432032694267  loss: 0.0557 (0.0572)  acc1: 99.2188 (98.7453)  acc5: 100.0000 (99.9527)  time: 20.6123  data: 0.0237
Epoch: [12]  [33/40]  eta: 0:02:24  lr: 0.0001  img/s: 6.208367765101953  loss: 0.0586 (0.0572)  acc1: 99.2188 (98.7362)  acc5: 100.0000 (99.9540)  time: 20.6194  data: 0.0237
Epoch: [12]  [34/40]  eta: 0:02:03  lr: 0.0001  img/s: 6.189048783425196  loss: 0.0586 (0.0573)  acc1: 99.2188 (98.7500)  acc5: 100.0000 (99.9554)  time: 20.6279  data: 0.0237
Epoch: [12]  [35/40]  eta: 0:01:43  lr: 0.0001  img/s: 6.2189065999935345  loss: 0.0497 (0.0567)  acc1: 99.2188 (98.7630)  acc5: 100.0000 (99.9566)  time: 20.6215  data: 0.0238
Epoch: [12]  [36/40]  eta: 0:01:22  lr: 0.0001  img/s: 6.218024194694442  loss: 0.0497 (0.0576)  acc1: 99.2188 (98.7331)  acc5: 100.0000 (99.9578)  time: 20.6187  data: 0.0238
Epoch: [12]  [37/40]  eta: 0:01:01  lr: 0.0001  img/s: 6.227531411762629  loss: 0.0497 (0.0571)  acc1: 99.2188 (98.7459)  acc5: 100.0000 (99.9589)  time: 20.6136  data: 0.0238
Epoch: [12]  [38/40]  eta: 0:00:41  lr: 0.0001  img/s: 6.232023335705399  loss: 0.0487 (0.0565)  acc1: 99.2188 (98.7780)  acc5: 100.0000 (99.9599)  time: 20.6102  data: 0.0238
Epoch: [12]  [39/40]  eta: 0:00:20  lr: 0.0001  img/s: 5.887096714767222  loss: 0.0487 (0.0617)  acc1: 99.2188 (98.7800)  acc5: 100.0000 (99.9600)  time: 19.6528  data: 0.0227
Epoch: [12] Total time: 0:13:25
  0%|                                                     | 0/2 [00:00<?, ?it/s] 50%|██████████████████████▌                      | 1/2 [00:12<00:12, 12.20s/it]100%|█████████████████████████████████████████████| 2/2 [00:24<00:00, 12.25s/it]100%|█████████████████████████████████████████████| 2/2 [00:36<00:00, 18.36s/it]
W1118 09:51:57.698594 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.698704 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.698767 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.698814 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.698863 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.698905 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.698955 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.698998 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699045 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699087 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699133 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699174 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699219 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699260 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699309 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699350 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699396 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699438 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699483 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699534 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699581 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699623 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699667 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699708 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699753 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699826 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699875 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699920 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.699969 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700010 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700055 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700096 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700142 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700182 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700227 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700267 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700311 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700352 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700398 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700438 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700483 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700524 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700569 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700609 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700661 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700722 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700785 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700829 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700874 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700914 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.700959 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701000 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701045 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701084 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701128 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701168 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701212 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701251 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701296 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701336 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701380 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701420 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701462 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701502 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701547 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701587 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701631 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701670 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701714 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701753 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701797 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701836 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 09:51:57.701999 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=59.752037048339844 quant)
W1118 09:51:57.702168 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1412 calibrator=HistogramCalibrator scale=899.2946166992188 quant)
W1118 09:51:57.702320 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5170 calibrator=HistogramCalibrator scale=241.09051513671875 quant)
W1118 09:51:57.702458 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=2279.60791015625 quant)
W1118 09:51:57.702600 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2133 calibrator=HistogramCalibrator scale=574.7078247070312 quant)
W1118 09:51:57.702735 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator scale=3882.4990234375 quant)
W1118 09:51:57.702884 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4478 calibrator=HistogramCalibrator scale=277.907958984375 quant)
W1118 09:51:57.703045 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0397 calibrator=HistogramCalibrator scale=3195.420166015625 quant)
W1118 09:51:57.703185 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1862 calibrator=HistogramCalibrator scale=660.4222412109375 quant)
W1118 09:51:57.703318 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0347 calibrator=HistogramCalibrator scale=3660.60595703125 quant)
W1118 09:51:57.703455 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4223 calibrator=HistogramCalibrator scale=294.2596130371094 quant)
W1118 09:51:57.703593 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator scale=3579.044677734375 quant)
W1118 09:51:57.703733 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1582 calibrator=HistogramCalibrator scale=781.07421875 quant)
W1118 09:51:57.703917 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator scale=5040.06396484375 quant)
W1118 09:51:57.704062 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3937 calibrator=HistogramCalibrator scale=315.8301696777344 quant)
W1118 09:51:57.704197 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator scale=4408.29052734375 quant)
W1118 09:51:57.704334 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1681 calibrator=HistogramCalibrator scale=742.4534912109375 quant)
W1118 09:51:57.704471 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator scale=4450.294921875 quant)
W1118 09:51:57.704609 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3937 calibrator=HistogramCalibrator scale=315.8301696777344 quant)
W1118 09:51:57.704741 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0551 calibrator=HistogramCalibrator scale=2306.1630859375 quant)
W1118 09:51:57.704879 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2476 calibrator=HistogramCalibrator scale=502.08636474609375 quant)
W1118 09:51:57.705012 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator scale=5454.41064453125 quant)
W1118 09:51:57.705160 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1203 calibrator=HistogramCalibrator scale=1029.717041015625 quant)
W1118 09:51:57.705319 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5771.935546875 quant)
W1118 09:51:57.705461 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2542 calibrator=HistogramCalibrator scale=489.44873046875 quant)
W1118 09:51:57.705597 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=5269.53662109375 quant)
W1118 09:51:57.705735 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1183 calibrator=HistogramCalibrator scale=1043.0257568359375 quant)
W1118 09:51:57.705870 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator scale=5724.5654296875 quant)
W1118 09:51:57.706013 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2621 calibrator=HistogramCalibrator scale=471.3428649902344 quant)
W1118 09:51:57.706147 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5767.67333984375 quant)
W1118 09:51:57.706286 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1308 calibrator=HistogramCalibrator scale=937.8525390625 quant)
W1118 09:51:57.706420 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator scale=6964.83544921875 quant)
W1118 09:51:57.706563 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2973 calibrator=HistogramCalibrator scale=412.30914306640625 quant)
W1118 09:51:57.706697 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator scale=7424.8056640625 quant)
W1118 09:51:57.706835 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1327 calibrator=HistogramCalibrator scale=926.8868408203125 quant)
W1118 09:51:57.706970 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator scale=12689.9384765625 quant)
W1118 09:51:57.707111 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.2973 calibrator=HistogramCalibrator scale=412.30914306640625 quant)
W1118 09:51:57.707245 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator scale=5907.0205078125 quant)
W1118 09:51:57.707418 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1608 calibrator=HistogramCalibrator scale=763.4462890625 quant)
W1118 09:51:57.707574 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator scale=23570.1171875 quant)
W1118 09:51:57.707715 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0835 calibrator=HistogramCalibrator scale=1471.1441650390625 quant)
W1118 09:51:57.707885 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator scale=24710.396484375 quant)
W1118 09:51:57.708033 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1561 calibrator=HistogramCalibrator scale=792.3388061523438 quant)
W1118 09:51:57.708169 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator scale=18983.900390625 quant)
W1118 09:51:57.708311 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0885 calibrator=HistogramCalibrator scale=1397.6080322265625 quant)
W1118 09:51:57.708444 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator scale=21349.779296875 quant)
W1118 09:51:57.708585 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1770 calibrator=HistogramCalibrator scale=698.3760375976562 quant)
W1118 09:51:57.708719 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator scale=25490.427734375 quant)
W1118 09:51:57.708859 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0848 calibrator=HistogramCalibrator scale=1458.8359375 quant)
W1118 09:51:57.708995 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25880.146484375 quant)
W1118 09:51:57.709135 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2292 calibrator=HistogramCalibrator scale=540.1610717773438 quant)
W1118 09:51:57.709269 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator scale=35586.0625 quant)
W1118 09:51:57.709408 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0717 calibrator=HistogramCalibrator scale=1719.941650390625 quant)
W1118 09:51:57.709542 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator scale=31525.89453125 quant)
W1118 09:51:57.709693 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2780 calibrator=HistogramCalibrator scale=446.0093078613281 quant)
W1118 09:51:57.709850 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator scale=36465.29296875 quant)
W1118 09:51:57.710000 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0754 calibrator=HistogramCalibrator scale=1625.0777587890625 quant)
W1118 09:51:57.710133 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator scale=26951.70703125 quant)
W1118 09:51:57.710272 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3499 calibrator=HistogramCalibrator scale=353.18426513671875 quant)
W1118 09:51:57.710406 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator scale=33513.1875 quant)
W1118 09:51:57.710543 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1418 calibrator=HistogramCalibrator scale=876.4010620117188 quant)
W1118 09:51:57.710676 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25662.263671875 quant)
W1118 09:51:57.710814 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3499 calibrator=HistogramCalibrator scale=353.18426513671875 quant)
W1118 09:51:57.710947 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator scale=6245.236328125 quant)
W1118 09:51:57.711086 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7104 calibrator=HistogramCalibrator scale=177.04835510253906 quant)
W1118 09:51:57.711218 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator scale=53943.83984375 quant)
W1118 09:51:57.711354 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1820 calibrator=HistogramCalibrator scale=687.3485107421875 quant)
W1118 09:51:57.711487 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator scale=21318.76171875 quant)
W1118 09:51:57.711636 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.0771 calibrator=HistogramCalibrator scale=116.98309326171875 quant)
W1118 09:51:57.711790 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator scale=63009.3046875 quant)
W1118 09:51:57.711958 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1781 calibrator=HistogramCalibrator scale=699.9087524414062 quant)
W1118 09:51:57.712115 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator scale=21765.048828125 quant)
Model saved to ./saved/retrained_model_epoch_12.pth after epoch 12
Epoch 13/15
Epoch: [13]  [ 0/40]  eta: 0:13:44  lr: 0.0001  img/s: 6.215294509436194  loss: 0.0430 (0.0430)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 20.6185  data: 0.0242
Epoch: [13]  [ 1/40]  eta: 0:13:24  lr: 0.0001  img/s: 6.203234615023318  loss: 0.0252 (0.0341)  acc1: 99.2188 (99.6094)  acc5: 100.0000 (100.0000)  time: 20.6383  data: 0.0239
Epoch: [13]  [ 2/40]  eta: 0:13:04  lr: 0.0001  img/s: 6.21015399807004  loss: 0.0430 (0.0467)  acc1: 99.2188 (99.4792)  acc5: 100.0000 (100.0000)  time: 20.6372  data: 0.0238
Epoch: [13]  [ 3/40]  eta: 0:12:42  lr: 0.0001  img/s: 6.237942518654684  loss: 0.0430 (0.0469)  acc1: 99.2188 (99.4141)  acc5: 100.0000 (100.0000)  time: 20.6137  data: 0.0238
Epoch: [13]  [ 4/40]  eta: 0:12:21  lr: 0.0001  img/s: 6.235270561526698  loss: 0.0477 (0.0502)  acc1: 99.2188 (99.0625)  acc5: 100.0000 (100.0000)  time: 20.6024  data: 0.0248
Epoch: [13]  [ 5/40]  eta: 0:12:02  lr: 0.0001  img/s: 6.171255893673282  loss: 0.0430 (0.0468)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 20.6296  data: 0.0247
Epoch: [13]  [ 6/40]  eta: 0:11:41  lr: 0.0001  img/s: 6.193552936156376  loss: 0.0477 (0.0516)  acc1: 99.2188 (99.1071)  acc5: 100.0000 (100.0000)  time: 20.6383  data: 0.0245
Epoch: [13]  [ 7/40]  eta: 0:11:21  lr: 0.0001  img/s: 6.203029273495102  loss: 0.0477 (0.0515)  acc1: 99.2188 (99.1211)  acc5: 100.0000 (100.0000)  time: 20.6408  data: 0.0244
Epoch: [13]  [ 8/40]  eta: 0:11:00  lr: 0.0001  img/s: 6.210363690921134  loss: 0.0477 (0.0486)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 20.6401  data: 0.0244
Epoch: [13]  [ 9/40]  eta: 0:10:39  lr: 0.0001  img/s: 6.2046292902383975  loss: 0.0477 (0.0490)  acc1: 99.2188 (99.1406)  acc5: 100.0000 (100.0000)  time: 20.6415  data: 0.0243
Epoch: [13]  [10/40]  eta: 0:10:19  lr: 0.0001  img/s: 6.161306204336555  loss: 0.0513 (0.0495)  acc1: 99.2188 (99.0767)  acc5: 100.0000 (100.0000)  time: 20.6558  data: 0.0242
Epoch: [13]  [11/40]  eta: 0:09:58  lr: 0.0001  img/s: 6.2302765581493444  loss: 0.0477 (0.0478)  acc1: 99.2188 (99.1536)  acc5: 100.0000 (100.0000)  time: 20.6485  data: 0.0242
Epoch: [13]  [12/40]  eta: 0:09:38  lr: 0.0001  img/s: 6.211779901416292  loss: 0.0477 (0.0465)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 20.6470  data: 0.0242
Epoch: [13]  [13/40]  eta: 0:09:17  lr: 0.0001  img/s: 6.244609234240784  loss: 0.0477 (0.0494)  acc1: 99.2188 (99.1629)  acc5: 100.0000 (100.0000)  time: 20.6382  data: 0.0243
Epoch: [13]  [14/40]  eta: 0:08:56  lr: 0.0001  img/s: 6.229935098870446  loss: 0.0513 (0.0559)  acc1: 99.2188 (99.0104)  acc5: 100.0000 (99.9479)  time: 20.6338  data: 0.0243
Epoch: [13]  [15/40]  eta: 0:08:35  lr: 0.0001  img/s: 6.21256786678172  loss: 0.0513 (0.0562)  acc1: 99.2188 (98.9258)  acc5: 100.0000 (99.9512)  time: 20.6333  data: 0.0243
Epoch: [13]  [16/40]  eta: 0:08:15  lr: 0.0001  img/s: 6.232649153610539  loss: 0.0518 (0.0561)  acc1: 99.2188 (98.9430)  acc5: 100.0000 (99.9540)  time: 20.6291  data: 0.0243
Epoch: [13]  [17/40]  eta: 0:07:54  lr: 0.0001  img/s: 6.209861500556171  loss: 0.0513 (0.0545)  acc1: 99.2188 (99.0017)  acc5: 100.0000 (99.9566)  time: 20.6295  data: 0.0243
Epoch: [13]  [18/40]  eta: 0:07:33  lr: 0.0001  img/s: 6.214425573308058  loss: 0.0517 (0.0543)  acc1: 99.2188 (99.0132)  acc5: 100.0000 (99.9589)  time: 20.6291  data: 0.0243
Epoch: [13]  [19/40]  eta: 0:07:13  lr: 0.0001  img/s: 6.215094556343753  loss: 0.0513 (0.0533)  acc1: 99.2188 (99.0625)  acc5: 100.0000 (99.9609)  time: 20.6285  data: 0.0242
Epoch: [13]  [20/40]  eta: 0:06:52  lr: 0.0001  img/s: 6.231502302536596  loss: 0.0517 (0.0550)  acc1: 99.2188 (98.9955)  acc5: 100.0000 (99.9628)  time: 20.6258  data: 0.0242
Epoch: [13]  [21/40]  eta: 0:06:31  lr: 0.0001  img/s: 6.200804286388915  loss: 0.0517 (0.0541)  acc1: 99.2188 (99.0057)  acc5: 100.0000 (99.9645)  time: 20.6262  data: 0.0242
Epoch: [13]  [22/40]  eta: 0:06:11  lr: 0.0001  img/s: 6.23175590019016  loss: 0.0517 (0.0544)  acc1: 99.2188 (98.9470)  acc5: 100.0000 (99.9660)  time: 20.6227  data: 0.0242
Epoch: [13]  [23/40]  eta: 0:05:50  lr: 0.0001  img/s: 6.1716230891676  loss: 0.0517 (0.0534)  acc1: 99.2188 (98.9583)  acc5: 100.0000 (99.9674)  time: 20.6337  data: 0.0242
Epoch: [13]  [24/40]  eta: 0:05:30  lr: 0.0001  img/s: 6.182344834457395  loss: 0.0513 (0.0532)  acc1: 99.2188 (98.9375)  acc5: 100.0000 (99.9688)  time: 20.6422  data: 0.0240
Epoch: [13]  [25/40]  eta: 0:05:09  lr: 0.0001  img/s: 6.2268851710020945  loss: 0.0517 (0.0538)  acc1: 98.4375 (98.9183)  acc5: 100.0000 (99.9700)  time: 20.6330  data: 0.0240
Epoch: [13]  [26/40]  eta: 0:04:48  lr: 0.0001  img/s: 6.227292748609629  loss: 0.0513 (0.0528)  acc1: 99.2188 (98.9583)  acc5: 100.0000 (99.9711)  time: 20.6274  data: 0.0240
Epoch: [13]  [27/40]  eta: 0:04:28  lr: 0.0001  img/s: 6.184004073808135  loss: 0.0517 (0.0529)  acc1: 98.4375 (98.9397)  acc5: 100.0000 (99.9721)  time: 20.6306  data: 0.0241
Epoch: [13]  [28/40]  eta: 0:04:07  lr: 0.0001  img/s: 6.172571785185163  loss: 0.0517 (0.0525)  acc1: 98.4375 (98.9494)  acc5: 100.0000 (99.9731)  time: 20.6370  data: 0.0241
Epoch: [13]  [29/40]  eta: 0:03:47  lr: 0.0001  img/s: 6.1766728378066365  loss: 0.0517 (0.0532)  acc1: 98.4375 (98.9323)  acc5: 100.0000 (99.9479)  time: 20.6417  data: 0.0241
Epoch: [13]  [30/40]  eta: 0:03:26  lr: 0.0001  img/s: 6.19004780776786  loss: 0.0473 (0.0527)  acc1: 99.2188 (98.9415)  acc5: 100.0000 (99.9496)  time: 20.6368  data: 0.0241
Epoch: [13]  [31/40]  eta: 0:03:05  lr: 0.0001  img/s: 6.210044523770137  loss: 0.0517 (0.0541)  acc1: 98.4375 (98.8525)  acc5: 100.0000 (99.9512)  time: 20.6403  data: 0.0241
Epoch: [13]  [32/40]  eta: 0:02:45  lr: 0.0001  img/s: 6.189999847230506  loss: 0.0541 (0.0542)  acc1: 98.4375 (98.8636)  acc5: 100.0000 (99.9527)  time: 20.6439  data: 0.0241
Epoch: [13]  [33/40]  eta: 0:02:24  lr: 0.0001  img/s: 6.233640735390708  loss: 0.0541 (0.0562)  acc1: 98.4375 (98.8051)  acc5: 100.0000 (99.9540)  time: 20.6456  data: 0.0240
Epoch: [13]  [34/40]  eta: 0:02:03  lr: 0.0001  img/s: 6.128624550380988  loss: 0.0522 (0.0561)  acc1: 99.2188 (98.8170)  acc5: 100.0000 (99.9554)  time: 20.6625  data: 0.0239
Epoch: [13]  [35/40]  eta: 0:01:43  lr: 0.0001  img/s: 6.1667558381086165  loss: 0.0517 (0.0556)  acc1: 99.2188 (98.8281)  acc5: 100.0000 (99.9566)  time: 20.6702  data: 0.0239
Epoch: [13]  [36/40]  eta: 0:01:22  lr: 0.0001  img/s: 6.217320812351971  loss: 0.0473 (0.0548)  acc1: 99.2188 (98.8598)  acc5: 100.0000 (99.9578)  time: 20.6727  data: 0.0239
Epoch: [13]  [37/40]  eta: 0:01:01  lr: 0.0001  img/s: 6.221669453706086  loss: 0.0517 (0.0549)  acc1: 99.2188 (98.8692)  acc5: 100.0000 (99.9589)  time: 20.6707  data: 0.0239
Epoch: [13]  [38/40]  eta: 0:00:41  lr: 0.0001  img/s: 6.192119170093431  loss: 0.0522 (0.0549)  acc1: 99.2188 (98.8782)  acc5: 100.0000 (99.9599)  time: 20.6747  data: 0.0241
Epoch: [13]  [39/40]  eta: 0:00:20  lr: 0.0001  img/s: 5.719182450718408  loss: 0.0531 (0.0639)  acc1: 99.2188 (98.8600)  acc5: 100.0000 (99.9600)  time: 19.7138  data: 0.0230
Epoch: [13] Total time: 0:13:26
  0%|                                                     | 0/2 [00:00<?, ?it/s] 50%|██████████████████████▌                      | 1/2 [00:12<00:12, 12.38s/it]100%|█████████████████████████████████████████████| 2/2 [00:24<00:00, 12.34s/it]100%|█████████████████████████████████████████████| 2/2 [00:37<00:00, 18.51s/it]
W1118 10:06:01.702528 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.702653 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.702730 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.702790 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.702852 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.702908 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.702970 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703027 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703088 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703143 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703203 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703259 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703319 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703375 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703438 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703493 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703553 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703608 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703666 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703721 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703805 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703860 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703910 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.703953 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704000 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704042 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704089 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704131 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704180 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704224 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704270 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704311 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704360 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704401 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704447 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704489 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704535 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704576 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704624 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704666 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704712 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704754 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704801 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704842 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704887 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.704948 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705020 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705087 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705159 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705224 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705280 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705323 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705370 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705411 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705458 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705499 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705545 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705586 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705635 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705676 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705723 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705764 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705811 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705852 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705899 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705940 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.705985 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.706026 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.706072 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.706112 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.706161 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.706202 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:06:01.706372 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=59.752037048339844 quant)
W1118 10:06:01.706543 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1412 calibrator=HistogramCalibrator scale=899.2946166992188 quant)
W1118 10:06:01.706697 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5072 calibrator=HistogramCalibrator scale=245.65435791015625 quant)
W1118 10:06:01.706850 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=2279.60791015625 quant)
W1118 10:06:01.706995 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2066 calibrator=HistogramCalibrator scale=595.2999267578125 quant)
W1118 10:06:01.707143 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator scale=3882.4990234375 quant)
W1118 10:06:01.707297 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4382 calibrator=HistogramCalibrator scale=283.597900390625 quant)
W1118 10:06:01.707499 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0397 calibrator=HistogramCalibrator scale=3195.420166015625 quant)
W1118 10:06:01.707691 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1808 calibrator=HistogramCalibrator scale=682.1943969726562 quant)
W1118 10:06:01.707897 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0347 calibrator=HistogramCalibrator scale=3660.60595703125 quant)
W1118 10:06:01.708087 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4145 calibrator=HistogramCalibrator scale=300.7381286621094 quant)
W1118 10:06:01.708272 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator scale=3579.044677734375 quant)
W1118 10:06:01.708460 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1542 calibrator=HistogramCalibrator scale=802.6818237304688 quant)
W1118 10:06:01.708646 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator scale=5040.06396484375 quant)
W1118 10:06:01.708834 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3863 calibrator=HistogramCalibrator scale=322.5752868652344 quant)
W1118 10:06:01.709017 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator scale=4405.9228515625 quant)
W1118 10:06:01.709203 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1653 calibrator=HistogramCalibrator scale=755.58740234375 quant)
W1118 10:06:01.709383 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator scale=4450.294921875 quant)
W1118 10:06:01.709567 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3863 calibrator=HistogramCalibrator scale=322.5752868652344 quant)
W1118 10:06:01.709748 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0551 calibrator=HistogramCalibrator scale=2306.1630859375 quant)
W1118 10:06:01.709934 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2428 calibrator=HistogramCalibrator scale=512.8646240234375 quant)
W1118 10:06:01.710114 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator scale=5450.48095703125 quant)
W1118 10:06:01.710311 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1174 calibrator=HistogramCalibrator scale=1055.67626953125 quant)
W1118 10:06:01.710497 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5771.935546875 quant)
W1118 10:06:01.710686 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2486 calibrator=HistogramCalibrator scale=499.5484924316406 quant)
W1118 10:06:01.710869 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=5269.53662109375 quant)
W1118 10:06:01.711052 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1153 calibrator=HistogramCalibrator scale=1073.5321044921875 quant)
W1118 10:06:01.711232 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator scale=5724.5654296875 quant)
W1118 10:06:01.711424 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2556 calibrator=HistogramCalibrator scale=484.4762878417969 quant)
W1118 10:06:01.711604 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5767.67333984375 quant)
W1118 10:06:01.711810 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1265 calibrator=HistogramCalibrator scale=970.7072143554688 quant)
W1118 10:06:01.711989 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator scale=6959.9990234375 quant)
W1118 10:06:01.712174 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2890 calibrator=HistogramCalibrator scale=427.1226501464844 quant)
W1118 10:06:01.712347 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator scale=7424.8056640625 quant)
W1118 10:06:01.712525 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1293 calibrator=HistogramCalibrator scale=957.0423583984375 quant)
W1118 10:06:01.712697 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator scale=12689.9384765625 quant)
W1118 10:06:01.712879 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.2890 calibrator=HistogramCalibrator scale=427.1226501464844 quant)
W1118 10:06:01.713051 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator scale=5907.0205078125 quant)
W1118 10:06:01.713252 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1569 calibrator=HistogramCalibrator scale=789.9432983398438 quant)
W1118 10:06:01.713430 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator scale=23570.1171875 quant)
W1118 10:06:01.713612 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0807 calibrator=HistogramCalibrator scale=1520.540283203125 quant)
W1118 10:06:01.713783 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator scale=24710.396484375 quant)
W1118 10:06:01.713966 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1520 calibrator=HistogramCalibrator scale=813.4678955078125 quant)
W1118 10:06:01.714138 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator scale=18983.900390625 quant)
W1118 10:06:01.714318 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0861 calibrator=HistogramCalibrator scale=1434.771484375 quant)
W1118 10:06:01.714493 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator scale=21349.779296875 quant)
W1118 10:06:01.714675 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1729 calibrator=HistogramCalibrator scale=717.489501953125 quant)
W1118 10:06:01.714848 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator scale=25490.427734375 quant)
W1118 10:06:01.715030 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0830 calibrator=HistogramCalibrator scale=1497.8984375 quant)
W1118 10:06:01.715202 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25880.146484375 quant)
W1118 10:06:01.715382 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2239 calibrator=HistogramCalibrator scale=554.06884765625 quant)
W1118 10:06:01.715555 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator scale=35586.0625 quant)
W1118 10:06:01.715734 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0699 calibrator=HistogramCalibrator scale=1770.3236083984375 quant)
W1118 10:06:01.715931 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator scale=31606.73046875 quant)
W1118 10:06:01.716127 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2721 calibrator=HistogramCalibrator scale=456.8435363769531 quant)
W1118 10:06:01.716304 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator scale=36465.29296875 quant)
W1118 10:06:01.716483 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0728 calibrator=HistogramCalibrator scale=1684.334716796875 quant)
W1118 10:06:01.716656 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator scale=26951.70703125 quant)
W1118 10:06:01.716838 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3421 calibrator=HistogramCalibrator scale=362.94244384765625 quant)
W1118 10:06:01.717009 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator scale=33513.1875 quant)
W1118 10:06:01.717186 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1397 calibrator=HistogramCalibrator scale=895.4818725585938 quant)
W1118 10:06:01.717358 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25662.263671875 quant)
W1118 10:06:01.717535 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3421 calibrator=HistogramCalibrator scale=362.94244384765625 quant)
W1118 10:06:01.717707 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator scale=6245.236328125 quant)
W1118 10:06:01.717887 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7052 calibrator=HistogramCalibrator scale=178.7633514404297 quant)
W1118 10:06:01.718060 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator scale=53943.83984375 quant)
W1118 10:06:01.718237 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1798 calibrator=HistogramCalibrator scale=697.7865600585938 quant)
W1118 10:06:01.718410 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator scale=21318.76171875 quant)
W1118 10:06:01.718591 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.0698 calibrator=HistogramCalibrator scale=117.90577697753906 quant)
W1118 10:06:01.718766 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator scale=63009.3046875 quant)
W1118 10:06:01.718961 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1754 calibrator=HistogramCalibrator scale=713.0731811523438 quant)
W1118 10:06:01.719134 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator scale=21765.048828125 quant)
Model saved to ./saved/retrained_model_epoch_13.pth after epoch 13
Epoch 14/15
Epoch: [14]  [ 0/40]  eta: 0:13:51  lr: 0.0001  img/s: 6.168673210349285  loss: 0.0483 (0.0483)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 20.7754  data: 0.0254
Epoch: [14]  [ 1/40]  eta: 0:13:30  lr: 0.0001  img/s: 6.163805797060584  loss: 0.0293 (0.0388)  acc1: 99.2188 (99.6094)  acc5: 100.0000 (100.0000)  time: 20.7826  data: 0.0244
Epoch: [14]  [ 2/40]  eta: 0:13:07  lr: 0.0001  img/s: 6.209311274777177  loss: 0.0483 (0.0426)  acc1: 99.2188 (99.4792)  acc5: 100.0000 (100.0000)  time: 20.7344  data: 0.0242
Epoch: [14]  [ 3/40]  eta: 0:12:47  lr: 0.0001  img/s: 6.171883401621085  loss: 0.0434 (0.0428)  acc1: 99.2188 (99.4141)  acc5: 100.0000 (100.0000)  time: 20.7416  data: 0.0241
Epoch: [14]  [ 4/40]  eta: 0:12:26  lr: 0.0001  img/s: 6.165949919896882  loss: 0.0434 (0.0418)  acc1: 99.2188 (99.3750)  acc5: 100.0000 (100.0000)  time: 20.7499  data: 0.0241
Epoch: [14]  [ 5/40]  eta: 0:12:05  lr: 0.0001  img/s: 6.211354661443277  loss: 0.0434 (0.0458)  acc1: 99.2188 (99.0885)  acc5: 100.0000 (100.0000)  time: 20.7302  data: 0.0241
Epoch: [14]  [ 6/40]  eta: 0:11:45  lr: 0.0001  img/s: 6.165126159049673  loss: 0.0434 (0.0431)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 20.7381  data: 0.0240
Epoch: [14]  [ 7/40]  eta: 0:11:24  lr: 0.0001  img/s: 6.16998282424597  loss: 0.0434 (0.0435)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 20.7424  data: 0.0244
Epoch: [14]  [ 8/40]  eta: 0:11:03  lr: 0.0001  img/s: 6.182754505447101  loss: 0.0434 (0.0424)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (100.0000)  time: 20.7407  data: 0.0244
Epoch: [14]  [ 9/40]  eta: 0:10:42  lr: 0.0001  img/s: 6.178920716894917  loss: 0.0434 (0.0455)  acc1: 99.2188 (98.9844)  acc5: 100.0000 (100.0000)  time: 20.7406  data: 0.0243
Epoch: [14]  [10/40]  eta: 0:10:21  lr: 0.0001  img/s: 6.232374429396019  loss: 0.0463 (0.0459)  acc1: 99.2188 (99.0057)  acc5: 100.0000 (100.0000)  time: 20.7244  data: 0.0244
Epoch: [14]  [11/40]  eta: 0:10:00  lr: 0.0001  img/s: 6.235749999152106  loss: 0.0434 (0.0447)  acc1: 99.2188 (99.0234)  acc5: 100.0000 (100.0000)  time: 20.7099  data: 0.0243
Epoch: [14]  [12/40]  eta: 0:09:39  lr: 0.0001  img/s: 6.219642693504912  loss: 0.0463 (0.0454)  acc1: 99.2188 (98.9784)  acc5: 100.0000 (100.0000)  time: 20.7018  data: 0.0243
Epoch: [14]  [13/40]  eta: 0:09:18  lr: 0.0001  img/s: 6.197270989080091  loss: 0.0463 (0.0503)  acc1: 99.2188 (98.9397)  acc5: 100.0000 (99.9442)  time: 20.7001  data: 0.0242
Epoch: [14]  [14/40]  eta: 0:08:58  lr: 0.0001  img/s: 6.1948391828272475  loss: 0.0483 (0.0527)  acc1: 99.2188 (98.9583)  acc5: 100.0000 (99.8958)  time: 20.6992  data: 0.0242
Epoch: [14]  [15/40]  eta: 0:08:37  lr: 0.0001  img/s: 6.17155313704317  loss: 0.0463 (0.0519)  acc1: 99.2188 (98.9746)  acc5: 100.0000 (99.9023)  time: 20.7032  data: 0.0242
Epoch: [14]  [16/40]  eta: 0:08:16  lr: 0.0001  img/s: 6.232588302677433  loss: 0.0483 (0.0537)  acc1: 99.2188 (98.8971)  acc5: 100.0000 (99.9081)  time: 20.6949  data: 0.0242
Epoch: [14]  [17/40]  eta: 0:07:55  lr: 0.0001  img/s: 6.22625949911186  loss: 0.0463 (0.0525)  acc1: 99.2188 (98.9583)  acc5: 100.0000 (99.9132)  time: 20.6887  data: 0.0243
Epoch: [14]  [18/40]  eta: 0:07:35  lr: 0.0001  img/s: 6.168930651292247  loss: 0.0463 (0.0518)  acc1: 99.2188 (99.0132)  acc5: 100.0000 (99.9178)  time: 20.6931  data: 0.0243
Epoch: [14]  [19/40]  eta: 0:07:14  lr: 0.0001  img/s: 6.197617104715366  loss: 0.0434 (0.0512)  acc1: 99.2188 (99.0234)  acc5: 100.0000 (99.9219)  time: 20.6923  data: 0.0242
Epoch: [14]  [20/40]  eta: 0:06:53  lr: 0.0001  img/s: 6.217807575147648  loss: 0.0434 (0.0521)  acc1: 99.2188 (99.0327)  acc5: 100.0000 (99.9256)  time: 20.6840  data: 0.0241
Epoch: [14]  [21/40]  eta: 0:06:33  lr: 0.0001  img/s: 6.207033913526096  loss: 0.0463 (0.0522)  acc1: 99.2188 (99.0412)  acc5: 100.0000 (99.9290)  time: 20.6768  data: 0.0242
Epoch: [14]  [22/40]  eta: 0:06:12  lr: 0.0001  img/s: 6.121100884628521  loss: 0.0463 (0.0545)  acc1: 99.2188 (98.9810)  acc5: 100.0000 (99.9321)  time: 20.6917  data: 0.0242
Epoch: [14]  [23/40]  eta: 0:05:51  lr: 0.0001  img/s: 6.149801778137014  loss: 0.0463 (0.0534)  acc1: 99.2188 (99.0234)  acc5: 100.0000 (99.9349)  time: 20.6954  data: 0.0242
Epoch: [14]  [24/40]  eta: 0:05:31  lr: 0.0001  img/s: 6.16746397912263  loss: 0.0463 (0.0528)  acc1: 99.2188 (99.0312)  acc5: 100.0000 (99.9375)  time: 20.6951  data: 0.0242
Epoch: [14]  [25/40]  eta: 0:05:10  lr: 0.0001  img/s: 6.17015705124481  loss: 0.0463 (0.0534)  acc1: 99.2188 (98.9784)  acc5: 100.0000 (99.9399)  time: 20.7022  data: 0.0243
Epoch: [14]  [26/40]  eta: 0:04:49  lr: 0.0001  img/s: 6.173859909817202  loss: 0.0463 (0.0529)  acc1: 99.2188 (99.0162)  acc5: 100.0000 (99.9421)  time: 20.7007  data: 0.0243
Epoch: [14]  [27/40]  eta: 0:04:29  lr: 0.0001  img/s: 6.174050757264951  loss: 0.0438 (0.0526)  acc1: 99.2188 (99.0234)  acc5: 100.0000 (99.9442)  time: 20.6999  data: 0.0242
Epoch: [14]  [28/40]  eta: 0:04:08  lr: 0.0001  img/s: 6.178811203209427  loss: 0.0438 (0.0517)  acc1: 99.2188 (99.0571)  acc5: 100.0000 (99.9461)  time: 20.7005  data: 0.0242
Epoch: [14]  [29/40]  eta: 0:03:47  lr: 0.0001  img/s: 6.199464805582495  loss: 0.0438 (0.0545)  acc1: 99.2188 (98.9844)  acc5: 100.0000 (99.9479)  time: 20.6971  data: 0.0242
Epoch: [14]  [30/40]  eta: 0:03:27  lr: 0.0001  img/s: 6.20425930396943  loss: 0.0405 (0.0538)  acc1: 99.2188 (99.0171)  acc5: 100.0000 (99.9496)  time: 20.7017  data: 0.0241
Epoch: [14]  [31/40]  eta: 0:03:06  lr: 0.0001  img/s: 6.186597331845304  loss: 0.0438 (0.0539)  acc1: 99.2188 (98.9990)  acc5: 100.0000 (99.9512)  time: 20.7099  data: 0.0241
Epoch: [14]  [32/40]  eta: 0:02:45  lr: 0.0001  img/s: 6.202642135977115  loss: 0.0438 (0.0547)  acc1: 99.2188 (98.9347)  acc5: 100.0000 (99.9527)  time: 20.7127  data: 0.0241
Epoch: [14]  [33/40]  eta: 0:02:24  lr: 0.0001  img/s: 6.196401292009858  loss: 0.0438 (0.0549)  acc1: 99.2188 (98.9200)  acc5: 100.0000 (99.9540)  time: 20.7128  data: 0.0241
Epoch: [14]  [34/40]  eta: 0:02:04  lr: 0.0001  img/s: 6.178497332894297  loss: 0.0405 (0.0545)  acc1: 99.2188 (98.9509)  acc5: 100.0000 (99.9554)  time: 20.7156  data: 0.0241
Epoch: [14]  [35/40]  eta: 0:01:43  lr: 0.0001  img/s: 6.192014258574417  loss: 0.0405 (0.0538)  acc1: 99.2188 (98.9800)  acc5: 100.0000 (99.9566)  time: 20.7121  data: 0.0241
Epoch: [14]  [36/40]  eta: 0:01:22  lr: 0.0001  img/s: 6.236801181508612  loss: 0.0404 (0.0532)  acc1: 99.2188 (99.0076)  acc5: 100.0000 (99.9578)  time: 20.7115  data: 0.0241
Epoch: [14]  [37/40]  eta: 0:01:02  lr: 0.0001  img/s: 6.211492856251569  loss: 0.0404 (0.0528)  acc1: 99.2188 (99.0132)  acc5: 100.0000 (99.9589)  time: 20.7138  data: 0.0240
Epoch: [14]  [38/40]  eta: 0:00:41  lr: 0.0001  img/s: 6.219869889584203  loss: 0.0405 (0.0525)  acc1: 99.2188 (99.0184)  acc5: 100.0000 (99.9599)  time: 20.7054  data: 0.0241
Epoch: [14]  [39/40]  eta: 0:00:20  lr: 0.0001  img/s: 5.81645922013231  loss: 0.0411 (0.0581)  acc1: 99.2188 (99.0200)  acc5: 100.0000 (99.9600)  time: 19.7404  data: 0.0230
Epoch: [14] Total time: 0:13:28
  0%|                                                     | 0/2 [00:00<?, ?it/s] 50%|██████████████████████▌                      | 1/2 [00:12<00:12, 12.35s/it]100%|█████████████████████████████████████████████| 2/2 [00:24<00:00, 12.30s/it]100%|█████████████████████████████████████████████| 2/2 [00:36<00:00, 18.47s/it]
W1118 10:20:07.436012 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436143 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436220 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436279 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436342 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436398 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436461 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436518 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436578 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436632 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436692 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436746 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436806 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436860 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436922 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.436976 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437035 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437091 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437150 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437205 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437265 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437318 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437377 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437430 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437489 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437542 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437599 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437651 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437711 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437765 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437823 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437877 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437938 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.437992 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438051 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438105 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438164 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438218 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438278 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438331 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438389 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438442 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438502 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438555 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438663 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438761 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438851 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.438934 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.439021 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.439103 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.439191 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.439273 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.439359 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.439441 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.439527 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.439608 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.439694 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.439797 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.439906 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.439990 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.440076 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.440156 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.440242 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.440323 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.440410 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.440491 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.440577 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.440657 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.440743 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.440824 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.440908 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.440995 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:20:07.441201 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=59.752037048339844 quant)
W1118 10:20:07.441452 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1412 calibrator=HistogramCalibrator scale=899.2946166992188 quant)
W1118 10:20:07.441687 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4983 calibrator=HistogramCalibrator scale=250.39434814453125 quant)
W1118 10:20:07.441910 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=2279.60791015625 quant)
W1118 10:20:07.442136 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2011 calibrator=HistogramCalibrator scale=614.6538696289062 quant)
W1118 10:20:07.442358 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator scale=3882.4990234375 quant)
W1118 10:20:07.442634 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4300 calibrator=HistogramCalibrator scale=289.84454345703125 quant)
W1118 10:20:07.442864 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0397 calibrator=HistogramCalibrator scale=3195.420166015625 quant)
W1118 10:20:07.443088 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1764 calibrator=HistogramCalibrator scale=702.4576416015625 quant)
W1118 10:20:07.443310 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0347 calibrator=HistogramCalibrator scale=3660.60595703125 quant)
W1118 10:20:07.443533 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4071 calibrator=HistogramCalibrator scale=306.4190979003906 quant)
W1118 10:20:07.443753 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator scale=3579.044677734375 quant)
W1118 10:20:07.444011 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1508 calibrator=HistogramCalibrator scale=823.7828369140625 quant)
W1118 10:20:07.444229 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator scale=5040.06396484375 quant)
W1118 10:20:07.444452 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3794 calibrator=HistogramCalibrator scale=328.7707214355469 quant)
W1118 10:20:07.444668 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator scale=4405.9228515625 quant)
W1118 10:20:07.444888 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1627 calibrator=HistogramCalibrator scale=768.3804321289062 quant)
W1118 10:20:07.445101 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator scale=4450.294921875 quant)
W1118 10:20:07.445322 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3794 calibrator=HistogramCalibrator scale=328.7707214355469 quant)
W1118 10:20:07.445538 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0551 calibrator=HistogramCalibrator scale=2306.1630859375 quant)
W1118 10:20:07.445760 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2388 calibrator=HistogramCalibrator scale=523.1593627929688 quant)
W1118 10:20:07.445974 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator scale=5450.48095703125 quant)
W1118 10:20:07.446243 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1151 calibrator=HistogramCalibrator scale=1081.4244384765625 quant)
W1118 10:20:07.446471 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5771.935546875 quant)
W1118 10:20:07.446693 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2437 calibrator=HistogramCalibrator scale=510.90185546875 quant)
W1118 10:20:07.446908 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=5269.53662109375 quant)
W1118 10:20:07.447127 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1125 calibrator=HistogramCalibrator scale=1101.13720703125 quant)
W1118 10:20:07.447342 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator scale=5724.5654296875 quant)
W1118 10:20:07.447565 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2500 calibrator=HistogramCalibrator scale=496.7804260253906 quant)
W1118 10:20:07.447825 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5767.67333984375 quant)
W1118 10:20:07.447995 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1227 calibrator=HistogramCalibrator scale=1004.2901000976562 quant)
W1118 10:20:07.448135 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator scale=6959.9990234375 quant)
W1118 10:20:07.448280 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2819 calibrator=HistogramCalibrator scale=439.40130615234375 quant)
W1118 10:20:07.448415 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator scale=7424.8056640625 quant)
W1118 10:20:07.448556 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1259 calibrator=HistogramCalibrator scale=982.2720336914062 quant)
W1118 10:20:07.448693 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator scale=12689.9384765625 quant)
W1118 10:20:07.448833 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.2819 calibrator=HistogramCalibrator scale=439.40130615234375 quant)
W1118 10:20:07.448969 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator scale=5907.0205078125 quant)
W1118 10:20:07.449137 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1526 calibrator=HistogramCalibrator scale=809.2530517578125 quant)
W1118 10:20:07.449301 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator scale=23570.1171875 quant)
W1118 10:20:07.449468 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0782 calibrator=HistogramCalibrator scale=1573.36865234375 quant)
W1118 10:20:07.449630 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator scale=24710.396484375 quant)
W1118 10:20:07.449798 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1476 calibrator=HistogramCalibrator scale=835.7545776367188 quant)
W1118 10:20:07.449958 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator scale=18983.900390625 quant)
W1118 10:20:07.450126 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0837 calibrator=HistogramCalibrator scale=1475.404541015625 quant)
W1118 10:20:07.450285 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator scale=21349.779296875 quant)
W1118 10:20:07.450449 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1688 calibrator=HistogramCalibrator scale=734.4989624023438 quant)
W1118 10:20:07.450609 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator scale=25490.427734375 quant)
W1118 10:20:07.450774 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0812 calibrator=HistogramCalibrator scale=1531.0098876953125 quant)
W1118 10:20:07.450937 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25880.146484375 quant)
W1118 10:20:07.451109 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2195 calibrator=HistogramCalibrator scale=567.260986328125 quant)
W1118 10:20:07.451289 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator scale=35586.0625 quant)
W1118 10:20:07.451483 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0684 calibrator=HistogramCalibrator scale=1816.1871337890625 quant)
W1118 10:20:07.451656 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator scale=31606.73046875 quant)
W1118 10:20:07.451860 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2667 calibrator=HistogramCalibrator scale=466.6680908203125 quant)
W1118 10:20:07.452026 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator scale=36465.29296875 quant)
W1118 10:20:07.452192 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0705 calibrator=HistogramCalibrator scale=1744.0765380859375 quant)
W1118 10:20:07.452349 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator scale=26951.70703125 quant)
W1118 10:20:07.452512 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3353 calibrator=HistogramCalibrator scale=371.2137145996094 quant)
W1118 10:20:07.452669 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator scale=33513.1875 quant)
W1118 10:20:07.452829 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1371 calibrator=HistogramCalibrator scale=909.132568359375 quant)
W1118 10:20:07.452985 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25662.263671875 quant)
W1118 10:20:07.453148 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3353 calibrator=HistogramCalibrator scale=371.2137145996094 quant)
W1118 10:20:07.453307 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator scale=6245.236328125 quant)
W1118 10:20:07.453469 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6999 calibrator=HistogramCalibrator scale=180.09739685058594 quant)
W1118 10:20:07.453626 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator scale=53943.83984375 quant)
W1118 10:20:07.453786 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1780 calibrator=HistogramCalibrator scale=706.3681030273438 quant)
W1118 10:20:07.453943 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator scale=21318.76171875 quant)
W1118 10:20:07.454106 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.0614 calibrator=HistogramCalibrator scale=118.70831298828125 quant)
W1118 10:20:07.454262 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator scale=63009.3046875 quant)
W1118 10:20:07.454425 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1736 calibrator=HistogramCalibrator scale=723.9667358398438 quant)
W1118 10:20:07.454591 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator scale=21765.048828125 quant)
Model saved to ./saved/retrained_model_epoch_14.pth after epoch 14
Epoch 15/15
Epoch: [15]  [ 0/40]  eta: 0:13:50  lr: 0.0001  img/s: 6.175239985599605  loss: 0.0929 (0.0929)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 20.7532  data: 0.0252
Epoch: [15]  [ 1/40]  eta: 0:13:33  lr: 0.0001  img/s: 6.11028766537182  loss: 0.0481 (0.0705)  acc1: 96.8750 (98.0469)  acc5: 100.0000 (100.0000)  time: 20.8637  data: 0.0256
Epoch: [15]  [ 2/40]  eta: 0:13:10  lr: 0.0001  img/s: 6.207529402291679  loss: 0.0868 (0.0759)  acc1: 97.6562 (97.9167)  acc5: 100.0000 (100.0000)  time: 20.7907  data: 0.0252
Epoch: [15]  [ 3/40]  eta: 0:12:50  lr: 0.0001  img/s: 6.1371115754641785  loss: 0.0481 (0.0641)  acc1: 97.6562 (98.4375)  acc5: 100.0000 (100.0000)  time: 20.8132  data: 0.0249
Epoch: [15]  [ 4/40]  eta: 0:12:29  lr: 0.0001  img/s: 6.164106428024481  loss: 0.0501 (0.0613)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 20.8084  data: 0.0247
Epoch: [15]  [ 5/40]  eta: 0:12:08  lr: 0.0001  img/s: 6.128295961068636  loss: 0.0501 (0.0681)  acc1: 97.6562 (98.3073)  acc5: 100.0000 (100.0000)  time: 20.8254  data: 0.0245
Epoch: [15]  [ 6/40]  eta: 0:11:47  lr: 0.0001  img/s: 6.203584121079446  loss: 0.0501 (0.0625)  acc1: 98.4375 (98.5491)  acc5: 100.0000 (100.0000)  time: 20.8013  data: 0.0244
Epoch: [15]  [ 7/40]  eta: 0:11:25  lr: 0.0001  img/s: 6.230903109998424  loss: 0.0501 (0.0634)  acc1: 98.4375 (98.6328)  acc5: 100.0000 (100.0000)  time: 20.7720  data: 0.0243
Epoch: [15]  [ 8/40]  eta: 0:11:04  lr: 0.0001  img/s: 6.224743647307252  loss: 0.0501 (0.0602)  acc1: 99.2188 (98.6979)  acc5: 100.0000 (100.0000)  time: 20.7516  data: 0.0244
Epoch: [15]  [ 9/40]  eta: 0:10:43  lr: 0.0001  img/s: 6.198892800903534  loss: 0.0501 (0.0657)  acc1: 98.4375 (98.5938)  acc5: 100.0000 (100.0000)  time: 20.7437  data: 0.0243
Epoch: [15]  [10/40]  eta: 0:10:22  lr: 0.0001  img/s: 6.154559037433049  loss: 0.0501 (0.0628)  acc1: 99.2188 (98.7216)  acc5: 100.0000 (100.0000)  time: 20.7507  data: 0.0243
Epoch: [15]  [11/40]  eta: 0:10:01  lr: 0.0001  img/s: 6.178390109473793  loss: 0.0481 (0.0603)  acc1: 99.2188 (98.8281)  acc5: 100.0000 (100.0000)  time: 20.7499  data: 0.0242
Epoch: [15]  [12/40]  eta: 0:09:40  lr: 0.0001  img/s: 6.211717085559902  loss: 0.0481 (0.0587)  acc1: 99.2188 (98.9183)  acc5: 100.0000 (100.0000)  time: 20.7407  data: 0.0242
Epoch: [15]  [13/40]  eta: 0:09:19  lr: 0.0001  img/s: 6.203982631836429  loss: 0.0481 (0.0606)  acc1: 99.2188 (98.9397)  acc5: 100.0000 (100.0000)  time: 20.7347  data: 0.0242
Epoch: [15]  [14/40]  eta: 0:08:59  lr: 0.0001  img/s: 6.182548239214128  loss: 0.0501 (0.0664)  acc1: 99.2188 (98.8542)  acc5: 100.0000 (99.9479)  time: 20.7341  data: 0.0242
Epoch: [15]  [15/40]  eta: 0:08:38  lr: 0.0001  img/s: 6.198441842730695  loss: 0.0501 (0.0666)  acc1: 99.2188 (98.8281)  acc5: 100.0000 (99.9512)  time: 20.7304  data: 0.0242
Epoch: [15]  [16/40]  eta: 0:08:17  lr: 0.0001  img/s: 6.197991522595386  loss: 0.0501 (0.0648)  acc1: 99.2188 (98.8971)  acc5: 100.0000 (99.9540)  time: 20.7276  data: 0.0245
Epoch: [15]  [17/40]  eta: 0:07:56  lr: 0.0001  img/s: 6.198119677917678  loss: 0.0501 (0.0640)  acc1: 99.2188 (98.8715)  acc5: 100.0000 (99.9566)  time: 20.7247  data: 0.0245
Epoch: [15]  [18/40]  eta: 0:07:35  lr: 0.0001  img/s: 6.20120881495363  loss: 0.0504 (0.0636)  acc1: 99.2188 (98.8898)  acc5: 100.0000 (99.9589)  time: 20.7215  data: 0.0245
Epoch: [15]  [19/40]  eta: 0:07:15  lr: 0.0001  img/s: 6.195211906824642  loss: 0.0504 (0.0633)  acc1: 99.2188 (98.8281)  acc5: 100.0000 (99.9609)  time: 20.7197  data: 0.0244
Epoch: [15]  [20/40]  eta: 0:06:54  lr: 0.0001  img/s: 6.192835791551718  loss: 0.0504 (0.0661)  acc1: 99.2188 (98.7351)  acc5: 100.0000 (99.9628)  time: 20.7167  data: 0.0244
Epoch: [15]  [21/40]  eta: 0:06:33  lr: 0.0001  img/s: 6.2126872075552315  loss: 0.0532 (0.0655)  acc1: 98.4375 (98.7216)  acc5: 100.0000 (99.9645)  time: 20.6993  data: 0.0243
Epoch: [15]  [22/40]  eta: 0:06:12  lr: 0.0001  img/s: 6.196592248244434  loss: 0.0532 (0.0669)  acc1: 98.4375 (98.7092)  acc5: 100.0000 (99.9660)  time: 20.7011  data: 0.0242
Epoch: [15]  [23/40]  eta: 0:05:52  lr: 0.0001  img/s: 6.206401605627251  loss: 0.0532 (0.0652)  acc1: 98.4375 (98.7630)  acc5: 100.0000 (99.9674)  time: 20.6895  data: 0.0242
Epoch: [15]  [24/40]  eta: 0:05:31  lr: 0.0001  img/s: 6.221718266770166  loss: 0.0532 (0.0646)  acc1: 99.2188 (98.7812)  acc5: 100.0000 (99.9688)  time: 20.6799  data: 0.0242
Epoch: [15]  [25/40]  eta: 0:05:10  lr: 0.0001  img/s: 6.221406871310706  loss: 0.0532 (0.0650)  acc1: 99.2188 (98.7079)  acc5: 100.0000 (99.9700)  time: 20.6643  data: 0.0243
Epoch: [15]  [26/40]  eta: 0:04:49  lr: 0.0001  img/s: 6.204050453299834  loss: 0.0532 (0.0638)  acc1: 99.2188 (98.7269)  acc5: 100.0000 (99.9711)  time: 20.6642  data: 0.0243
Epoch: [15]  [27/40]  eta: 0:04:29  lr: 0.0001  img/s: 6.189532199578024  loss: 0.0532 (0.0648)  acc1: 98.4375 (98.6886)  acc5: 100.0000 (99.9721)  time: 20.6711  data: 0.0243
Epoch: [15]  [28/40]  eta: 0:04:08  lr: 0.0001  img/s: 6.200775137732464  loss: 0.0532 (0.0641)  acc1: 98.4375 (98.7069)  acc5: 100.0000 (99.9731)  time: 20.6750  data: 0.0242
Epoch: [15]  [29/40]  eta: 0:03:47  lr: 0.0001  img/s: 6.175790654177741  loss: 0.0532 (0.0660)  acc1: 98.4375 (98.6458)  acc5: 100.0000 (99.9479)  time: 20.6789  data: 0.0242
Epoch: [15]  [30/40]  eta: 0:03:26  lr: 0.0001  img/s: 6.23635302198225  loss: 0.0532 (0.0649)  acc1: 98.4375 (98.6895)  acc5: 100.0000 (99.9496)  time: 20.6652  data: 0.0242
Epoch: [15]  [31/40]  eta: 0:03:06  lr: 0.0001  img/s: 6.209713969288068  loss: 0.0565 (0.0657)  acc1: 98.4375 (98.6328)  acc5: 100.0000 (99.9512)  time: 20.6600  data: 0.0242
Epoch: [15]  [32/40]  eta: 0:02:45  lr: 0.0001  img/s: 6.232985917355765  loss: 0.0573 (0.0656)  acc1: 98.4375 (98.6506)  acc5: 100.0000 (99.9527)  time: 20.6565  data: 0.0242
Epoch: [15]  [33/40]  eta: 0:02:24  lr: 0.0001  img/s: 6.2215418367586866  loss: 0.0573 (0.0657)  acc1: 98.4375 (98.6673)  acc5: 100.0000 (99.9540)  time: 20.6535  data: 0.0242
Epoch: [15]  [34/40]  eta: 0:02:04  lr: 0.0001  img/s: 6.2347200958366376  loss: 0.0565 (0.0653)  acc1: 98.4375 (98.6607)  acc5: 100.0000 (99.9554)  time: 20.6454  data: 0.0247
Epoch: [15]  [35/40]  eta: 0:01:43  lr: 0.0001  img/s: 6.1951093210440815  loss: 0.0565 (0.0654)  acc1: 98.4375 (98.6328)  acc5: 100.0000 (99.9566)  time: 20.6459  data: 0.0247
Epoch: [15]  [36/40]  eta: 0:01:22  lr: 0.0001  img/s: 6.240062600668449  loss: 0.0565 (0.0643)  acc1: 98.4375 (98.6698)  acc5: 100.0000 (99.9578)  time: 20.6386  data: 0.0244
Epoch: [15]  [37/40]  eta: 0:01:02  lr: 0.0001  img/s: 6.226598966836559  loss: 0.0573 (0.0646)  acc1: 98.4375 (98.6637)  acc5: 100.0000 (99.9589)  time: 20.6339  data: 0.0244
Epoch: [15]  [38/40]  eta: 0:00:41  lr: 0.0001  img/s: 6.222523107772672  loss: 0.0565 (0.0639)  acc1: 98.4375 (98.6779)  acc5: 100.0000 (99.9599)  time: 20.6304  data: 0.0244
Epoch: [15]  [39/40]  eta: 0:00:20  lr: 0.0001  img/s: 5.759146049473026  loss: 0.0643 (0.0694)  acc1: 98.4375 (98.6800)  acc5: 100.0000 (99.9600)  time: 19.6657  data: 0.0233
Epoch: [15] Total time: 0:13:27
  0%|                                                     | 0/2 [00:00<?, ?it/s] 50%|██████████████████████▌                      | 1/2 [00:12<00:12, 12.31s/it]100%|█████████████████████████████████████████████| 2/2 [00:24<00:00, 12.36s/it]100%|█████████████████████████████████████████████| 2/2 [00:37<00:00, 18.54s/it]
W1118 10:34:12.372941 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373066 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373143 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373203 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373268 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373326 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373391 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373446 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373507 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373562 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373624 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373687 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373748 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373803 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373867 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373923 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.373984 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374040 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374100 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374155 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374216 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374272 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374332 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374387 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374448 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374503 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374563 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374618 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374680 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374736 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374796 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374851 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374916 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.374972 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375031 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375087 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375148 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375204 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375265 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375321 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375380 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375436 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375497 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375551 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375609 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375666 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375727 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375805 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375872 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375928 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.375989 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376044 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376103 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376158 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376217 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376272 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376332 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376388 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376450 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376505 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376564 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376619 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376678 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376733 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376794 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376850 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376909 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.376965 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.377025 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.377079 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.377138 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.377192 128479731717952 tensor_quantizer.py:173] Disable HistogramCalibrator
W1118 10:34:12.377369 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator scale=59.752037048339844 quant)
W1118 10:34:12.377568 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1412 calibrator=HistogramCalibrator scale=899.2946166992188 quant)
W1118 10:34:12.377762 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4919 calibrator=HistogramCalibrator scale=254.88475036621094 quant)
W1118 10:34:12.377935 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=2279.60791015625 quant)
W1118 10:34:12.378109 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1957 calibrator=HistogramCalibrator scale=631.64892578125 quant)
W1118 10:34:12.378279 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0327 calibrator=HistogramCalibrator scale=3882.4990234375 quant)
W1118 10:34:12.378453 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4227 calibrator=HistogramCalibrator scale=295.37469482421875 quant)
W1118 10:34:12.378619 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0397 calibrator=HistogramCalibrator scale=3195.420166015625 quant)
W1118 10:34:12.378789 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1724 calibrator=HistogramCalibrator scale=719.7645874023438 quant)
W1118 10:34:12.378956 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0347 calibrator=HistogramCalibrator scale=3660.60595703125 quant)
W1118 10:34:12.379127 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4008 calibrator=HistogramCalibrator scale=311.9435119628906 quant)
W1118 10:34:12.379296 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator scale=3579.044677734375 quant)
W1118 10:34:12.379469 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1475 calibrator=HistogramCalibrator scale=842.38427734375 quant)
W1118 10:34:12.379638 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0252 calibrator=HistogramCalibrator scale=5036.58544921875 quant)
W1118 10:34:12.379834 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3729 calibrator=HistogramCalibrator scale=334.771728515625 quant)
W1118 10:34:12.380001 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator scale=4403.55810546875 quant)
W1118 10:34:12.380163 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1604 calibrator=HistogramCalibrator scale=780.773681640625 quant)
W1118 10:34:12.380320 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0285 calibrator=HistogramCalibrator scale=4450.294921875 quant)
W1118 10:34:12.380484 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3729 calibrator=HistogramCalibrator scale=334.771728515625 quant)
W1118 10:34:12.380641 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0551 calibrator=HistogramCalibrator scale=2306.1630859375 quant)
W1118 10:34:12.380804 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2348 calibrator=HistogramCalibrator scale=531.8948364257812 quant)
W1118 10:34:12.380961 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator scale=5450.48095703125 quant)
W1118 10:34:12.381123 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1129 calibrator=HistogramCalibrator scale=1103.59130859375 quant)
W1118 10:34:12.381279 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5771.935546875 quant)
W1118 10:34:12.381443 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2389 calibrator=HistogramCalibrator scale=521.05224609375 quant)
W1118 10:34:12.381601 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0241 calibrator=HistogramCalibrator scale=5269.53662109375 quant)
W1118 10:34:12.381762 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1101 calibrator=HistogramCalibrator scale=1128.544677734375 quant)
W1118 10:34:12.381918 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator scale=5724.5654296875 quant)
W1118 10:34:12.382087 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2447 calibrator=HistogramCalibrator scale=508.0708923339844 quant)
W1118 10:34:12.382244 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator scale=5767.67333984375 quant)
W1118 10:34:12.382407 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1194 calibrator=HistogramCalibrator scale=1034.9814453125 quant)
W1118 10:34:12.382562 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0182 calibrator=HistogramCalibrator scale=6959.9990234375 quant)
W1118 10:34:12.382726 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2742 calibrator=HistogramCalibrator scale=450.50201416015625 quant)
W1118 10:34:12.382882 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0171 calibrator=HistogramCalibrator scale=7424.8056640625 quant)
W1118 10:34:12.383044 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1225 calibrator=HistogramCalibrator scale=1008.8678588867188 quant)
W1118 10:34:12.383201 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator scale=12689.9384765625 quant)
W1118 10:34:12.383365 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.2742 calibrator=HistogramCalibrator scale=450.50201416015625 quant)
W1118 10:34:12.383521 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator scale=5907.0205078125 quant)
W1118 10:34:12.383694 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1489 calibrator=HistogramCalibrator scale=832.3745727539062 quant)
W1118 10:34:12.383887 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0054 calibrator=HistogramCalibrator scale=23570.1171875 quant)
W1118 10:34:12.384061 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0762 calibrator=HistogramCalibrator scale=1624.914306640625 quant)
W1118 10:34:12.384228 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0051 calibrator=HistogramCalibrator scale=24710.396484375 quant)
W1118 10:34:12.384402 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1438 calibrator=HistogramCalibrator scale=860.3067626953125 quant)
W1118 10:34:12.384567 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator scale=18983.900390625 quant)
W1118 10:34:12.384739 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0817 calibrator=HistogramCalibrator scale=1516.8818359375 quant)
W1118 10:34:12.384908 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0059 calibrator=HistogramCalibrator scale=21349.779296875 quant)
W1118 10:34:12.385080 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1656 calibrator=HistogramCalibrator scale=752.33447265625 quant)
W1118 10:34:12.385246 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0050 calibrator=HistogramCalibrator scale=25490.427734375 quant)
W1118 10:34:12.385418 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0794 calibrator=HistogramCalibrator scale=1563.9346923828125 quant)
W1118 10:34:12.385584 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25880.146484375 quant)
W1118 10:34:12.385754 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2155 calibrator=HistogramCalibrator scale=578.57666015625 quant)
W1118 10:34:12.385919 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0036 calibrator=HistogramCalibrator scale=35586.0625 quant)
W1118 10:34:12.386087 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0669 calibrator=HistogramCalibrator scale=1856.589599609375 quant)
W1118 10:34:12.386253 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator scale=31633.765625 quant)
W1118 10:34:12.386423 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2622 calibrator=HistogramCalibrator scale=476.1195983886719 quant)
W1118 10:34:12.386587 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0035 calibrator=HistogramCalibrator scale=36465.29296875 quant)
W1118 10:34:12.386754 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0683 calibrator=HistogramCalibrator scale=1801.8001708984375 quant)
W1118 10:34:12.386920 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0047 calibrator=HistogramCalibrator scale=26951.70703125 quant)
W1118 10:34:12.387090 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3287 calibrator=HistogramCalibrator scale=378.8106384277344 quant)
W1118 10:34:12.387256 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0038 calibrator=HistogramCalibrator scale=33513.1875 quant)
W1118 10:34:12.387425 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1350 calibrator=HistogramCalibrator scale=926.0728759765625 quant)
W1118 10:34:12.387589 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0049 calibrator=HistogramCalibrator scale=25662.263671875 quant)
W1118 10:34:12.387758 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3287 calibrator=HistogramCalibrator scale=378.8106384277344 quant)
W1118 10:34:12.387943 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator scale=6245.236328125 quant)
W1118 10:34:12.388114 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6959 calibrator=HistogramCalibrator scale=181.45152282714844 quant)
W1118 10:34:12.388271 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator scale=53943.83984375 quant)
W1118 10:34:12.388429 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1759 calibrator=HistogramCalibrator scale=713.49755859375 quant)
W1118 10:34:12.388587 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0060 calibrator=HistogramCalibrator scale=21318.76171875 quant)
W1118 10:34:12.388750 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.0553 calibrator=HistogramCalibrator scale=119.65852355957031 quant)
W1118 10:34:12.388905 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0020 calibrator=HistogramCalibrator scale=63061.20703125 quant)
W1118 10:34:12.389065 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1716 calibrator=HistogramCalibrator scale=731.4158935546875 quant)
W1118 10:34:12.389220 128479731717952 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).
layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0058 calibrator=HistogramCalibrator scale=21765.048828125 quant)
Model saved to ./saved/retrained_model_epoch_15.pth after epoch 15
  0%|                                                    | 0/78 [00:00<?, ?it/s]  1%|▌                                           | 1/78 [00:12<15:37, 12.17s/it]  3%|█▏                                          | 2/78 [00:24<15:27, 12.21s/it]  4%|█▋                                          | 3/78 [00:36<15:16, 12.22s/it]  5%|██▎                                         | 4/78 [00:49<15:09, 12.30s/it]  6%|██▊                                         | 5/78 [01:01<15:01, 12.35s/it]  8%|███▍                                        | 6/78 [01:13<14:43, 12.27s/it]  9%|███▉                                        | 7/78 [01:26<14:37, 12.36s/it] 10%|████▌                                       | 8/78 [01:38<14:18, 12.27s/it] 12%|█████                                       | 9/78 [01:50<14:00, 12.19s/it] 13%|█████▌                                     | 10/78 [02:02<13:47, 12.17s/it] 14%|██████                                     | 11/78 [02:14<13:34, 12.15s/it] 15%|██████▌                                    | 12/78 [02:26<13:19, 12.12s/it] 17%|███████▏                                   | 13/78 [02:38<13:09, 12.15s/it] 18%|███████▋                                   | 14/78 [02:50<12:58, 12.17s/it] 19%|████████▎                                  | 15/78 [03:03<12:46, 12.16s/it] 21%|████████▊                                  | 16/78 [03:15<12:32, 12.14s/it] 22%|█████████▎                                 | 17/78 [03:27<12:18, 12.11s/it] 23%|█████████▉                                 | 18/78 [03:39<12:05, 12.09s/it] 24%|██████████▍                                | 19/78 [03:51<11:52, 12.08s/it] 26%|███████████                                | 20/78 [04:03<11:40, 12.08s/it] 27%|███████████▌                               | 21/78 [04:15<11:29, 12.10s/it] 28%|████████████▏                              | 22/78 [04:27<11:19, 12.14s/it] 29%|████████████▋                              | 23/78 [04:40<11:15, 12.28s/it] 31%|█████████████▏                             | 24/78 [04:52<10:59, 12.21s/it] 32%|█████████████▊                             | 25/78 [05:04<10:46, 12.19s/it] 33%|██████████████▎                            | 26/78 [05:16<10:32, 12.17s/it] 35%|██████████████▉                            | 27/78 [05:28<10:18, 12.13s/it] 36%|███████████████▍                           | 28/78 [05:40<10:07, 12.15s/it] 37%|███████████████▉                           | 29/78 [05:52<09:53, 12.12s/it] 38%|████████████████▌                          | 30/78 [06:05<09:41, 12.12s/it] 40%|█████████████████                          | 31/78 [06:17<09:29, 12.11s/it] 41%|█████████████████▋                         | 32/78 [06:29<09:16, 12.10s/it] 42%|██████████████████▏                        | 33/78 [06:41<09:03, 12.08s/it] 44%|██████████████████▋                        | 34/78 [06:53<08:51, 12.08s/it] 45%|███████████████████▎                       | 35/78 [07:05<08:39, 12.09s/it] 46%|███████████████████▊                       | 36/78 [07:17<08:27, 12.09s/it] 47%|████████████████████▍                      | 37/78 [07:29<08:15, 12.09s/it] 49%|████████████████████▉                      | 38/78 [07:41<08:03, 12.08s/it] 50%|█████████████████████▌                     | 39/78 [07:53<07:51, 12.09s/it] 51%|██████████████████████                     | 40/78 [08:05<07:39, 12.10s/it] 53%|██████████████████████▌                    | 41/78 [08:18<07:27, 12.09s/it] 54%|███████████████████████▏                   | 42/78 [08:30<07:15, 12.08s/it] 55%|███████████████████████▋                   | 43/78 [08:42<07:02, 12.08s/it] 56%|████████████████████████▎                  | 44/78 [08:54<06:51, 12.10s/it] 58%|████████████████████████▊                  | 45/78 [09:06<06:39, 12.12s/it] 59%|█████████████████████████▎                 | 46/78 [09:18<06:27, 12.11s/it] 60%|█████████████████████████▉                 | 47/78 [09:30<06:15, 12.12s/it] 62%|██████████████████████████▍                | 48/78 [09:42<06:03, 12.11s/it]^C 62%|██████████████████████████▍                | 48/78 [09:53<06:11, 12.37s/it]
Traceback (most recent call last):
  File "/workspace/adapt/examples/Homework03.py", line 188, in <module>
  File "/workspace/adapt/examples/Homework03.py", line 180, in main
    
  File "/workspace/adapt/examples/Homework03.py", line 94, in eval
    outputs = model(images)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/adapt/examples/models/resnet.py", line 280, in forward
    x = self.layer4(x)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/adapt/examples/models/resnet.py", line 97, in forward
    identity = self.downsample(x)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/adapt/adapt/approx_layers/axx_layers.py", line 232, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/workspace/adapt/adapt/approx_layers/axx_layers.py", line 229, in _conv_forward
    return AdaPT_Conv2d_Function.apply(input, weight,  self.quantizer, self.quantizer_w, self.kernel_size, self.quantizer.amax, self.quantizer_w.amax, self.max_value, self.out_channels, self.bias_, self.axx_conv2d_kernel, bias, self.stride, self.padding, self.dilation, self.groups, self.padding_mode)
  File "/workspace/adapt/adapt/approx_layers/axx_layers.py", line 153, in forward
    out = axx_conv2d_kernel.forward(quant_input, quant_weight, kernel_size, stride, padding)
KeyboardInterrupt
