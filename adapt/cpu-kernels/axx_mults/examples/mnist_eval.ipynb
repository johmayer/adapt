{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation with AdaPT on MNIST dataset\n",
    "\n",
    "In this notebook you can evaluate different approximate multipliers on various models based on MNIST dataset\n",
    "\n",
    "Steps:\n",
    "* Select number of threads to use\n",
    "* Load dataset\n",
    "* Load Adapt Layers\n",
    "* Define Model\n",
    "* Run model calibration for quantization\n",
    "* Evaluate\n",
    "\n",
    "\n",
    "**Note**:\n",
    "* This notebook should be run on a X86 machine\n",
    "\n",
    "* Please make sure you have run the installation steps first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select number of threads to use\n",
    "\n",
    "For optimal performance set them as the number of your cpu threads (not cpu cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = 40\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "#maybe better performance\n",
    "%env OMP_PLACES=cores\n",
    "%env OMP_PROC_BIND=close\n",
    "%env OMP_WAIT_POLICY=active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST('./datasets/mnist_data/data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "test = MNIST('./datasets/mnist_data/data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=64,num_workers=1, pin_memory=False)\n",
    "train_loader = dataloader.DataLoader(train, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Adapt Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load ADAPT layers\n",
    "from adapt.approx_layers import axx_layers as approxNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose approximate multiplier \n",
    "\n",
    "Two approximate multipliers are already provided\n",
    "\n",
    "**mul8s_acc** - (header file: mul8s_acc.h)   <--  default\n",
    "\n",
    "**mul8s_1L2H** - (header file: mul8s_1L2H.h)\n",
    "\n",
    "\n",
    "\n",
    "In order to use your custom multiplier you need to use the provided tool (LUT_generator) to easily create the C++ header for your multiplier. Then you just place it inside the adapt/cpu-kernels/axx_mults folder. The name of the axx_mult here must match the name of the header file. The same axx_mult is used in all layers. \n",
    "\n",
    "Tip: If you want explicitly to set for each layer a different axx_mult you must do it from the model definition using the respective AdaPT_Conv2d class of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axx_mult = 'mul8s_1L2H'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "Jit compilation method loads 'on the fly' the C++ extentions of the approximate multipliers. Then the pytorch model is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set flag for use of AdaPT custom layers or vanilla PyTorch\n",
    "use_adapt=True\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        if use_adapt:\n",
    "             self.fc1 = approxNN.AdaPT_Linear(784, 548, axx_mult = axx_mult)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(784, 548)\n",
    "        \n",
    "        self.bc1 = nn.BatchNorm1d(548)\n",
    "\n",
    "        if use_adapt:\n",
    "            self.fc2 = approxNN.AdaPT_Linear(548, 252, axx_mult = axx_mult)\n",
    "        else:    \n",
    "            self.fc2 = nn.Linear(548, 252)\n",
    "            \n",
    "        self.bc2 = nn.BatchNorm1d(252)     \n",
    "        \n",
    "        if use_adapt:\n",
    "            self.fc3 = approxNN.AdaPT_Linear(252, 10, axx_mult = axx_mult)\n",
    "        else:\n",
    "            self.fc3 = nn.Linear(252, 10)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 784))\n",
    "        h = self.fc1(x)\n",
    "        h = self.bc1(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        \n",
    "        h = self.fc2(h)\n",
    "        h = self.bc2(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.2, training=self.training)\n",
    "        \n",
    "        h = self.fc3(h)\n",
    "        out = F.log_softmax(h,-1)\n",
    "        return out\n",
    "\n",
    "model = Model()\n",
    "model.cpu()\n",
    "\n",
    "#load pretrained weights\n",
    "model.load_state_dict(torch.load('models/state_dicts/mnist.pt'))\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters \n",
    "\n",
    "Need to re-run it each time the model changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "        \n",
    "     evaluate_x = Variable(data_loader.dataset.data.type_as(torch.FloatTensor())).cpu()\n",
    "     model(evaluate_x)\n",
    "        \n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, test_loader, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "    #amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_x = Variable(test_loader.dataset.data.type_as(torch.FloatTensor())).cpu()\n",
    "evaluate_y = Variable(test_loader.dataset.targets).cpu()\n",
    "\n",
    "\n",
    "output = model(evaluate_x)\n",
    "pred = output.data.max(1)[1]\n",
    "d = pred.eq(evaluate_y.data).cpu()\n",
    "accuracy = d.sum()/d.size()[0]\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
